['example', 'polynomial', 'curve', 'fitting'] example polynomial curve fitting begin simple regression problem shall example throughout chapter motivate number concept pose observe input variable wish observation predict value target variable present purpose consider example synthetically data know precise process data comparison learned model data example function random noise included target value detail appendix suppose given training observation written together corresponding observation value figure show plot training data point input data figure value spaced uniformly range target data corresponding value function small level random noise distribution section point order obtain corresponding value generating data property many real data set namely posse underlying regularity wish learn individual observation corrupted random noise noise might arise intrinsically stochastic random ce radioactive decay typically source variability unobserved goal exploit training order make prediction value target variable value input variable shall later implicitly trying discover underlying function intrinsically cult problem generalize data furthermore data corrupted noise given uncertainty appropriate value probability theory section framework uncertainty precise quantitative manner decision theory section exploit probabilistic representation order make prediction optimal according appropriate criterion moment however shall proceed rather informally consider simple approach based curve particular shall data polynomial function form order polynomial raised power polynomial collectively vector note although polynomial function nonlinear function linear function function polynomial linear unknown parameter important property linear model extensively chapter value determined polynomial training data done error function measure function given value training data point simple choice error function widely used given square error prediction data point corresponding target value minimize factor included later convenience shall discus choice error function later chapter moment simply note quantity would zero figure error function half square displacement shown vertical green bar data point function function exactly training data point interpretation error function figure solve curve problem choosing value small possible error function quadratic function derivative respect linear element minimization error function unique solution found closed form resulting polynomial exercise given function remains problem choosing order polynomial shall turn example important concept model comparison model selection figure show four example result polynomial order data shown figure notice constant order polynomial give rather poor data consequently rather poor representation function third order polynomial give best function example shown figure much higher order polynomial obtain excellent training data fact polynomial pass exactly data point however curve wildly give poor representation function latter behaviour known noted goal achieve good generalization making accurate prediction data obtain quantitative insight dependence generalization performance considering separate test data point exactly procedure used generate training point choice random noise value included target value choice evaluate residual value given training data also evaluate test data sometimes convenient figure plot polynomial various order shown curve data shown figure error division compare different size data set equal footing square root measured scale unit target variable graph training test error shown various value figure test error measure well value data observation note figure small value give relatively large value test error fact corresponding polynomial rather incapable oscillation function value range give small value test error also give reasonable representation generating function seen case figure figure graph error training inde pendent test various value training test training error go zero might expect polynomial degree freedom corresponding tuned exactly data point training however test error become large figure corresponding function exhibit wild oscillation seem paradoxical polynomial given order lower order polynomial special case polynomial therefore generating result least good polynomial furthermore might suppose best predictor data would function data shall later indeed case know power series expansion function term order might expect result improve monotonically increase gain insight problem examining value polynomial various order shown table increase magnitude typically get particular polynomial become tuned data large positive negative value correspond table table polynomial various order observe typical crease dramatically polynomial increase figure plot solution error function polynomial data point left plot data point right plot increasing size data problem polynomial function match data point exactly data point particularly near end range function exhibit large figure intuitively happening polynomial value becoming increasingly tuned random noise target value also interesting examine behaviour given model size data varied shown figure given model complexity problem become severe size data increase another data complex word model afford data rough heuristic sometimes number data point multiple number adaptive parameter model however shall chapter number parameter necessarily appropriate measure model complexity also something rather unsatisfying limit number parameter model according size available training would seem reasonable choose complexity model according problem shall least square approach model parameter case maximum likelihood section problem understood general property maximum likelihood approach section problem shall perspective model number parameter greatly number data point indeed model effective number parameter automatically size data moment however instructive continue current approach consider practice apply data set limited size figure plot polynomial data shown figure error function value regularization parameter corresponding case regularizer corresponding shown bottom right figure wish relatively complex model technique often used control phenomenon case regularization penalty term error function order discourage reaching large value penalty term take form square leading error function form importance regularization term error term note often regularizer inclusion cause result depend choice origin target variable included regularization shall discus topic detail section error function exactly closed form technique known exercise statistic literature shrinkage method reduce value particular case quadratic regularizer ridge sion context neural network approach known weight decay figure show result polynomial order data error function given value suppressed obtain much closer representation underlying function however large value obtain poor shown figure corresponding polynomial given table showing regularization desired effect reducing table table polynomial various value regularization parameter note model regularization graph bottom right value increase typical magnitude get smaller magnitude impact regularization term generalization error seen plotting value error training test set shown figure effect control effective complexity model hence degree issue model complexity important length section simply note trying solve practical application approach error function would determine suitable value model complexity result suggest simple namely taking available data partitioning training used determine separate validation also holdout used optimize model complexity either many case however prove wasteful valuable training data seek sophisticated approach section discussion polynomial curve largely tuition seek approach problem pattern recognition turning discussion probability theory well providing foundation nearly subsequent development book also figure graph versus polynomial training test give important insight concept text polynomial curve allow extend complex situation
['probability', 'theory'] probability theory concept pattern recognition uncertainty noise measurement well size data set prob ability theory consistent framework cation uncertainty form central foundation pattern recognition combined decision theory section make optimal prediction given information available even though incomplete ambiguous introduce basic concept probability theory considering example imagine box blue apple orange blue apple orange figure suppose randomly pick box randomly select item fruit sort fruit replace came could imagine process many time suppose pick time pick blue time remove item fruit equally likely select piece fruit example identity chosen random variable shall denote random variable take possible value namely corresponding corresponding blue similarly identity fruit also random variable take either value apple orange begin shall probability event fraction time event total number trial limit total number trial go thus probability figure simple example box fruit apple shown green shown orange basic idea probability figure derive product rule probability considering random variable take value take value illustration consider total number instance variable denote number instance number point corresponding cell array number point column corresponding number point corresponding probability blue write probability note probability must interval also event mutually exclusive include possible outcome instance example must either blue probability event must question overall probability lection procedure pick apple given chosen orange probability chose blue answer question indeed much complex question associated problem pattern recognition rule probability known rule product rule rule shall return box fruit example order derive rule probability consider slightly general ample shown figure random variable could instance fruit variable considered shall suppose take value take value consider total trial sample variable number trial also number trial take value irrespective value take similarly number trial take value probability take value take value written joint probability given number point falling cell fraction total number point hence implicitly considering limit similarly probability take value irrespective value written given fraction total number point fall column number instance column figure number instance cell column therefore rule probability note sometimes marginal probability variable case consider instance fraction instance written conditional probability given fraction point column fall cell hence given derive following relationship product rule probability quite careful make distinction random vari able fruit example value random variable take example thus probability take value although help avoid ambiguity lead rather cumbersome notation many case need pedantry instead simply write denote distribution variable denote distribution particular value provided interpretation clear context compact notation write fundamental rule probability theory following form rule probability rule product rule joint probability probability similarly quantity conditional probability probability given whereas quantity marginal probability simply probability simple rule form basis probabilistic machinery throughout book product rule together symmetry property immediately obtain following relationship conditional probability theorem play central role pattern recognition machine learning rule denominator theorem expressed term quantity numerator view denominator theorem normalization constant ensure conditional probability side value equal figure show simple example joint distribution variable illustrate concept marginal conditional distribution sample data point drawn joint distribution shown left right histogram fraction data point value probability fraction would equal corresponding probability limit view histogram simple model probability distribution given number point drawn distribution distribution data lie heart statistical pattern recognition great detail book plot figure show corresponding histogram estimate return example box fruit moment shall explicit distinguishing random variable seen probability either blue box given respectively note satisfy suppose pick random turn blue probability apple fraction apple blue fact write four conditional probability type fruit given selected figure illustration distribution variable take possible value take possible value left show sample point drawn joint probability variable show histogram estimate marginal distribution well conditional distribution corresponding bottom left note probability similarly product rule probability evaluate overall probability choosing apple rule suppose instead told piece fruit selected orange would like know came evaluate probability distribution box conditioned identity fruit whereas probability give probability distribution fruit conditioned identity solve problem reversing conditional probability theorem give rule provide important interpretation theorem chosen told identity selected item fruit complete information available provided probability call prior probability probability available observe identity fruit told fruit orange theorem compute probability shall call posterior probability probability note example prior probability likely select blue however piece selected fruit orange posterior probability likely selected fact result accord intuition proportion orange much higher blue observation fruit orange cant evidence fact evidence strong prior make likely chosen rather blue finally note joint distribution variable product said independent product rule conditional distribution given indeed independent value instance box fruit example fraction apple orange probability apple independent chosen
['probability', 'densities'] probability density well considering probability discrete set event also wish consider probability respect continuous variable shall limit relatively informal discussion probability variable falling interval given probability density figure probability interval given figure concept probability discrete variable probability density continuous variable probability lying inter given probability density expressed derivative cumulative function probability value must real axis probability density must satisfy condition nonlinear change variable probability density differently simple function factor instance consider change variable function becomes consider probability density density respect variable denote fact different density observation falling range small value range hence consequence property concept maximum probability density dependent choice variable exercise probability lie interval given cumulative distribution function shown figure several continuous variable collectively vector joint probability density probability falling volume point given probability density must satisfy integral taken whole space also consider joint probability distribution combination discrete continuous variable note discrete variable sometimes probability mass function probability mass concentrated value product rule probability well theorem apply equally case probability density combination discrete variable instance real variable product rule take form formal cation product rule continuous variable feller branch mathematics measure theory lie outside scope book validity seen informally however dividing real variable interval width considering discrete probability interval taking limit turn sum integral give desired result
['expectations', 'covariances'] expectation covariance important operation probability weighted average function average value function probability distribution expectation discrete distribution given average weighted relative probability different value case continuous variable expectation expressed term integration respect corresponding probability density either case given number point drawn probability distribution probability density expectation point shall make extensive result discus sampling method chapter approximation becomes exact limit sometimes considering expectation function several variable case subscript indicate variable instance average function respect distribution note function also consider conditional expectation respect conditional distribution analogous continuous variable variance measure much variability around mean value expanding square variance also written term expectation exercise particular consider variance variable given random variable covariance express extent vary together dent covariance exercise case vector random variable covariance matrix consider covariance component vector slightly simpler notation
['bayesian', 'probabilities'] probability chapter probability term frequency random repeatable event shall refer classical interpretation probability turn general view probability provide cation uncertainty consider uncertain event example whether moon orbit around whether arctic century event repeated numerous time order notion probability context box fruit nevertheless generally idea example quickly think polar melting obtain fresh evidence instance earth observation satellite gathering novel form diagnostic information revise opinion rate loss assessment matter affect action take instance extent reduce emission greenhouse gas circumstance would like able quantify expression uncertainty make precise revision uncertainty light evidence well subsequently able take optimal action decision consequence elegant general interpretation probability probability represent uncertainty however choice inevitable respect common sense making rational coherent inference instance numerical value used represent degree belief simple axiom common sense property belief lead uniquely rule degree belief equivalent product rule probability provided rigorous proof probability theory could extension logic situation uncertainty numerous author different set property axiom measure uncertainty satisfy good savage case resulting numerical quantity behave according rule probability therefore natural refer quantity probability pattern recognition helpful general born bridge well clergyman well amateur scientist mathematician studied logic theology fellow royal society century sue regarding probability arose connection gambling concept insurance particularly important problem concerned verse probability solution paper essay towards problem doctrine chance three year death sophical transaction royal society fact theory case form prior inde pendently theory general form broad applicability probability consider example polynomial curve section reasonable apply notion probability random value variable however would like address quantify uncertainty surround appropriate choice model param shall perspective machinery probability theory describe uncertainty model parameter indeed choice model theorem recall box fruit example observation identity fruit provided relevant information probability chosen example theorem used convert prior probability posterior probability evidence provided data shall detail later adopt similar approach making inference quantity parameter polynomial curve example capture assumption observing data form prior probability distribution effect data expressed conditional probability shall later section explicitly theorem take form evaluate uncertainty form posterior probability quantity side theorem data function parameter vector case likelihood function express probable data different setting parameter vector note likelihood probability distribution integral respect necessarily equal given likelihood state theorem word posterior likelihood prior quantity function denominator normalization constant posterior distribution side valid probability density indeed side respect express denominator theorem term prior distribution likelihood function paradigm likelihood function play central role however manner used fundamentally approach setting considered parameter whose value determined form estimator error bar estimate considering distribution possible data set contrast viewpoint single data namely actually uncertainty parameter expressed probability distribution widely used estimator maximum likelihood value likelihood function choosing value probability data machine learning literature negative likelihood function error function negative logarithm monotonically creasing function likelihood equivalent error approach error bar bootstrap multiple data set suppose original data data point create data drawing point random replacement point replicated whereas point absent process repeated time generate data set size sampling original data statistical accuracy parameter estimate looking variability prediction different bootstrap data set advantage viewpoint inclusion prior edge naturally suppose instance coin three time land head time classical maximum likelihood estimate probability landing head would give future toss land section head contrast approach reasonable prior lead much extreme conclusion much controversy debate associated relative paradigm fact unique even viewpoint instance common criticism approach prior distribution selected basis mathematical convenience rather prior belief even subjective nature conclusion choice prior seen source reducing dependence prior motivation noninformative prior section however lead different model indeed method based poor choice prior give poor result high evaluation method offer protection prob technique remain useful area model section comparison book place strong emphasis viewpoint huge growth practical importance method past year also useful concept although framework origin century tical application method long time severely limited carrying full procedure particularly need marginalize integrate whole parameter space shall order make prediction compare different model development sampling method chain monte chapter along dramatic improvement speed memory capacity computer door practical technique pressive range problem domain monte method applied wide range model however intensive mainly used problem recently highly deterministic approximation scheme variational expectation propagation chapter offer complementary alternative sampling method technique used application
['gaussian', 'distribution'] distribution shall devote whole chapter study various probability property convenient however introduce important probability distribution continuous variable normal distribution shall make extensive distribution remainder chapter indeed throughout much book case single variable distribution parameter mean vari square root variance given standard deviation reciprocal variance written precision shall motivation term shortly figure show plot distribution form distribution also straightforward show exercise said modesty point declared best mathematician time claim true well also made numerous contribution nebular hypothesis earth thought formed cooling large rotating disk dust edition state probability theory nothing common sense reduced calculation work included discus sion inverse probability calculation later theorem used solve problem life expectancy jurisprudence planetary mass triangulation error estimation figure plot showing mean standard deviation thus requirement valid probability density readily expectation function particular average value given exercise parameter average value distribution mean similarly second order moment variance given hence variance parameter maximum distribution known mode mode mean exercise also interested distribution vector continuous variable given vector mean matrix covariance determinant shall make distribution chapter although property studied detail section figure illustration likelihood function distribution shown curve black point note data value likelihood function given product blue value likelihood mean vari product suppose data observation observation scalar variable note type face distinguish single observation variable denote shall suppose observation drawn independently distribution whose mean variance unknown would like determine parameter data data point drawn independently distribution said independent identically distributed often seen joint probability independent event given product marginal probability event separately data therefore write probability data given form function likelihood function gaus figure common criterion parameter probability data parameter value maximize likelihood function might seem like strange criterion fore going discussion probability theory would seem natural maximize probability parameter given data probability data given parameter fact criterion related shall discus context curve section moment however shall determine value unknown likelihood function tice convenient maximize likelihood function logarithm monotonically increasing function argument maximization function equivalent maximization function taking subsequent mathematical analysis also help numerically product large number small probability easily numerical precision computer resolved instead probability likelihood function written form respect obtain maximum likelihood solution given exercise sample mean mean value similarly respect obtain maximum likelihood solution variance form sample variance measured respect sample mean note joint maximization respect case distribution solution evaluate subsequently result evaluate later chapter also subsequent chapter shall highlight cant limitation maximum likelihood approach give indication problem context solution maximum likelihood param setting distribution particular shall show maximum likelihood approach systematically underestimate variance distribution example phenomenon bias related problem context polynomial curve section note maximum likelihood solution function data value consider expectation quantity respect data value come distribution parameter straightforward show exercise average maximum likelihood estimate obtain correct mean underestimate true variance factor intuition behind result given figure following estimate variance parameter unbiased figure illustration bias likelihood determine variance green curve show true distribution data three curve show distribution three data set consist data point shown blue maximum likelihood result across three data set mean correct variance systematically measured relative sample mean relative true mean section shall result automatically adopt approach note bias maximum likelihood solution becomes cant number data point increase limit maximum likelihood solution variance equal true variance distribution data practice anything small bias prove serious problem however throughout book shall interested complex model many parameter bias problem maximum likelihood much severe fact shall issue bias maximum likelihood lie root problem context polynomial curve
['curve', 'fitting', 'visited'] curve seen problem polynomial curve expressed term error minimization return curve example view section probabilistic perspective thereby gaining insight error function regularization well taking towards full treatment goal curve problem able make prediction target variable given value input variable basis training data input value corresponding target value express uncertainty value target variable probability distribution purpose shall assume given value corresponding value distribution mean equal value polynomial curve given thus consistency notation later chapter sion parameter corresponding inverse variance distribution schematically figure figure schematic illustration gaus conditional distribution given given mean given function precision given parameter related vari training data determine value unknown parameter maximum likelihood data assumed drawn independently distribution likelihood function given case simple distribution convenient maximize logarithm likelihood function substituting form distribution given obtain likelihood function form consider determination maximum likelihood solution determined respect purpose omit last term side depend also note scaling likelihood positive constant alter location maximum respect replace finally instead likelihood equivalently minimize negative likelihood therefore likelihood equivalent concerned error function thus error function arisen consequence likelihood assumption noise distribution also maximum likelihood determine precision parameter conditional distribution respect give determine parameter vector governing mean sequently precision case simple distribution section determined parameter make prediction value probabilistic model expressed term predictive distribution give probability distribution rather simply point estimate substituting maximum likelihood parameter give take step towards approach introduce prior distribution polynomial simplicity consider distribution form precision distribution total number element vector order polynomial variable control distribution model parameter theorem posterior distribution proportional product prior distribution likelihood function determine probable value given data word posterior distribution technique maximum posterior simply taking negative logarithm combining maximum posterior given minimum thus posterior distribution equivalent error function form regularization parameter given
['bayesian', 'curve', 'fitting'] curve although included prior distribution still point estimate amount treatment fully approach consistently apply product rule probability shall shortly integrate heart method pattern recognition curve problem given training data along test point goal predict value therefore wish evaluate predictive distribution shall assume parameter known advance later chapter shall discus parameter data setting treatment simply consistent application product rule probability allow predictive distribution written form given dependence simplify notation posterior distribution param found side shall section problem curve example posterior distribution analytically similarly also analytically result predictive distribution given form mean variance given matrix given unit matrix vector element variance well mean predictive distribution dependent term uncertainty value noise target variable expressed already maximum likelihood predictive distribution however second term uncertainty parameter consequence treatment predictive distribution synthetic sinusoidal regression problem figure figure predictive distribution result treatment polynomial curve polynomial parameter corresponding known noise variance curve mean predictive distribution region deviation around mean
['model', 'selection'] model selection example polynomial curve least square optimal order polynomial gave best generalization order polynomial control number free parameter model thereby model complexity least square regularization also control effective complexity model whereas complex model mixture distribution neural network multiple governing complexity practical application need determine value parameter principal objective usually achieve best predictive performance data furthermore well appropriate value complexity parameter within given model wish consider range different type model order best particular application already seen maximum likelihood approach training good indicator predictive performance seen data problem data plentiful approach simply available data train range model given model range value complexity parameter compare independent data sometimes validation select best predictive performance model design many time size data validation data occur necessary keep aside third test performance selected model many application however supply data training testing limited order build good model wish much available data possible training however validation small give relatively noisy estimate predictive performance solution dilemma figure proportion available data used training making figure technique case available data partitioning group case equal size group used train model group procedure repeated possible choice group block score run data ass performance data particularly scarce appropriate consider case total number data point give technique major drawback number training run must factor prove problematic model training expensive problem technique separate data ass performance might multiple complexity parameter single model stance might several regularization parameter exploring combination setting parameter could worst case require number training run exponential number parameter clearly need better ideally rely training data allow multiple model type single training fore need measure performance training data suffer bias historically various information criterion attempt correct bias maximum likelihood addition penalty term compensate complex model example information criterion model quan best likelihood number adjustable parameter model variant quantity information criterion section criterion take account uncertainty model parameter however practice tend overly simple model therefore turn section fully approach shall complexity penalty arise natural
['curse', 'dimensionality'] curse dimensionality polynomial curve example input variable tical application pattern recognition however deal space figure scatter plot data input variable nous class green annular class blue laminar class goal test point noted high dimensionality many input variable discus pose serious challenge important factor design pattern recognition technique order illustrate problem consider synthetically data measurement taken pipeline mixture bishop three material present three different geometrical known homogenous annular laminar fraction three material also vary data point dimensional input vector measurement taken gamma densitometer measure attenuation gamma ray passing along beam pipe data detail appendix figure show point data plot showing input value purpose illustration data point according three class goal data training order able observation cross figure observe cross surrounded numerous point might suppose class however also plenty green point nearby might think could instead belong green class unlikely blue class intuition identity cross determined strongly nearby point training strongly distant point fact intuition turn reasonable fully later chapter turn intuition learning algorithm simple would divide input space regular cell given test point wish predict class decide cell training data point figure illustration simple approach solution cation problem input space divided cell test point assigned class majority number cell test point shall shortly simplistic approach severe shortcoming fall cell identity test point class number training point cell test point tie broken random numerous problem naive approach becomes apparent consider extension problem number input variable corresponding input space higher dimensionality origin problem figure show divide region space regular cell number cell dimensionality space problem exponentially large number cell would need exponentially large quantity training data order ensure cell empty clearly hope technique space variable need sophisticated approach gain insight problem space example polynomial curve considering would section figure illustration curse dimensionality showing number region regular grid exponentially dimensionality space clarity subset cubical region shown extend approach deal input space several variable input variable general polynomial order would take form increase number independent independent interchange symmetry amongst variable portionally practice capture complex dependency data need polynomial polynomial order growth number like although power growth exercise rather exponential growth still point method becoming rapidly unwieldy limited practical utility geometrical intuition formed life spent space three fail badly consider space higher dimensionality simple example consider sphere radius space dimension fraction volume sphere lie radius evaluate fraction volume sphere radius dimension must scale write constant thus fraction given exercise plotted function various value figure large fraction even small value thus space high dimensionality volume sphere concentrated thin shell near surface example direct relevance pattern recognition consider behaviour distribution space transform polar integrate directional variable obtain expression density function radius origin exercise thus probability mass inside thin shell thickness radius distribution plotted various value figure large probability mass concentrated thin shell severe arise space many dimension sometimes curse dimensionality bellman book shall make tensive illustrative example input space dimension make particularly easy illustrate technique graphically reader however intuition space dimensionality generalize space many dimension figure plot fraction volume sphere lying range various value dimensionality volume fraction although curse dimensionality certainly raise important issue tern recognition application prevent effective technique applicable space reason twofold first real data often region space lower effective dimension particular direction important variation target variable occur second real data typically exhibit smoothness property least locally part small change input variable produce small change target variable local technique allow make prediction target variable value input variable successful pattern recognition tech exploit property consider example application image identical planar object belt goal determine orientation image point figure plot probability density respect radius gaus distribution various value dimensionality space probability mass within thin shell radius space whose dimensionality determined number object occur different position within image different orientation three degree freedom variability image image live three dimensional manifold within space complex relationship object position orientation intensity manifold highly nonlinear goal learn model take input image output orientation object irrespective position degree freedom variability within manifold cant
['decision', 'theory'] decision theory seen section probability theory consistent mathematical framework uncertainty turn discussion decision theory combined probability theory make optimal decision situation uncertainty pattern recognition suppose input vector together corresponding vector target variable goal predict given value regression problem comprise continuous variable whereas cation problem represent class label joint probability distribution complete summary uncertainty associated variable determination training data example inference typically cult problem whose solution form subject much book practical application however must often make prediction value generally take action based understanding value likely take aspect subject decision theory consider example medical diagnosis problem taken image patient wish determine whether patient cancer case input vector intensity image output variable represent presence cancer denote class absence cancer denote class might instance choose binary variable class class shall later choice label value particularly convenient probabilistic model general inference problem joint distribution equivalently give complete probabilistic description situation although useful informative quantity must decide either give treatment patient would like choice optimal appropriate sense hart decision step subject decision theory tell make optimal decision given appropriate probability shall decision stage generally simple even trivial inference problem give introduction idea decision theory rest book background well detailed account found berger bather giving detailed analysis consider informally might expect probability play role making decision obtain image patient goal decide class assign image interested probability class given image given theorem probability expressed form note quantity theorem joint distribution either respect appropriate variable interpret prior probability class corresponding posterior probability thus sent probability person cancer take measurement similarly corresponding probability theorem light information minimize chance wrong class intuitively would choose class higher posterior probability show intuition correct also discus general criterion making decision
['minimizing', 'misclassification', 'rate'] cation rate suppose goal simply make cation possible need rule value available class rule divide input space region decision region class point assigned class boundary decision region decision boundary decision surface note decision region need contiguous could comprise number disjoint region shall encounter example decision boundary decision region later chapter order optimal decision rule consider case class cancer problem instance mistake input vector belonging class assigned class vice probability given free choose decision rule point class clearly minimize arrange assigned whichever class smaller value integrand thus given value assign class product rule probability factor common term restate result saying minimum figure schematic illustration joint probability class plotted together decision boundary value class hence belong decision region whereas point belong error arise blue green region error point class green region conversely point region error point class blue region vary location decision boundary combined area blue green region remains constant whereas size region optimal choice curve cross corresponding case region equivalent minimum cation rate decision rule value class higher posterior probability probability making mistake value assigned class posterior probability result class single input variable figure general case class slightly easier maximize probability correct given region chosen assigned class product rule factor common term assigned class posterior probability figure example loss matrix cancer treatment problem row correspond true class whereas column respond assignment class made sion criterion cancer normal cancer normal
['minimizing', 'expected', 'loss'] loss many application objective complex simply number cation consider medical diagnosis problem note patient cancer incorrectly cancer consequence patient distress plus need investigation conversely patient cancer healthy result premature death lack treatment thus consequence type mistake dramatically different would clearly better make mistake second kind even expense making mistake kind formalize issue introduction loss function also cost function single overall measure loss taking available decision action goal minimize total loss note author consider instead utility function whose value maximize equivalent concept take utility simply negative loss throughout text shall loss function convention suppose value true class assign class equal incur level loss denote view element loss matrix instance cancer example might loss matrix form shown figure particular loss matrix say loss correct decision made loss healthy patient cancer whereas loss patient cancer healthy optimal solution loss function however loss function true class unknown given input vector uncertainty true class expressed joint probability distribution seek instead minimize average loss average respect distribution given assigned independently decision region goal choose region order minimize loss minimize product rule eliminate common factor thus decision rule loss figure illustration reject option input probability equal threshold reject region class quantity minimum clearly trivial know posterior class
['reject', 'option'] reject option seen cation error arise region input space posterior probability unity equivalently joint distribution comparable value region relatively uncertain class membership application appropriate avoid making decision cult case anticipation lower error rate example cation made known reject option example hypothetical medical illustration appropriate automatic system image little doubt correct class human expert ambiguous case achieve threshold input posterior probability equal case class single continuous input variable figure note setting ensure example whereas class setting ensure example thus fraction example value easily extend reject criterion minimize loss loss matrix given taking account loss reject decision made exercise
['inference', 'decision'] inference decision broken cation problem separate stage inference stage training data learn model subsequent decision stage posterior probability make class assignment alternative possibility would solve problem together simply learn function map input directly decision function discriminant function fact identify three distinct approach decision problem used practical application given decreasing order complexity first solve inference problem density class individually also separately infer prior class probability theorem form posterior class probability usual denominator theorem found term quantity numerator equivalently model joint distribution directly normalize obtain posterior probability found posterior probability decision theory determine class membership input approach explicitly implicitly model distribution input well output known generative model sampling possible generate synthetic data point input space first solve inference problem posterior class probability subsequently decision theory assign class approach model posterior probability directly discriminative model find function discriminant function map input directly onto class label instance case problem might binary valued class class case probability play role consider relative merit three alternative approach demanding joint distribution many application high dimensionality consequently need large training order able determine density reasonable accuracy note class prior often simply fraction training data point class advantage approach however also marginal density data determined useful data point probability model prediction class density figure example density class single input variable left plot together corresponding posterior probability right plot note mode density shown blue left plot effect posterior probability vertical green line right plot show decision boundary give minimum cation rate accuracy known outlier detection novelty detection bishop however wish make cation decision waste computational resource excessively demanding data joint distribution fact really need posterior probability directly approach indeed class conditional density contain structure little effect probability figure much interest exploring relative merit generative discriminative approach machine learning way combine even simpler approach training data discriminant function map directly onto class label thereby combining inference decision stage single learning problem example figure would correspond value shown vertical green line decision boundary giving minimum probability cation option however longer access posterior probability many powerful reason wanting compute posterior probability even subsequently make decision include risk consider problem element loss matrix subjected revision time time might occur application know posterior probability trivially revise minimum risk decision criterion appropriately discriminant function change loss matrix would require return training data solve cation problem afresh reject option posterior probability allow determine rejection criterion minimize cation rate generally loss given fraction data point compensating class prior consider medical problem suppose collected large number image eral population training data order build screening system cancer rare amongst general population might every example presence used data train adaptive model could severe small proportion cancer class instance assigned every point normal class would already achieve accuracy would cult avoid trivial solution also even large data contain example image cancer learning algorithm exposed broad range example image hence likely generalize well balanced data selected equal number exam class would allow accurate model however compensate effect cation training data suppose used data found model posterior probability theorem posterior probability proportional prior probability interpret fraction point class therefore simply take posterior probability balanced data divide class fraction data multiply class fraction population wish apply model finally need normalize ensure posterior probability note procedure applied learned discriminant function directly instead posterior probability combining model complex application wish break problem number smaller tackled module example hypothetical medical diagnosis problem information available blood test well age rather combine heterogeneous information huge input space effective build system interpret image different interpret blood data long model give posterior probability class combine output systematically rule probability simple assume class separately distribution input image blood data independent example conditional independence property section hold distribution conditioned class posterior probability given blood data given thus need class prior probability easily estimate fraction data point class need normalize resulting posterior probability particular independence assumption example naive model section note joint marginal distribution typically factorize model shall later chapter construct model combining data require conditional independence assumption
['loss', 'functions', 'regression'] loss function regression decision theory context cation prob turn case regression problem curve example decision stage choosing section mate value input suppose incur loss average loss given common choice loss function regression problem squared loss given case loss written goal choose minimize assume completely function formally calculus variation appendix give product rule probability obtain figure regression function squared loss given mean conditional conditional average conditioned known regression function result figure readily extended tiple target variable vector case optimal solution conditional average exercise also derive result slightly different also shed light nature regression problem armed knowledge optimal solution conditional expectation expand square term keep notation uncluttered denote substituting loss function integral obtain expression loss function form function seek determine term equal case term vanish simply result derived previously show optimal least square predictor given conditional mean second term variance distribution intrinsic variability target data noise independent irreducible minimum value loss function cation problem either determine appropriate prob ability make optimal decision build model make decision directly indeed identify three distinct approach regression problem given order decreasing complexity first solve inference problem joint density normalize conditional density marginalize conditional mean given first solve inference problem conditional density subsequently marginalize conditional mean given find regression function directly training data relative merit three approach follow line problem squared loss possible choice loss function regression indeed situation squared loss lead poor result need develop sophisticated approach important example concern situation conditional distribution multimodal often solution inverse problem consider simple section generalization squared loss loss whose expectation given squared loss function plotted various value figure minimum given conditional mean conditional median conditional mode exercise
['information', 'theory'] information theory chapter variety concept probability theory decision theory form foundation much subsequent discussion book close chapter additional concept information theory also prove useful development pattern recognition machine learning technique shall focus concept refer reader elsewhere detailed discussion cover begin considering discrete random variable much information received observe value variable amount information degree surprise learning value told highly improbable event received information told likely event knew event certain happen would receive information measure information content therefore depend probability distribution therefore look quantity monotonic function probability express information content form found event unrelated information gain observing information separately unrelated event statistically independent relationship easily shown must given logarithm exercise figure plot quantity various value negative sign information positive zero note probability event correspond high information content choice basis logarithm arbitrary moment shall adopt convention prevalent information theory logarithm base case shall shortly unit bit binary digit suppose sender wish transmit value random variable receiver average amount information transmit process taking expectation respect distribution given important quantity entropy random variable note limp shall take whenever encounter value given rather heuristic motivation corresponding entropy show indeed posse useful property consider random variable possible state equally likely order communicate value receiver would need transmit message length bit notice entropy variable given bit consider example cover variable state respective probability given entropy case given bit nonuniform distribution smaller entropy uniform shall gain insight shortly discus interpretation entropy term disorder moment consider would transmit identity variable state receiver could number however take advantage nonuniform distribution shorter code probable event expense longer code probable event hope getting shorter average code length done state instance following code string average length code average code length bit entropy random variable note shorter code string used must possible concatenation string component part instance uniquely state sequence relation entropy length general noiseless theorem state entropy lower bound number bit transmit state random variable shall switch natural logarithm provide convenient link idea elsewhere book case entropy measured unit instead bit differ simply factor concept entropy term average amount information specify state random variable fact concept entropy much origin physic context equilibrium thermodynamics later given interpretation measure disorder development statistical mechanic understand alternative view entropy considering identical object divided amongst bin object consider number different way object bin way choose object way choose second object leading total way allocate object bin pronounced factorial product however dont wish distinguish rearrangement object within way object total number way object bin given multiplicity entropy logarithm multiplicity scaled appropriate constant consider limit fraction apply approximation give used probability object assigned physic terminology object bin overall distribution occupation number expressed ratio multiplicity also known weight interpret bin state discrete random variable entropy random variable distribution sharply peaked around value relatively entropy whereas spread evenly across many value higher entropy figure entropy equal minimum value maximum entropy found multiplier enforce normalization constraint appendix probability thus maximize probability probability figure histogram probability distribution bin higher value entropy distribution entropy would arise uniform distribution would give equal given total number state corresponding value entropy result also derived inequality shortly verify stationary point indeed maximum exercise evaluate second derivative entropy give element identity matrix extend entropy include distribution variable first divide bin width assuming continuous mean value theorem tell must exist value quantize continuous variable value value whenever fall probability observing value give discrete distribution entropy take form used omit second term side consider limit term side approach integral limit quantity side differential entropy discrete continuous form entropy differ quantity limit fact specify continuous variable precisely large number bit density multiple continuous variable collectively vector differential entropy given case discrete distribution maximum entropy equal distribution probability across possible state variable consider maximum entropy continuous variable order maximum well constrain second moment well normalization constraint therefore maximize differential entropy physicist statistical mechanic prior concept already known classical thermodynamics fact take energy system energy typically available useful work entropy macroscopic quantity could related statistical property micro scopic level expressed famous equation number possible unit joule kelvin known constant idea many scientist arose second thermo dynamic state entropy closed system increase time contrast microscopic level classical physic reversible found cult latter could explain didnt fully appreciate statistical nature entropy could never decrease time simply overwhelming probability would generally increase even long running dispute editor leading german physic journal refer atom molecule anything convenient construct continued attack work lead bout depression eventually suicide shortly death experiment colloidal suspension veri theory value constant equation tombstone three constraint constrained maximization multiplier appendix maximize following functional respect calculus variation derivative functional zero giving appendix multiplier found back substitution result three constraint equation leading result exercise distribution differential entropy note constrain distribution entropy however resulting distribution indeed hindsight constraint necessary evaluate differential entropy obtain exercise thus entropy increase distribution becomes increase result also show differential entropy unlike discrete entropy negative suppose joint distribution draw pair value value already known additional information specify corresponding value given thus average additional information specify written conditional entropy given easily seen product rule conditional entropy relation exercise differential entropy differential marginal distribution thus information describe given information describe alone plus additional information specify given
['relative', 'entropy', 'mutual', 'information'] relative entropy mutual information section number concept information theory notion entropy start relate idea pattern recognition consider unknown distribution suppose distribution construct scheme purpose value receiver average additional amount information specify value assuming choose scheme result instead true distribution given known relative entropy divergence diver distribution note symmetrical quantity show divergence equality introduce concept convex function function said convex property every chord lie function shown figure value interval written form corresponding point chord given graduating michigan bell telephone laboratory paper mathematical theory communication bell system technical journal laid foundation modern information paper word information could sent stream communication said term entropy cause similarity quantity used physic also nobody know entropy really discussion always figure convex function chord shown blue lie function shown chord corresponding value function convexity equivalent requirement second derivative function everywhere positive example convex function exercise function strictly convex equality function opposite property namely every chord lie function concave corresponding strictly concave function convex concave technique proof induction show exercise convex function point result known inequality interpret probability distribution discrete variable taking value written expectation continuous variable inequality take form apply inequality form divergence give used fact convex function together condition fact strictly convex function equality hold thus divergence measure dissimilarity distribution intimate relationship data compression estimation problem unknown probability distribution compression know true distribution different true must necessarily average additional information must least equal divergence tween distribution suppose data unknown distribution wish model approximate distribution parametric distribution adjustable parameter example determine minimize divergence respect directly dont know suppose however training point drawn expectation respect point second term side independent term negative likelihood function distribution training thus divergence equivalent likelihood function consider joint distribution set variable given set variable independent joint distribution factorize product variable independent gain idea whether close dent considering divergence joint distribution product given mutual information variable property divergence equal independent product rule probability mutual information related conditional entropy exercise thus view mutual information reduction uncertainty virtue told value vice perspective view prior distribution posterior data mutual information therefore reduction uncertainty consequence observation exercise consider error function given function given polynomial show minimize error function given solution following linear equation index component whereas raised power write coupled linear equation analogous minimize error function given suppose three box blue green apple orange lime apple orange lime apple orange lime chosen random probability piece fruit removed equal probability item probability apple observe selected fruit fact orange probability came green consider probability density continuous vari able suppose make nonlinear change variable density according show location maximum density general related location maximum density simple functional relation consequence factor show maximum probability density contrast simple function dependent choice variable verify case linear transformation location maximum variable show show variable independent covariance zero exercise prove normalization condition consider integral evaluate writing square form make transformation polar substitute show integral taking square root side obtain finally result show distribution normal change variable verify distribution given next side normalization condition respect verify finally show hold show mode maximum distribution given similarly show mode given suppose variable statistically independent show mean variance setting derivative likelihood function respect equal zero verify result result show denote data point distribution mean variance otherwise hence prove result suppose variance result maximum likelihood estimate true value mean show estimator property expectation given true variance show arbitrary square matrix element written form symmetric matrix respectively satisfying consider second order term higher order polynomial dimension given show contribution matrix therefore without loss generality matrix chosen symmetric element matrix chosen show number independent parameter matrix given exercise next explore number dent parameter polynomial order polynomial dimensionality input space start writing order term polynomial dimension form comprise element number independent parameter many interchange symmetry factor begin showing redundancy removed order term form note precise relationship need made explicit result show number independent param appear order following recursion relation next proof induction show following result hold done proving result arbitrary making result assuming correct dimension correct dimension finally previous result together proof induction show show result true value comparison result exercise make together show result hold order also hold order exercise proved result number independent parameter order term polynomial expression total number independent parameter term order first show number independent parameter term order make result together proof induction show done proving result hold arbitrary assuming hold order hence showing hold order finally make approximation form large show quantity like like consider cubic polynomial dimension evaluate numerically total number independent parameter correspond typical machine learning application gamma function integration part prove relation show also hence integer result derive expression surface area volume sphere unit radius dimension consider following result transforming polar gamma function together evaluate side equation hence show next respect radius show volume unit sphere dimension given finally result show reduce usual expression consider sphere radius together concentric hypercube side sphere touch hypercube side result exercise show ratio volume sphere volume cube given volume sphere volume cube make formula form valid show ratio go zero show also ratio distance hypercube corner divided perpendicular distance side therefore go result space high dimensionality volume cube concentrated large number corner become long spike exercise explore behaviour distribution space consider distribution dimension given wish density respect radius polar direction variable show integral probability density thin shell radius thickness given surface area unit sphere dimension show function single stationary point large considering show large show maximum radial probability density also decay exponentially away maximum length scale already seen large probability mass concentrated thin shell large radius finally show probability density origin radius factor therefore probability mass distribution different radius region high probability density property distribution space high dimensionality important consequence consider inference model parameter later chapter consider number show result show decision region cation problem chosen minimize probability cation probability satisfy given loss matrix element risk choose class verify loss matrix given element identity matrix criterion choosing class posterior probability interpretation form loss matrix derive criterion loss general loss matrix general prior probability class consider cation problem loss input vector class belonging class given loss matrix loss reject option find decision criterion give minimum loss verify reject criterion section loss matrix given relationship rejection threshold consider generalization squared loss function single target variable case multiple target variable vector given calculus variation show function loss given show result case single target variable expansion square derive result analogous hence show function squared loss case vector target variable given conditional expectation consider loss regression problem loss function given write condition must satisfy order minimize show solution conditional median function probability mass also show minimum loss given conditional mode function equal value section idea entropy information observing value random variable distribution independent variable entropy function additive exercise derive relation form function first show hence induction positive integer hence show also positive integer positive rational number hence continuity positive real number finally show must take form consider discrete random variable equality form show entropy distribution evaluate divergence table joint distribution binary variable used exercise consider variable joint distribution show differential entropy pair variable equality statistically independent consider vector continuous variable distribution entropy suppose make nonsingular linear transformation obtain variable show corresponding entropy given determinant suppose conditional entropy discrete random variable zero show value variable must function word value calculus variation show stationary point functional given constraint eliminate multiplier hence show maximum entropy solution given result show entropy given strictly convex function every chord lie function show equivalent condition second derivative function positive together product rule probability prove result proof induction show inequality convex function result consider binary variable joint distribution given table evaluate following quantity draw diagram show relationship various quantity inequality show metic mean real number never geometrical mean product rule probability show mutual information relation distribution chapter central role probability theory solution pattern recognition problem turn exploration particular example probability distribution property well great interest right distribution form building block complex model used extensively throughout book distribution chapter also serve another important purpose namely provide opportunity discus statistical concept inference context simple model encounter complex situation later chapter role distribution chapter model prob ability distribution random variable given observation problem known density estimation purpose chapter shall assume data point independent identically distributed problem density estimation many probability distribution could given rise data indeed distribution nonzero data point potential candidate issue choosing appropriate distribution problem model already context polynomial curve chapter central issue pattern recognition begin considering binomial multinomial distribution discrete random variable distribution continuous random variable example parametric distribution small number adaptive parameter mean variance case example apply model problem density estimation need procedure suitable value parameter given data treatment choose value parameter criterion likelihood function contrast treatment introduce prior distribution parameter theorem compute corresponding posterior distribution given data shall important role conjugate prior lead posterior distribution functional form prior fore lead greatly analysis example conjugate prior parameter multinomial distribution distribution conjugate prior mean another distribution example exponential family distribution posse number important property detail limitation parametric approach functional form distribution turn inappropriate particular application alternative approach given density estimation method form distribution typically size data model still contain parameter control model complexity rather form distribution chapter considering three method based respectively histogram kernel
['binary', 'variables'] binary variable begin considering single binary random variable example might describe outcome coin head tail imagine coin probability landing head necessarily landing tail probability parameter probability distribution therefore written form known distribution easily veri distribution exercise mean variance given suppose data value construct likelihood function function assumption observation drawn independently setting estimate value likelihood function equivalently logarithm likelihood case distribution likelihood function given point worth likelihood function observation example statistic data distribution shall study role statistic detail derivative section respect equal zero obtain maximum likelihood estimator also known swiss mathematician many family pursue career science mathematics although study philosophy theology parent travelled extensively graduating order meet many leading scientist time returned taught mechanic professor mathematics unfortunately rivalry younger brother turned initially productive bitter public dispute cant contribution mathematics conjecture eight year death deal topic probability become known distribution figure histogram plot binomial function also known sample mean denote number observation head within data write form probability landing head given maximum likelihood frame work fraction observation head data suppose coin time happen observe head case maximum likelihood result would predict future observation give head common sense tell unreasonable fact extreme example maximum likelihood shall shortly arrive sensible conclusion introduction prior distribution also work distribution number observation given data size binomial distribution proportional order obtain normalization note coin possible way head binomial distribution written number way choosing object total identical object exercise figure show plot binomial distribution mean variance binomial distribution found result exercise show independent event mean mean variance variance observation mean variance given respectively result also proved directly calculus exercise
['beta', 'distribution'] beta distribution seen maximum likelihood setting parameter distribution hence binomial distribution given fraction observation data already noted give severely result small data set order develop treatment problem need introduce prior distribution parameter consider form prior distribution simple interpretation well useful analytical property motivate prior note likelihood function take form product factor form choose prior proportional power posterior distribution proportional product prior likelihood function functional form prior property conjugacy several example later chapter therefore choose prior beta distribution given beta gamma function beta distribution exercise beta mean variance beta distribution given exercise parameter often control distribution parameter figure show plot beta distribution various value posterior distribution multiplying beta prior binomial likelihood function keeping factor depend posterior distribution form figure plot beta distribution beta given function various value therefore number tail coin example functional dependence prior distribution conjugacy property prior respect like function indeed simply another beta distribution normalization therefore comparison give effect observing data observation observation increase value value going prior distribution posterior distribution provide simple interpretation prior effective number observation respectively note need integer furthermore posterior distribution prior subsequently observe additional data imagine taking observation time observation current posterior prior likelihood function posterior figure illustration step sequential inference prior given beta distribution parameter likelihood function given single observation posterior given beta distribution parameter distribution multiplying likelihood function observation obtain posterior distribution stage posterior beta distribution total number prior actual value given parameter incorporation additional observation simply value whereas observation increment figure step process sequential approach learning naturally adopt viewpoint independent choice prior likelihood function assumption data sequential method make observation time small batch discard next observation used used example learning scenario steady stream data prediction must made data seen require whole data loaded memory sequential method also useful large data set maximum likelihood method also cast sequential framework section goal predict best outcome next trial must evaluate predictive distribution given data product rule probability take form result posterior distribution together result mean beta distribution obtain simple interpretation total fraction observation real prior observation correspond note limit large data result maximum likelihood result shall general property maximum likelihood result agree limit large data data posterior mean always lie prior mean maximum likelihood estimate corresponding relative frequency event given exercise figure number observation increase posterior distribution becomes sharply peaked also seen result variance beta distribution variance go zero fact might wonder whether general property learning observe data uncertainty posterior distribution steadily decrease address take view learning show average property indeed hold consider general inference problem parameter data joint distribution following result exercise say posterior mean distribution generating data equal prior mean similarly show term side prior variance right hand side term average posterior variance second term measure variance posterior mean variance positive quantity result show average posterior variance smaller prior variance reduction variance greater variance posterior mean greater note however result hold average particular data possible posterior variance prior variance
['multinomial', 'variables'] multinomial variable binary variable used describe quantity take possible value often however encounter discrete variable take possible mutually exclusive state although various alternative way express variable shall shortly particularly convenient scheme variable vector element equal element equal instance variable take state particular observation variable correspond state note vector satisfy denote probability parameter distribution given parameter constrained satisfy represent probability distribution generalization distribution outcome easily seen distribution consider data independent observation corresponding likelihood function take form likelihood function data point quantity represent number observation statistic distribution section order maximum likelihood solution need maximize respect taking account constraint must multiplier appendix setting derivative respect zero obtain solve multiplier substituting constraint give thus obtain maximum likelihood solution form fraction observation consider joint distribution quantity conditioned parameter total number observation take form known multinomial distribution normalization number way partitioning object group size given note variable subject constraint
['dirichlet', 'distribution'] distribution introduce family prior distribution parameter multinomial distribution inspection form multinomial distribution conjugate prior given parameter distribution note summation constraint distribution space simplex dimensionality figure form distribution exercise distribution gamma function figure distribution three variable simplex bounded linear manifold form shown consequence constraint plot distribution simplex various setting param shown figure multiplying prior likelihood function obtain posterior distribution parameter form posterior distribution take form distribution indeed conjugate prior multinomial determine normalization comparison case binomial distribution beta prior interpret parameter prior effective number observation note quantity either binary variable peter modest mathematician made contribution number theory astronomy gave rigorous analysis series family name come young person paper brought instant fame concerned mat last theorem claim positive integer solution gave partial proof case sent review turn proof later gave complete proof although full proof last arbitrary wait work wile year century figure plot distribution three variable horizontal ax plane simplex vertical axis value density left plot plot right plot binomial distribution variable multinomial distribution
['gaussian', 'distribution'] distribution also known normal distribution widely used model distribution continuous variable case single variable distribution written form mean variance vector distribution take form mean vector covariance matrix determinant distribution many different context variety different perspective example already seen section single real variable distribution entropy property also exercise another situation distribution consider multiple random variable central limit theorem tell subject certain mild condition random variable course random variable distribution becomes number term increase walker figure histogram plot mean uniformly distributed number various value observe increase distribution towards illustrate considering variable uniform distribution interval considering distribution mean large distribution figure practice convergence increase rapid consequence result binomial distribution distribution observation random binary variable tend figure case distribution many important analytical property shall consider several detail result section rather tech involved section require familiarity various matrix identity however strongly encourage reader become appendix distribution technique prove invaluable understanding complex model later chapter begin considering geometrical form distribution carl gauss said gauss went elementary school teacher trying keep class pupil integer teacher amazement gauss answer matter moment pair added giving problem actually form somewhat harder sequence starting value increment gauss german math scientist reputation perfectionist many show least square derived assumption normally distributed error also early formulation geometry geometrical theory axiom reluctant cuss openly fear reputation might suffer seen geometry point gauss conduct geodetic survey state normal distribution also known death study revealed discovered several mathematical result year even decade fore functional dependence quadratic form exponent quantity distance distance identity matrix distribution constant surface quadratic form constant first note matrix taken symmetric without loss generality component would disappear exponent consider equation covariance matrix exercise real symmetric matrix eigenvalue real chosen form exercise element identity matrix otherwise covariance matrix expressed expansion term tor form exercise similarly inverse covariance matrix expressed substituting quadratic form becomes interpret system vector rotated respect original forming vector figure curve show tical surface constant density space density value major ax ellipse matrix correspond eigenvalue matrix whose row given orthogonal matrix hence also appendix identity matrix quadratic form hence density constant surface constant eigenvalue positive surface represent ellipsoid ax along scaling factor direction ax given figure distribution well necessary eigenvalue covariance matrix strictly positive otherwise properly matrix whose eigenvalue strictly positive said positive chapter encounter distribution eigenvalue zero case distribution singular subspace lower dimensionality eigenvalue covariance matrix said positive consider form distribution system going system matrix element given element matrix property matrix square determinant matrix hence also determinant covariance matrix written product eigenvalue hence thus system distribution take form product independent distribution vector therefore rotated respect joint probability distribution product independent distribution integral distribution system used result normalization indeed look moment distribution thereby provide interpretation parameter expectation distribution given variable note exponent even function component integral taken range term factor vanish symmetry thus refer mean distribution consider second order moment case considered second order moment given gaus second order moment given group together form matrix matrix written variable note vanish symmetry term constant taken outside integral unity distribution consider term make expansion covariance matrix given together completeness write give made equation together fact integral side middle line symmetry unless line made result together thus single random variable mean taking second order variance similarly case convenient subtract mean giving rise covariance random vector case distribution make together result give parameter matrix covariance distribution covariance matrix although distribution widely used density model cant limitation consider number free parameter distribution general symmetric covariance matrix independent parameter another independent parameter exercise parameter total large total number parameter figure contour constant probability density distribution dimension covariance matrix general form diagonal elliptical contour ax proportional identity matrix contour concentric circle therefore quadratically computational task large matrix become prohibitive address prob restricted form covariance matrix consider covariance matrix diagonal total inde pendent parameter density model corresponding contour constant density given ellipsoid could restrict covariance matrix proportional identity matrix known isotropic variance giving independent parameter model spherical surface constant density three possibility general diagonal isotropic matrix figure unfortunately whereas approach limit number degree freedom distribution make inversion covariance matrix much faster operation also greatly restrict form probability density limit ability capture interesting correlation data limitation distribution intrinsically modal single maximum unable provide good approximation multimodal distribution thus distribution sense many parameter also limited range distribution adequately represent later latent variable also hidden variable unobserved variable problem particular rich family multimodal distribution discrete latent variable leading mixture section similarly introduction continuous latent variable chapter lead model number free parameter independently dimensionality data space still model capture dominant correlation data indeed approach combined extended derive rich hierarchical model broad range tical application instance version random section widely used probabilistic model image distribution joint space intensity tractable imposition considerable structure spatial organization similarly linear dynamical system used model time series data application section also joint distribution potentially large number latent variable tractable structure distribution powerful framework form property complex distribution probabilistic graphical model form subject chapter
['conditional', 'gaussian', 'distributions'] conditional distribution important property distribution set variable jointly conditional distribution conditioned similarly marginal distribution either also consider case conditional distribution suppose vector distribution partition joint subset without loss generality take form component component also corresponding partition mean vector given covariance matrix given note symmetry covariance matrix symmetric many situation convenient work inverse matrix known precision matrix fact shall property distribution naturally expressed term covariance whereas take simpler form term precision therefore also introduce partitioned form precision matrix corresponding partitioning vector inverse symmetric matrix also symmetric symmetric exercise point instance simply given inverse fact shall shortly examine relation inverse partitioned matrix inverse partition begin expression conditional distribution product rule probability conditional distribution joint distribution simply value resulting expression obtain valid probability distribution instead normalization explicitly obtain solution considering quadratic form exponent distribution given normalization calculation make partitioning obtain function quadratic form hence conditional distribution completely mean covariance goal identify expression mean covariance inspection example rather common operation associated distribution sometimes square given quadratic form exponent term distribution need determine corresponding mean covariance problem straightforwardly exponent general distribution written term independent made symmetry thus take general quadratic form express form given side immediately equate matrix entering second order term inverse covariance matrix linear term obtain apply procedure conditional distribution quadratic form exponent given denote mean covariance distribution respectively consider functional dependence constant pick term second order immediately conclude covariance inverse precision given consider term linear used discussion general form expression must equal hence made result expressed term partitioned precision matrix original joint distribution also express result term corresponding partitioned covariance matrix make following identity inverse partitioned matrix exercise quantity known complement matrix side respect making obtain following expression mean covariance conditional distribution conditional distribution take simpler form expressed term partitioned precision matrix expressed term partitioned covariance matrix note mean conditional distribution given linear function covariance given independent example model section
['marginal', 'gaussian', 'distributions'] marginal distribution seen joint distribution distribution turn discussion marginal distribution given shall also strategy distribution focus quadratic form exponent joint distribution thereby identify mean covariance marginal distribution quadratic form joint distribution expressed precision matrix form goal integrate easily considering term square order facilitate integration term involve dependence cast standard quadratic form distribution corresponding term side plus term depend depend thus take exponential quadratic form integration take form integration easily integral result reciprocal normalization know form given independent mean determinant covariance matrix thus square respect integrate term contribution side last term side given combining term term depend obtain quantity independent comparison covariance marginal distribution given similarly mean given used covariance expressed term partitioned precision matrix given rewrite term corresponding partitioning covariance matrix given conditional distribution partitioned matrix related making thus obtain intuitively satisfying result marginal distribution mean covariance given marginal distribution mean covariance simply term partitioned covariance matrix contrast conditional distribution partitioned precision matrix give rise simpler sion result marginal conditional distribution partitioned gaus partitioned given joint distribution figure plot left show contour distribution variable plot right show marginal distribution blue curve conditional distribution curve conditional distribution marginal distribution illustrate idea conditional marginal distribution associated example variable figure
['bayes', 'theorem', 'gaussian', 'variables'] theorem variable section considered vector found expression conditional distribution marginal distribution noted mean conditional distribution linear function shall suppose given marginal distribution conditional distribution mean linear function covariance independent example linear model shall study greater generality section wish marginal distribution conditional distribution problem arise frequently subsequent chapter prove convenient derive general result shall take marginal conditional distribution parameter governing mean precision matrix dimensionality dimensionality matrix size first expression joint distribution consider joint distribution term independent quadratic function component hence distribution precision consider second order term written distribution precision inverse covariance matrix given covariance matrix found taking inverse precision done matrix inversion formula give exercise similarly mean distribution identify linear term given result square quadratic form mean given making obtain exercise next expression marginal distribution recall marginal distribution subset random vector take particularly simple form term partitioned covariance matrix mean section covariance given respectively making mean covariance marginal distribution given special case result case mean convolution mean covariance convolution covariance finally seek expression conditional recall result conditional distribution easily expressed term partitioned precision matrix result section conditional distribution mean covariance given evaluation conditional seen example theorem interpret distribution prior distribution variable conditional distribution corresponding posterior distribution found marginal conditional effectively expressed joint distribution form result marginal conditional given marginal distribution conditional given form marginal distribution conditional distribution given given
['maximum', 'likelihood', 'gaussian'] maximum likelihood given data observation drawn independently distribution estimate parameter distribution maximum likelihood hood function given simple rearrangement likelihood function data quantity known statistic distribution derivative likelihood respect given appendix setting derivative zero obtain solution maximum likelihood estimate mean given mean data point maximization respect rather involved approach ignore symmetry constraint show resulting solution symmetric exercise alternative derivation result impose symmetry positive constraint explicitly found result take form result joint maximization respect note solution depend evaluate evaluate evaluate expectation maximum likelihood solution true distribution obtain following result exercise expectation maximum likelihood estimate mean equal true mean however maximum likelihood estimate covariance expectation true value hence correct bias different estimator given clearly expectation equal
['sequential', 'estimation'] sequential estimation discussion maximum likelihood solution parameter gaus distribution convenient opportunity give general discussion topic sequential estimation maximum likelihood sequential method allow data point time important application also large data set involved batch data point infeasible consider result maximum likelihood estimator mean denote based observation figure schematic illustration correlated variable together regression function given expectation algorithm general procedure root function dissect contribution data point obtain result nice interpretation observing data point observe data point obtain estimate moving estimate small amount proportional direction error signal note increase contribution successive data point get smaller result clearly give answer batch result formula equivalent however always able rive sequential algorithm route seek general formulation sequential learning lead algorithm consider pair random variable joint distribution expectation given deterministic function given schematically figure function regression function goal root large data observation could model regression function directly obtain estimate root suppose however observe value time wish corresponding sequential estimation scheme following general procedure problem given shall assume conditional variance shall also without loss generality consider case case figure procedure sequence successive estimate root given value take value represent sequence positive number satisfy condition shown sequence estimate given indeed converge root probability note condition successive correction decrease magnitude process converge limiting value second ensure algorithm converge short root third condition ensure noise variance hence spoil convergence consider general maximum likelihood problem sequentially algorithm maximum like solution stationary point likelihood function hence derivative summation taking limit maximum likelihood solution root regression function therefore apply procedure take form figure case distribution corresponding mean regression function figure take form straight line shown case random variable derivative likelihood function given expectation regression function straight line given root sion function maximum like estimator example consider sequential estimation mean distribution case parameter estimate mean random variable given thus distribution mean substituting obtain form provided choose form note although case single variable technique together restriction apply equally case
['bayesian', 'inference', 'gaussian'] inference maximum likelihood framework gave point estimate parameter develop treatment prior distribution parameter begin simple example consider single random variable shall suppose variance known consider task mean given observation likelihood function probability data given function given emphasize likelihood function probability likelihood function take form exponential quad form thus choose prior given conjugate distribution likelihood function corresponding product exponential quadratic function hence also therefore take prior distribution posterior distribution given simple manipulation square exponent show exercise posterior distribution given maximum likelihood solution given sample mean worth spending moment form posterior mean variance first note mean posterior distribution given compromise prior mean maximum likelihood solution number data point prior mean posterior mean given maximum likelihood solution similarly consider result variance posterior distribution naturally expressed term inverse variance precision furthermore precision additive precision posterior given precision prior plus contribution data precision data point increase number data point precision steadily increase corresponding posterior distribution steadily decreasing variance data point prior variance whereas number data point variance go zero posterior distribution becomes peaked around maximum likelihood solution therefore maximum likelihood result point estimate given precisely formalism limit number observation note also take limit prior variance posterior mean maximum likelihood result posterior variance given figure illustration inference mean variance known curve show prior distribution curve case along posterior distribution given increasing number data point data point mean variance prior chosen mean prior likelihood function variance true value illustrate analysis inference mean distribution figure generalization result case dimensional random variable known covariance unknown mean straightforward exercise already seen maximum likelihood expression mean recast sequential update formula mean section observing data point expressed term mean observing data point together contribution data point fact paradigm lead naturally sequential view inference problem context inference mean write posterior distribution contribution data point term square bracket normalization posterior distribution observing data point prior distribution combined theorem likelihood function associated data point arrive posterior distribution observing data point sequential view inference general problem data assumed independent identically distributed assumed variance distribution data known goal infer mean suppose mean known wish infer variance calculation greatly choose conjugate form prior distribution turn convenient work precision likelihood function take form figure plot gamma distribution various value parameter corresponding conjugate prior therefore proportional product power exponential linear function gamma distribution gamma function correctly gamma distribution integral exercise distribution plotted various value figure mean variance gamma distribution given exercise consider prior distribution multiply likelihood function obtain posterior distribution recognize gamma distribution form maximum likelihood estimator variance note need keep track normalization constant prior likelihood function correct found form gamma distribution effect observing data point increase value thus interpret parameter prior term effective prior observation similarly data point contribute parameter variance interpret parameter prior effective prior observation variance recall made analogous interpretation prior distribution section example exponential family shall interpretation conjugate prior term effective data point general exponential family distribution instead working precision consider variance conjugate prior case inverse gamma distribution although shall discus convenient work precision suppose mean precision unknown conjugate prior consider dependence likelihood function wish identify prior distribution functional dependence likelihood function therefore take form constant since always write inspection particular whose precision linear function gamma prior take form constant given distribution distribution plotted figure note simply product independent prior gamma prior precision linear function even chose prior independent posterior distribution would exhibit coupling precision value figure contour plot distribution parameter value case distribution dimensional variable conjugate prior distribution mean assuming precision known known mean unknown precision matrix conjugate prior distribution given exercise number degree freedom distribution scale matrix trace normalization constant given also possible conjugate prior covariance matrix rather precision matrix lead inverse although shall discus mean precision unknown following similar line reasoning case conjugate prior given known distribution
['student', 'distribution'] student seen conjugate prior precision given gamma distribution together section gamma prior integrate precision obtain marginal distribution form exercise figure plot student various value limit distribution mean precision made change variable convention parameter given term distribution take form known student parameter sometimes precision even though general equal inverse variance parameter degree freedom effect figure particular case distribution limit becomes mean precision exercise student number distribution mean different sion mixture mixture detail section result distribution eral longer tail seen figure give distribution important property robustness mean much sensitive presence data point outlier robustness figure compare maximum likelihood solution note likelihood solution found expectation maximization algorithm effect small number exercise figure illustration robustness student histogram distribution data point drawn distribution together maximum likelihood curve green curve largely hidden curve special case give almost solution data three additional outlying data point showing green curve strongly distorted outlier whereas curve relatively unaffected outlier much cant outlier arise practical application either process data distribution heavy tail simply data robustness also important property regression problem least square approach regression exhibit robustness maximum likelihood conditional distribution regression model distribution obtain robust model back substitute alternative parameter written form generalize obtain student form technique case evaluate integral give exercise dimensionality squared distance form student following property exercise corresponding result case
['periodic', 'variables'] periodic variable although distribution great practical right building block complex probabilistic model situation inappropriate density model continuous vari important case practical application periodic variable example periodic variable would wind direction particular geographical location might instance measure value wind direction number day wish summarize parametric distribution another example calendar time interested quantity periodic hour annual cycle quantity conveniently angular polar might treat periodic variable choosing direction origin conventional distribution approach however would give result strongly dependent arbitrary choice origin suppose instance observation model standard distribution choose origin sample mean data standard deviation whereas choose origin mean standard deviation clearly need develop special approach treatment periodic variable consider problem mean observation periodic variable shall assume measured radian already seen simple average strongly dependent invariant measure mean note observation point unit circle therefore instead unit vector figure average vector figure illustration representation periodic variable dimensional vector living unit circle also shown average vector instead give corresponding angle average clearly ensure location mean independent origin angular note typically inside unit circle observation given write carte sample mean form substituting component give taking ratio identity solve give shortly shall result naturally maximum likelihood estimator appropriately distribution periodic variable consider periodic generalization mi distribution shall limit attention distribution although periodic distribution also found arbitrary dimension extensive discussion periodic distribution convention consider distribution period probability density must integrate figure mi distribution derived considering form whose density contour shown blue unit circle shown must also periodic thus must satisfy three condition integer easily obtain distribution three prop consider distribution variable mean covariance matrix identity matrix contour constant circle figure suppose consider value distribution along circle radius distribution periodic although determine form distribution transforming polar also mean polar writing next substitute transformation condition unit circle interested dependence exponent distribution figure mi distribution plotted different parameter value shown plot left corresponding polar plot right term independent made following trigonometrical identity exercise obtain expression distribution along unit circle form mi distribution circular normal param mean distribution known concentration parameter analogous inverse variance precision normalization expressed term function kind large distribution becomes approximately mi exercise plotted figure function plotted figure consider maximum likelihood estimator parameter mi distribution likelihood function given figure plot function together function setting derivative respect equal zero give solve make trigonometric identity sina obtain exercise recognize result mean space similarly respect making substituted maximum likelihood solution joint optimization function plotted figure making trigonometric write form figure plot faith data blue curve show contour constant density left single distribution data maximum likelihood note distribution capture clump data indeed place much probability mass central region clump data relatively sparse right distribution given linear combination data maximum likelihood technique chap give better data side easily function inverted numerically completeness mention alternative technique periodic distribution approach histogram observation angular divided bin virtue simplicity also cant limitation shall discus histogram method detail section another approach start like mi distribution distribution space onto unit circle rather however lead complex form distribution finally valid distribution real axis turned periodic distribution interval width onto periodic variable wrapping real axis around unit circle resulting distribution complex handle mi distribution limitation mi distribution unimodal forming mixture mi distribution obtain framework periodic variable handle multimodality example machine learn application make mi distribution extension conditional density regression problem bishop
['mixtures', 'gaussians'] mixture distribution important analytical property cant limitation come real data set consider example shown figure known faithful data measurement eruption faithful geyser national park measurement duration appendix figure example mixture distribution dimension showing three scaled blue eruption minute horizontal axis time minute next vertical axis data form dominant clump simple distribution unable capture structure whereas linear superposition give better characterization data superposition formed taking linear combination basic probabilistic model known mixture distribution peel figure linear combination give rise complex density number mean covariance well linear combination almost continuous density arbitrary accuracy therefore consider superposition density form mixture density component mixture mean covariance contour surface plot mixture component shown figure section shall consider component illustrate frame work mixture model generally mixture model comprise linear distribution instance section shall consider mixture distribution example mixture model discrete variable section parameter integrate side respect note individual component obtain also requirement together combining condition obtain figure illustration mixture space contour constant density mixture component component blue green value shown component contour marginal probability density mixture distribution surface plot distribution therefore satisfy requirement product rule marginal density given equivalent view prior prob ability component density probability conditioned shall later chapter role posterior probability also known responsibility theorem given shall discus probabilistic interpretation mixture distribution greater detail chapter form mixture distribution parameter used notation value parameter maximum likelihood likelihood function given immediately situation much complex single presence summation inside logarithm result maximum likelihood solution parameter longer analytical solution approach likelihood function iterative numerical optimization technique fletcher wright bishop employ powerful framework expectation maximization length chapter
['exponential', 'family'] exponential family probability distribution studied chapter exception mixture example broad class exponential family hart smith member exponential family many important property illuminating discus property generality exponential family distribution given parameter distribution form scalar vector discrete continuous natural parameter distribution function function therefore integration summation discrete variable begin taking example distribution chapter showing indeed member exponential family consider distribution side exponential logarithm comparison identify solve give logistic sigmoid function thus write distribution standard representation form used easily proved parison show next consider multinomial distribution single observation take form write standard representation note parameter independent parameter constraint given parameter value parameter circumstance convenient remove constraint distribution term parameter relationship eliminate term thereby leaving parameter note parameter still subject constraint making constraint multinomial distribution becomes identify solve side give function exponential multinomial distribution therefore take form standard form exponential family parameter vector finally consider distribution simple rearrangement cast standard exponential family form exercise
['maximum', 'likelihood', 'sufficient', 'statistics'] maximum likelihood statistic consider problem parameter vector eral exponential family distribution technique maximum hood taking gradient side respect making give used therefore obtain result note covariance expressed term second derivative similarly higher order moment thus provided normalize exercise distribution exponential family always moment simple differentiation consider independent identically distributed data likelihood function given setting gradient respect zero following condition maximum likelihood estimator principle obtain solution maximum likelihood estimator data therefore statistic distribution need store entire data value statistic distribution example function given need keep data point whereas keep consider limit side becomes limit equal true value fact property hold also inference although shall defer discussion chapter tool graphical model thereby gain insight important concept
['conjugate', 'priors'] conjugate prior already concept conjugate prior several time example context distribution conjugate prior beta distribution conjugate prior mean conjugate prior precision distribution general given probability distribution seek prior conjugate likelihood function posterior distribution functional form prior member exponential family conjugate prior written form normalization function pear indeed conjugate multiply prior likelihood function obtain posterior distribution form take functional form prior conjugacy furthermore parameter effective number prior value statistic given
['noninformative', 'priors'] noninformative prior application probabilistic inference prior knowledge conveniently expressed prior distribution example prior zero probability value variable posterior necessarily also assign zero probability value irrespective subsequent observation data many case however little idea form distribution take seek form prior distribution noninformative prior intended little posterior distribution possible smith sometimes data speak distribution parameter might propose prior distribution suitable prior discrete variable state simply amount setting prior probability state case continuous parameter however potential approach domain unbounded prior distribution correctly integral prior improper practice improper prior often used provided corresponding posterior distribution proper correctly instance uniform prior distribution mean posterior distribution mean least data point proper second transformation behaviour probability density nonlinear change variable given function constant change variable also constant however choose density constant density given density constant issue arise maximum likelihood likelihood function simple function free convenient however choose prior distribution constant must take care appropriate representation parameter consider simple example noninformative prior berger first density take form parameter known location parameter family density exhibit translation invariance shift constant give thus density take form variable original density independent choice origin would like choose prior distribution translation invariance property choose prior equal probability mass interval interval must hold choice constant example location parameter would mean distribution seen conjugate prior case obtain noninformative prior taking limit indeed give posterior distribution contribution prior vanish second example consider density form note density provided correctly parameter known scale parameter density exhibit exercise scale invariance scale constant give transformation change scale example meter kilometer length would like choose prior distribution scale invariance consider interval scaled interval prior assign equal probability mass interval thus must hold choice hence note improper prior integral distribution divergent sometimes also convenient think prior distribution scale parameter term density parameter transformation rule density thus prior probability mass range range example scale parameter would standard deviation distribution taken account location parameter often convenient work term precision rather transformation rule density distribution distribution form seen conjugate prior gamma distribution given noninformative prior section special case examine result posterior distribution posterior term data prior
['nonparametric', 'methods'] method throughout chapter probability distribution functional form small number parameter whose value determined data parametric approach density important limitation approach chosen density might poor model distribution data result poor predictive performance instance process data multimodal aspect distribution never necessarily unimodal section consider approach density make assumption form distribution shall focus mainly simple method reader aware however method increasing interest walker neal muller start discussion histogram method density estimation already context marginal conditional distribution figure context central limit theorem figure explore property histogram density model detail case single continuous variable standard histogram simply partition distinct bin width count number observation falling order turn count probability density simply divide total number observation width bin obtain probability value given easily seen give model density constant width often bin chosen width figure illustration histogram approach density estimation data data point distribution shown green curve histogram density estimate based common width shown various value figure show example histogram density estimation data drawn distribution corresponding green curve formed mixture also shown three example density estimate corresponding three different choice width small resulting density model spiky structure present underlying distribution data conversely large bottom result model smooth consequently capture bimodal prop green curve best result intermediate value middle principle histogram density model also dependent choice edge location bin though typically much cant value note histogram method property unlike method cussed shortly histogram data advantageous data large also histogram approach easily applied data point sequentially practice histogram technique useful quick visual data dimension unsuited density estimation application obvious problem density discontinuity edge rather property underlying distribution data another major limitation histogram approach scaling dimensionality divide variable space bin total number bin exponential scaling example curse dimensionality space high dimensional section quantity data provide meaningful estimate local probability density would prohibitive histogram approach density estimation however teach lesson first estimate probability density particular location consider data point within local point note concept locality assume form measure assuming distance histogram property bin natural smooth parameter spatial extent local region case width second value smoothing parameter neither large small order obtain good result reminiscent choice model complexity polynomial curve chapter degree polynomial alternatively value regularization parameter optimal intermediate value neither large small armed insight turn discussion widely used tech density estimation kernel estimator nearest better scaling dimensionality simple histogram model
['kernel', 'density', 'estimators'] kernel density estimator suppose observation drawn unknown density space shall take wish estimate value discussion locality consider small region probability mass associated region given suppose collected data observation drawn data point probability falling within total number point inside distributed according binomial distribution section mean fraction point falling inside region similarly variance around mean large distribution sharply peaked around mean however also assume region small probability density roughly constant region volume combining obtain density estimate form note validity contradictory assumption namely region small density approximately constant region large relation value density number point falling inside region binomial distribution sharply peaked exploit result different way either determine value data give rise technique shortly determine data rise kernel approach shown density estimator kernel density estimator converge true probability density limit provided shrink suitably hart begin kernel method detail start take region small hypercube point wish determine probability density order count number point falling within region convenient following function otherwise unit cube origin function example kernel function context also window quantity data point lie inside cube side zero otherwise total number data point lying inside cube therefore substituting expression give following result density used volume hypercube side symmetry function reinterpret equation single cube cube data point stand kernel density estimator suffer problem histogram method namely presence discontinuity case boundary cube obtain smoother density model choose smoother kernel function common choice give rise following kernel density model standard deviation component thus density model data point contribution whole data dividing correctly figure apply model data figure illustration kernel density model applied data used demonstrate histogram approach figure act smoothing parameter small panel result noisy density model whereas large bottom panel bimodal nature underlying distribution data shown green curve washed best model value middle panel used demonstrate histogram technique parameter play role smoothing parameter sensitivity noise small large optimization problem model complexity analogous choice width histogram density estimation degree polynomial used curve choose kernel function subject ensure resulting probability distribution everywhere class density model given kernel density estimator estimator great merit involved training phase simply storage training however also great weakness cost density linearly size data
['nearest', 'neighbour', 'methods'] method kernel approach density estimation parameter governing kernel width kernel region high data density large value lead washing structure might otherwise extracted data however reducing lead noisy estimate elsewhere data space density smaller thus optimal choice dependent location within data space issue method density estimation therefore return general result local density estimation instead value data consider value data appropriate value consider small sphere point wish estimate figure illustration estimation data figure parameter degree smoothing small value lead noisy density model panel whereas large value panel bimodal true distribution shown green curve data density allow radius sphere grow precisely data point estimate density given volume resulting sphere technique known nearest figure various choice parameter data used figure figure value degree smoothing optimum choice neither large small note model produced nearest true density model integral space exercise close chapter showing technique density estimation extended problem cation apply density estimation technique class separately make theorem suppose data point class point total wish point draw sphere precisely point irrespective class suppose sphere volume point class estimate density associated class similarly unconditional density given class prior given combine theorem obtain posterior probability class membership figure point shown black diamond according majority class membership train data point case approach cation resulting decision boundary composed form perpendicular pair point different class wish minimize probability cation done test point class posterior probability corresponding value thus point identify nearest point training data assign point class number representative amongst tie broken random particular case rule test point simply assigned class nearest point training concept figure figure show result data chapter various value control degree smoothing small produce many small region class whereas large lead region figure plot data point data showing value plotted green blue point correspond laminar annular homogeneous class respectively also shown cation input space given algorithm various value interesting property limit error rate never twice minimum achievable error rate optimal us true class distribution cover hart method kernel estimator require entire training data leading expensive computation data large effect offset expense additional computation search structure allow approximate near found without exhaustive search data nevertheless method still severely limited hand seen simple parametric model restricted term form distribution represent therefore need density model complexity model independently size training shall subsequent chapter achieve exercise verify distribution following prop show entropy distributed random binary variable given form distribution given symmetric tween value situation convenient equivalent formulation case distribution written show distribution evaluate mean variance entropy exercise prove binomial distribution first number combination identical object chosen total show result prove induction following result known binomial theorem valid real value finally show binomial distribution done factor summation making binomial theorem show mean binomial distribution given differentiate side normalization condition respect rearrange obtain expression mean similarly twice respect making result mean binomial distribution prove result variance binomial exercise prove beta distribution given correctly hold equivalent showing gamma function expression prove first bring integral inside integrand integral next make change variable interchange order integration make change variable make result show mean variance mode beta distribution given respectively mode consider binomial random variable given prior distribution given beta distribution suppose occur occurrence show posterior mean value lie prior mean maximum likelihood estimate show posterior mean written time prior mean plus time maximum likelihood estimate posterior distribution compromise prior distribution maximum likelihood solution consider variable joint distribution prove follow result vary expectation conditional distribution similar notation conditional variance exercise prove normalization induction already shown exercise beta distribution special case assume distribution variable prove variable consider distribution variable take account constraint written goal expression integrate taking care limit integration make change variable integral limit assuming correct result making derive expression property gamma function derive following result mean variance covariance distribution given expectation distribution derivative respect show given digamma function uniform distribution continuous variable verify distribution expression mean variance evaluate divergence exercise distribution entropy given covariance entropy distribution given wish maximize distribution subject constraint mean covariance variational maximization multiplier enforce constraint show maximum likelihood distribution given show entropy given dimensionality consider random variable mean precision respectively derive expression differential entropy variable distribution relation square exponent observe convolution distribution make result entropy consider distribution given writing precision matrix inverse covariance matrix metric matrix show term appear exponent hence precision matrix taken symmetric without loss generality inverse symmetric matrix also symmetric exercise covariance matrix also chosen symmetric without loss generality consider real symmetric matrix whose eigenvalue equation given taking complex conjugate equation original equation forming inner product show eigenvalue real similarly symmetry property show orthogonal provided finally show without loss generality chosen satisfy even eigenvalue zero show real symmetric matrix equation expressed expansion given eigenvalue form similarly show inverse matrix representation form positive matrix quadratic form positive real value vector show necessary condition positive eigenvalue positive show real symmetric matrix size independent parameter show inverse symmetric matrix symmetric system expansion show volume within corresponding constant distance given volume unit sphere dimension distance prove identity multiplying side matrix making section considered conditional marginal generally consider partitioning component three group corresponding mean vector covariance matrix form making result section expression conditional distribution useful result linear algebra matrix inversion formula given multiplying side prove correctness result independent random vector show mean given mean variable separately similarly show covariance matrix given covariance matrix result exercise consider joint distribution variable whose mean covariance given respectively result show marginal distribution given similarly making result show conditional distribution given partitioned matrix inversion formula show inverse precision matrix given covariance matrix starting making result verify result consider multidimensional random vector distribution respectively together result expression marginal distribution considering model product marginal distribution conditional distribution exercise next provide practice quadratic form arise model well giving dent check result derived main text consider joint distribution marginal conditional distribution given examining quadratic form exponent joint distribution technique square section expression mean covariance marginal distribution variable make matrix inversion formula verify result agree result chapter consider joint distribution exercise technique square expression mean covariance conditional distribution verify agree expression maximum likelihood solution covariance matrix need maximize likelihood function respect covariance matrix must symmetric positive proceed constraint straightforward maximization result appendix show covariance matrix likelihood function given sample covariance note result necessarily symmetric positive provided sample covariance nonsingular result prove result show data point distribution mean covariance element identity matrix hence prove result analogous procedure used obtain derive expression sequential estimation variance distribution starting maximum likelihood expression verify substituting expression distribution sequential estimation formula give result form hence obtain expression corresponding analogous procedure used obtain derive pression sequential estimation covariance distribution starting maximum likelihood expression verify substituting expression distribution estimation formula give result form hence obtain expression corresponding technique square quadratic form derive result starting result posterior distribution mean random variable dissect contribution data point hence obtain expression sequential update derive result starting posterior distribution multiplying likelihood square obtain posterior distribution observation consider random variable covariance known wish infer mean observation given prior distribution corresponding posterior distribution gamma function show gamma evaluate mean variance mode gamma distribution following distribution generalization distribution show distribution consider regression model target variable given random noise variable drawn distribution show likelihood function data input vector corresponding target variable given term independent note function error function considered section consider distribution conjugate prior given data observation show posterior distribution also functional form prior write expression parameter posterior distribution verify distribution indeed conjugate prior precision matrix verify integral lead result show limit becomes hint ignore normalization simply look following analogous step used derive student verify result form dent variable show integration variable correctly student convolution gamma distribution verify property show limit student mean precision various trigonometric identity used discussion periodic variable chapter proven easily relation square root minus considering identity prove result similarly identity real part prove finally sina imaginary part prove result large mi distribution becomes sharply peaked around mode making cosine function given show mi distribution trigonometric identity show solution given second derivative mi distribution show maximum distribution minimum making result together trigonometric identity show maximum likelihood solution mi distribution radius mean observation unit vector plane figure express beta distribution gamma distribution mi distribution member exponential family thereby identify natural parameter verify distribution cast exponential family form derive expression analogous result negative gradient family given expectation taking second derivative show variable show density correctly provided correctly consider density model space region density take constant value region volume region suppose observation observation fall region multiplier enforce normalization constraint density derive expression maximum likelihood estimator show density model improper whose integral space divergent model regression focus book unsupervised learning topic density estimation data clustering turn discussion super learning starting regression goal regression predict value continuous target variable given value input variable already example regression problem considered polynomial curve chapter polynomial example broad class function linear regression model share property linear function adjustable parameter form focus chapter form linear regression model also linear function input variable however obtain much useful class function taking linear combination nonlinear function input variable known basis function model linear function parameter give simple analytical property nonlinear respect input variable given training data observation together corresponding target value goal predict value value approach done directly appropriate function whose value input constitute prediction corresponding value generally probabilistic perspective model predictive distribution express uncertainty value value conditional make prediction value minimize value suitably chosen loss function common choice loss function variable squared loss optimal solution given conditional expectation although linear model cant limitation practical technique pattern recognition particularly problem input space high nice analytical property form foundation model later chapter
['linear', 'basis', 'function', 'models'] linear basis function model linear model regression linear combination input variable often simply known linear regression property model linear function parameter also however linear function input variable cant limitation model therefore extend class model considering linear combination nonlinear function input variable form known basis function maximum value index total number parameter model parameter offset data sometimes bias parameter confused bias statistical sense often convenient additional dummy basis function many practical plication pattern recognition apply form feature extraction original data variable original variable vector feature expressed term basis function nonlinear basis function allow function linear function input vector function form linear model however function linear linearity greatly simplify analysis class model however also lead cant limitation discus section example polynomial regression considered chapter particular example model single input variable basis take form power limitation polynomial basis function global function input variable change region input space affect region resolved dividing input space region different polynomial region leading spline function many possible choice basis function example govern location basis function input space spatial scale usually basis function although noted prob interpretation particular normalization unimportant basis function adaptive parameter another possibility sigmoidal basis function form logistic sigmoid function equivalently tanh function related logistic sigmoid tanha general linear combination logistic sigmoid function equivalent general linear combination tanh function various choice basis function figure another possible choice basis function basis lead expansion sinusoidal function basis function spatial extent contrast basis function region input space necessarily comprise spectrum different spatial frequency many signal application interest consider function space frequency leading class function known wavelet also mutually orthogonal simplify application wavelet applicable input value live figure example basis function showing polynomial left form sigmoidal form right regular lattice successive time point temporal sequence image useful text wavelet include discussion chapter however independent particular choice basis function discussion shall specify particular form basis function except purpose numerical lustration indeed much discussion equally applicable situation vector basis function simply identity order keep notation simple shall focus case single target variable however section consider cation deal multiple target variable
['maximum', 'likelihood', 'least', 'squares'] maximum likelihood least square chapter polynomial function data set error function also error function could maximum likelihood solution assumed noise model return discussion consider least square approach relation maximum likelihood detail assume target variable given deterministic additive noise zero mean random variable precision inverse variance thus write recall assume squared loss function optimal prediction value given conditional mean target variable section case conditional distribution form conditional mean simply note noise assumption conditional distribution given unimodal inappropriate application tension mixture conditional distribution permit multimodal conditional distribution section consider data input corresponding target value group target variable column vector denote chosen distinguish single observation target would making assumption data point drawn independently distribution obtain following expression likelihood function function adjustable parameter form used note learning problem sion cation seeking model distribution input variable thus always appear variable drop explicit expression keep notation uncluttered taking logarithm likelihood function making standard form error function written likelihood function maximum likelihood determine consider maximization respect already section maximization likelihood function conditional noise distribution linear model equivalent error function given gradient likelihood function take form setting gradient zero give obtain known normal equation least square problem matrix design matrix whose element given quantity known matrix mitra loan generalization notion matrix inverse matrix indeed square invertible property point gain insight role bias parameter make bias parameter explicit error function becomes setting derivative respect equal zero obtain thus bias difference average training target value weighted average basis function value also maximize likelihood function respect noise precision parameter giving figure geometrical interpretation solution space whose ax value regression function projection data vector onto subspace basis function basis function length element inverse noise precision given residual variance target value around regression function
['geometry', 'least', 'squares'] geometry least square point instructive consider geometrical interpretation solution consider space whose ax given vector space basis function data point also vector space figure note column whereas number basis function smaller number data point vector span linear subspace dimensionality vector whose element given arbitrary linear combination vector live anywhere subspace error equal factor squared distance thus solution choice lie subspace intuitively figure anticipate solution orthogonal projection onto subspace indeed case easily veri solution given take form orthogonal projection exercise practice direct solution normal equation lead numerical close singular particular basis vector colinear nearly resulting parameter value large magnitude near degeneracy uncommon dealing real data set resulting numerical technique singular value decomposition press bishop note addition regularization term matrix singular even presence degeneracy
['sequential', 'learning'] sequential learning batch technique maximum likelihood solution entire training costly large data set chapter data large sequential algorithm also known algorithm data point considered time model parameter presentation sequential learning also appropriate real time application data observation continuous stream prediction must made data point seen obtain sequential learning algorithm technique stochastic gradient descent also known sequential gradient descent error function data point pattern stochastic gradient descent algorithm update parameter vector iteration number learning rate parameter shall discus choice value shortly value starting vector case error function give known algorithm value need chosen care ensure algorithm bishop
['regularized', 'least', 'squares'] least square section idea regularization term error function order control total error function take form regularization control relative importance error regularization term form regularizer given weight vector also consider error function given total error function becomes particular choice regularizer known machine learning literature weight decay sequential learning algorithm weight value decay towards zero unless data statistic ample parameter shrinkage method shrink parameter value towards figure contour regularization term various value parameter zero advantage error function remains quadratic function exact minimizer found closed form setting gradient respect zero obtain simple extension solution general regularizer sometimes used error take form quadratic regularizer figure show tour regularization function different value case know lasso statistic literature property large driven zero leading sparse model corresponding basis function play role note equivalent error subject constraint exercise appropriate value parameter approach related multiplier origin sparsity seen figure appendix show minimum error function subject constraint increasing number parameter driven zero regularization complex model trained data set limited size without severe essentially limiting effective model complexity however problem optimal model complexity appropriate number basis function suitable value regularization shall return issue model complexity later chapter figure plot contour error function blue along constraint quadratic regular left lasso regularizer right optimum value vector lasso give sparse solution remainder chapter shall focus quadratic regularizer practical importance analytical tractability
['multiple', 'outputs'] multiple output considered case single target variable wish predict target variable denote collectively target vector could done different basis component leading multiple independent regression problem however interesting common approach basis function model component target vector column vector matrix parameter column vector element suppose take conditional distribution target vector isotropic form observation combine matrix size given similarly combine input vector matrix likelihood function given maximize function respect giving examine result target variable column vector component thus solution regression problem different target variable need compute single matrix vector extension general noise distribution arbitrary matrix straightforward lead inde exercise pendent regression problem result unsurprising parameter mean noise distribution know maximum likelihood solution mean gaus independent covariance shall therefore consider single target variable simplicity
['bias', 'variance', 'decomposition'] decomposition discussion linear model regression assumed form number basis function seen chapter maximum likelihood equivalently least square lead severe complex model trained data set limited size however limiting number basis function order avoid side effect limiting model capture interesting important trend data although introduction regularization term control model many parameter raise question determine suitable value regularization seeking solution error function respect weight vector regularization clearly right approach since lead solution seen chapter phenomenon really unfortunate property maximum likelihood arise marginalize parameter setting chapter shall consider view model complexity depth however instructive consider viewpoint model complexity issue known bias variance although shall introduce concept context linear basis function model easy illustrate idea simple example discussion general applicability section decision theory regression problem considered various loss function lead corresponding optimal prediction given conditional distribution popular choice squared loss function optimal prediction given conditional expectation denote given point worth distinguishing squared loss function decision theory error function arose likelihood estimation model parameter might sophisticated technique least square example regularization fully determine conditional distribution combined squared loss function purpose making prediction section squared loss written form recall second term independent intrinsic noise data minimum achievable value loss term choice function seek make term minimum hope make term zero unlimited supply data unlimited computational resource could principle sion function desired degree accuracy would represent optimal choice however practice data number data point consequently know regression function exactly model parametric function vector perspective uncertainty model expressed posterior distribution treatment however making point estimate based data try instead interpret uncertainty estimate following thought suppose large number data set size drawn independently distribution given data learning algorithm obtain prediction function different data set ensemble give different function consequently different value squared loss performance particular learning algorithm assessed taking average ensemble data set consider integrand term particular data take form quantity dependent particular data take aver ensemble data set subtract quantity inside brace expand obtain take expectation expression respect note term vanish giving bias variance squared difference regression function expressed term term squared bias extent average prediction data set desired regression function second term variance measure extent solution individual data set vary around average hence measure extent function sensitive particular choice data shall provide intuition support shortly consider simple example considered single input value substitute expansion back obtain following decomposition squared loss loss bias variance noise bias variance noise bias variance term refer quantity goal minimize loss decomposed squared bias variance constant noise term shall bias variance model bias high variance relatively rigid model high bias variance model optimal predictive capability lead best balance bias variance considering sinusoidal data chapter generate data set appendix data point independently sinusoidal curve data set indexed data figure illustration dependence bias variance model complexity parameter sinusoidal data chapter data set data point basis function model total number parameter bias parameter left column show result model data set various value clarity shown right column show corresponding average along sinusoidal function data set green figure plot squared bias variance together correspond result shown also shown average test error test data size point minimum value bias variance around close value give minimum error test data bias variance bias variance test error model basis function error function give prediction function shown figure large value regularization give variance curve left plot look similar high bias curve right plot different conversely bottom small large variance shown high variability curve left plot bias shown good average model original sinusoidal function note result many solution complex model good regression function bene procedure indeed weighted multiple solution lie heart approach although respect posterior distribution parameter respect multiple data set also examine quantitatively example average prediction squared bias variance given bias variance integral weighted distribution data point drawn distribution quantity along plotted function figure small value allow model become tuned noise individual data leading large variance conversely large value pull weight parameter towards zero leading large bias although decomposition provide interesting sight model complexity issue perspective practical value decomposition based average respect ensemble data set whereas practice single data large number independent training set given size would better combining single large training course would reduce level given model complexity given limitation turn next section treatment linear basis function model powerful insight issue also lead practical technique question model complexity
['bayesian', 'linear', 'regression'] linear regression discussion maximum likelihood setting parameter linear model seen effective model complexity number basis function need according size data regularization term likelihood function mean effective model complexity value regularization although choice number form basis function course still important overall behaviour model leaf issue appropriate model complexity problem decided simply likelihood always lead excessively complex model dependent holdout data used determine model complexity section expensive wasteful able data therefore turn treatment linear regression avoid problem maximum likelihood also lead automatic method model complexity training data alone simplicity focus case single target variable tension multiple target variable straightforward discussion section
['parameter', 'distribution'] parameter distribution begin discussion treatment linear regression prior probability distribution model parameter shall treat noise precision parameter known constant first note likelihood function exponential quadratic function corresponding conjugate prior therefore given distribution form mean covariance next compute posterior distribution proportional product likelihood function prior choice conjugate gaus prior distribution posterior also evaluate distribution usual procedure square exponential normalization standard result however already done necessary work exercise eral result write posterior distribution directly form note posterior distribution mode mean thus maximum posterior weight vector simply given consider broad prior mean posterior distribution maximum likelihood value given similarly posterior distribution prior furthermore data point arrive sequentially posterior distribution stage act prior distribution subsequent data point posterior distribution given exercise remainder chapter shall consider particular form gaus prior order simplify treatment consider isotropic single precision parameter corresponding posterior distribution given posterior distribution given likelihood prior function take form maximization posterior distribution respect therefore lent minimization error function addition quadratic regularization term corresponding illustrate learning linear basis function model well sequential update posterior distribution simple example consider single input variable single target variable linear model form parameter plot prior posterior distribution directly parameter space generate synthetic data function param value choosing value uniform distribution noise standard deviation obtain target value goal recover value data explore dependence size data assume noise variance known hence precision parameter true value similarly parameter shall shortly discus strategy training data figure show result learning model size data sequential nature learning current posterior distribution form prior data point worth taking time study detail several important aspect inference situation data point show plot prior distribution space together sample function value drawn prior second situation observing single data point location data point shown blue circle column column plot likelihood function data point function note likelihood function soft constraint line must close data point close determined noise precision comparison true parameter value used generate data shown white cross plot left column figure multiply likelihood function prior normalize obtain posterior distribution shown middle plot second regression function drawing sample posterior distribution shown plot note sample line close data point third show effect serving second data point shown blue circle plot column corresponding likelihood function second data point alone shown left plot multiply likelihood function posterior distribution second obtain posterior distribution shown middle plot third note exactly posterior distribution would combining original prior likelihood function data point posterior data point point line already give relatively compact posterior distribution sample posterior distribution give rise function shown third column function close data point fourth show effect observing total data point plot show likelihood function data point alone middle plot show resulting posterior distribution absorbed information observation note posterior much sharper third limit number data point figure illustration sequential learning simple linear model form detailed description given text posterior distribution would become delta function true parameter value shown white cross form prior parameter considered instance generalize prior give distribution case prior conjugate likelihood function finding maximum distribution minimization error function case prior mode posterior distribution equal mean although longer hold
['predictive', 'distribution'] predictive distribution practice usually interested value rather making prediction value evaluate predictive distribution vector target value training corresponding input vector side statement simplify notation conditional distribution target vari able given posterior weight distribution given convolution distribution making result section predictive distribution take form exercise variance predictive distribution given term noise data whereas second term uncertainty associated parameter noise process distribution independent variance additive note additional data point posterior distribution becomes narrower consequence shown limit second term go zero variance exercise predictive distribution solely additive noise parameter illustration predictive distribution linear regression model return synthetic sinusoidal data section figure figure example predictive distribution model basis function form synthetic sinusoidal data section text detailed discussion model linear combination basis function data set various size look corresponding posterior distribution green curve correspond function data point addition noise data set size shown four plot blue circle plot curve show mean corresponding predictive distribution shaded region span standard deviation either side mean note predictive uncertainty data point also note level uncertainty decrease data point plot figure show pointwise predictive variance order gain insight covariance prediction different value draw sample posterior distribution plot corresponding function shown figure figure plot function sample posterior distribution corresponding plot figure used basis function region away basis function contribution second term variance zero leaving noise contribution thus model becomes dent prediction outside region basis function generally undesirable behaviour problem alternative approach known process section note unknown introduce conjugate prior distribution discussion section given distribution case exercise predictive distribution student exercise figure equivalent basis function figure shown plot versus together three slice matrix three different value data used generate kernel comprised value equally spaced interval
['equivalent', 'kernel'] equivalent kernel posterior mean solution linear basis function model interpretation stage kernel method process substitute expression predictive chapter mean written form thus mean predictive distribution point given linear combination training target variable write function known smoother matrix equivalent kernel regression function make prediction taking linear combination training target value known linear smoother note equivalent kernel input value data appear equivalent kernel case basis function figure kernel function plotted function three different value around mean predictive distribution given forming weighted combination target value data point close given higher weight point removed intuitively reasonable weight local evidence strongly distant evidence note localization property hold basis function also nonlocal polynomial sigmoidal basis function figure figure example lent kernel plotted function left polynomial function right basis function shown note local function even though corresponding basis function nonlocal insight role equivalent kernel covariance given made form equivalent kernel predictive mean nearby point highly correlated whereas distant pair point correlation smaller predictive distribution shown figure visualize point wise uncertainty prediction however drawing posterior distribution plotting corresponding model function figure joint uncertainty posterior distribution value value equivalent kernel formulation linear regression term kernel function alternative approach regression instead basis function implicitly equivalent kernel instead kernel directly make prediction input vector given training lead practical framework regression cation process detail section seen effective kernel weight training target value combined order make prediction value shown weight word value intuitively pleasing result easily proven informally exercise summation equivalent considering predictive mean target data provided basis function linearly independent data point basis function basis function constant corresponding bias parameter clear training data exactly hence predictive mean simply obtain note kernel function negative well positive although summation constraint corresponding prediction necessarily convex combination training target variable finally note equivalent kernel important property kernel function general namely expressed form chapter inner product respect vector nonlinear function
['bayesian', 'model', 'comparison'] model comparison chapter problem well cross validation technique setting value regularization parameter choosing alternative model consider problem model lection perspective section discussion general section shall idea applied determination regularization parameter linear regression shall associated maximum likelihood model parameter stead making point estimate value model training data without need validation available data used training multiple training run model associated also multiple complexity determined simultaneously part training process example chapter shall introduce relevance vector machine model complexity parameter every training data point view model comparison simply probability represent uncertainty choice model along consistent application product rule probability suppose wish compare model model probability distribution data case polynomial curve problem distribution target value input value assumed known type model joint distribution shall suppose data model section uncertain uncertainty expressed prior probability distribution given training wish evaluate posterior distribution prior express preference different model simply assume model given equal prior probability interesting term model evidence express preference shown data different model shall examine term detail shortly model evidence sometimes also marginal likelihood likelihood function space model parameter ratio model evidence model known factor ka know posterior distribution model predictive distribution given product rule example mixture distribution overall predictive predictive distribution individual model weighted posterior probability model stance model aposteriori equally likely narrow distribution around narrow distribution around overall predictive distribution bimodal distribution mode single model simple approximation model single probable model alone make prediction known model selection model parameter model evidence given product rule probability sampling perspective marginal likelihood chapter generating data model whose parameter random prior also interesting note evidence precisely term denominator theorem posterior distribution parameter obtain insight model evidence making simple integral parameter consider case model single parameter posterior distribution parameter proportional omit dependence model keep notation uncluttered assume posterior distribution sharply peaked around probable value width approximate value integrand maximum time width peak assume prior width figure obtain rough approximation model evidence assume posterior distribution sharply peaked around mode taking log obtain approximation figure term data given probable parameter value prior would correspond likelihood second term model according complexity term negative increase magnitude ratio get smaller thus parameter tuned data posterior distribution penalty term large model parameter make similar approximation parameter turn assuming parameter ratio obtain thus simple approximation size complexity penalty increase linearly number adaptive parameter model increase complexity model term typically decrease complex model better able data whereas second term increase dependence optimal model complexity determined maximum evidence given term shall later develop version approximation based approximation posterior distribution section gain insight model comparison understand marginal likelihood model intermediate complexity figure horizontal axis representation space possible data set point axis data consider three model successively increasing complexity imagine running model generatively produce exam data set looking distribution data set result given figure schematic illustration distribution data set three model different complex note example data model complexity evidence model generate variety different data set since parameter prior probability distribution choice parameter random noise target variable generate particular data model choose value parameter prior distribution parameter value sample data model example based order polynomial little variability generate data set fairly similar distribution therefore relatively small region horizontal axis contrast complex model ninth order polynomial generate great variety different data set distribution spread large region space data set distribution particular data highest value evidence model intermediate complexity essentially simpler model data well whereas complex model spread predictive probability broad range data set relatively small probability implicit model comparison framework assumption true distribution data within model consideration provided show model comparison average correct model consider model truth given data possible factor incorrect model however average factor distribution data set obtain factor form average taken respect true distribution data quantity example divergence prop section always positive unless distribution equal case zero thus average factor always correct model seen framework problem model basis training data alone however approach like approach pattern recognition need make form model invalid result misleading particular figure model evidence sensitive many aspect prior behaviour tail indeed evidence prior improper seen improper prior arbitrary scaling factor word normalization distribution sider proper prior take suitable limit order obtain improper prior example prior take limit variance evidence zero seen figure however possible consider evidence ratio model take limit obtain meaningful answer practical application therefore wise keep aside independent test data evaluate overall performance system
['evidence', 'approximation'] evidence approximation fully treatment linear basis function model would prior distribution make prediction respect well respect parameter however although integrate analytically either complete variable analytically intractable discus approximation value determined marginal hood function parameter framework known statistic literature empirical smith type maximum likelihood berger generalized maximum likelihood machine learning literature also evidence approximation gull introduce predictive distribution given given respectively dependence input variable keep notation uncluttered posterior distribution sharply peaked around value predictive distribution simply value theorem posterior distribution given prior relatively evidence framework value marginal likelihood function shall proceed marginal likelihood linear basis function model maximum allow determine value training data alone without recourse recall ratio analogous regularization parameter aside worth conjugate gamma prior analytically give student although resulting integral longer analytically tractable might thought integral example approximation section based local mode posterior distribution might provide practical alternative evidence framework however integrand function typically strongly skewed mode approximation capture bulk probability mass leading evidence evidence framework note approach take maximization evidence evaluate evidence function analytically derivative equal zero obtain equation shall section alternatively technique expectation maximization algorithm cussed section shall also show approach converge solution
['evaluation', 'evidence', 'function'] evaluation evidence function marginal likelihood function weight parameter evaluate integral make result conditional distribution model shall evaluate exercise integral instead square exponent making standard form normalization write evidence function form exercise dimensionality recognize equal constant proportionality error function complete square exercise giving together note matrix second derivative error function known matrix also given hence equivalent previous therefore mean posterior distribution integral simply appealing standard result normalization giving exercise write marginal likelihood form expression evidence function polynomial regression problem plot model evidence order polynomial shown figure assumed prior form parameter form plot instructive back figure polynomial poor data consequently give relatively value figure plot model evidence versus order polynomial model showing evidence model evidence going polynomial greatly data hence evidence higher however going data marginally fact underlying sinusoidal function data function even term polynomial expansion indeed figure show residual data error reduced slightly going model greater complexity penalty evidence actually fall going obtain cant improvement data seen figure evidence giving highest overall evidence polynomial increase value produce small improvement data suffer increasing complexity penalty leading overall decrease evidence value looking figure generalization error roughly constant would cult choose model basis plot alone evidence value however show clear preference since model give good explanation data
['maximizing', 'evidence', 'function'] evidence function consider maximization respect done following equation eigenvalue consider term respect thus stationary point respect satisfy multiplying obtain since term quantity written interpretation quantity shortly value marginal likelihood exercise note implicit solution also mode posterior distribution choice therefore adopt iterative procedure make initial choice given also evaluate given value used process repeated convergence note matrix compute eigenvalue start simply multiply obtain value determined purely look training data contrast maximum likelihood method independent data order optimize model complexity similarly maximize marginal likelihood respect note eigenvalue proportional hence giving stationary point marginal likelihood therefore obtain exercise implicit solution choosing initial value calculate convergence determined data value together update figure contour likelihood function prior green ax parameter space rotated align mode given maximum likelihood solution whereas nonzero mode direction eigenvalue small quantity close zero corresponding value also close zero contrast direction eigenvalue large quantity close unity value close maximum likelihood value
['effective', 'number', 'parameters'] effective number parameter result elegant interpretation insight solution consider contour like function prior figure implicitly rotated ax parameter space tor contour likelihood function ellipsis eigenvalue measure curvature likelihood function figure eigenvalue small smaller curvature greater elongation contour likelihood positive matrix positive eigenvalue ratio consequently quantity range direction corresponding parameter close maximum likelihood value ratio close parameter well determined value tightly constrained data conversely direction corresponding parameter close zero ratio direction likelihood function relatively insensitive parameter value parameter small value prior quantity therefore measure effective total number well determined parameter obtain insight result paring corresponding maximum likelihood result given formula express variance inverse precision average squared difference target model prediction however differ number data point denominator maximum like result result recall maximum likelihood estimate variance distribution single variable given estimate maximum likelihood solution mean noise data effect used degree freedom model corresponding unbiased estimate given take form shall section result treat marginalize unknown mean factor denominator result take account fact degree free used mean remove bias maximum likelihood consider corresponding result linear regression model mean target distribution given function parameter however parameter tuned data effective number parameter determined data parameter small value prior result variance factor denominator thereby correcting bias maximum likelihood result illustrate evidence framework setting sinusoidal synthetic data section together basis model basis function total number parameter model given bias simplicity true value used evidence framework determine shown figure also parameter control magnitude parameter plotting individual parameter versus effective number param shown figure consider limit number data point large relation number parameter parameter well determined data implicit data point eigenvalue increase size data case equation become respectively result used approximation full evidence figure left plot show curve blue curve versus sinusoidal synthetic data intersection curve optimum value given evidence procedure right plot show corresponding graph evidence versus curve showing peak crossing point curve left plot also shown test error blue curve showing evidence maximum close point best generalization formula require evaluation eigenvalue spectrum figure plot parameter basis function model versus effective parameter varied range causing vary range
['limitations', 'fixed', 'basis', 'functions'] limitation fixed basis function throughout chapter model linear nonlinear basis function seen assumption linearity parameter range useful property solution problem well tractable treatment furthermore suitable choice basis function model arbitrary input variable target next chapter shall study anal class model cation might appear therefore linear model constitute general purpose framework problem pattern recognition unfortunately cant shortcoming linear model cause turn later chapter complex model support vector machine neural network stem assumption basis function training data manifestation curse section consequence number basis function need grow rapidly often exponentially dimensionality input space fortunately property real data set exploit help alleviate problem first data vector typically close linear manifold whose intrinsic dimensionality smaller input space result strong correlation input variable example consider image digit chapter basis function arrange scattered input space region data approach used radial basis function network also support vector relevance vector machine neural network model adaptive basis function sigmoidal adapt parameter region input space basis function vary data manifold second property target variable cant dependence small number possible direction within data manifold neural network exploit property choosing direction input space basis function respond exercise show tanh function logistic sigmoid function related tanha hence show general linear combination logistic sigmoid function form equivalent linear combination tanh function form tanh expression relate parameter original show matrix take vector project onto space column result show solution orthogonal projection vector onto manifold shown figure consider data data point associated weighting factor error function becomes find expression solution error function give alternative interpretation weighted error function term data dependent noise variance replicated data point consider linear model form together error function form suppose noise zero mean variance added dependently input variable making show noise distribution equivalent error input variable addition regularization term bias parameter regularizer technique multiplier appendix show minimization error function equivalent error subject constraint discus relationship parameter consider linear basis function regression model target variable distribution form together training data input basis vector target vector show maximum likelihood solution parameter matrix property column given expression form solution isotropic noise distribution note independent covariance matrix show maximum likelihood solution given technique square verify result posterior distribution parameter linear basis function model respectively consider linear basis function model section suppose already data point posterior distribution given posterior prior next considering additional data point square exponential show resulting posterior distribution given repeat previous exercise instead square hand make general result model given making result evaluate integral verify predictive distribution linear regression model given variance given seen size data increase uncertainty associated posterior distribution model parameter decrease make matrix identity appendix show uncertainty associated linear regression function given section conjugate prior distribution unknown mean unknown precision inverse variance distribution property also hold case conditional linear regression model consider likelihood function conjugate prior given show corresponding posterior distribution take functional form expression posterior parameter show predictive distribution model given student form obtain expression exercise explore detail property equivalent kernel suppose basis function linearly independent number data point greater number basis function furthermore basis function constant taking suitable linear combination basis function construct basis space otherwise take show equivalent kernel written result show kernel summation constraint consider linear basis function model regression evidence framework show function relation derive result evidence function linear regression model making evaluate integral directly show evidence function linear regression model written form square show error function linear regression written form show integration linear regression model give result hence show marginal likelihood given starting verify step show marginal likelihood function respect lead equation alternative derive result optimal value evidence framework make identity prove identity considering eigenvalue expansion real symmetric matrix making standard result determinant trace expressed term eigenvalue appendix make derive starting starting verify step show marginal likelihood function respect lead equation show marginal probability data word model evidence model exercise given respect respect repeat previous exercise theorem form substitute prior posterior distribution likelihood order derive result model cation previous chapter class regression model particularly simple analytical computational property discus analogous class model cation problem goal cation take input vector assign discrete class common scenario class taken disjoint input assigned class input space thereby divided decision region whose boundary decision boundary decision surface chapter consider linear model cation mean decision surface linear function input vector hence dimensional within input space data set whose class exactly linear decision surface said linearly separable regression problem target variable simply vector real whose value wish predict case cation various way target value represent class label probabilistic model convenient case problem binary representation single target variable class class interpret value probability class value probability taking extreme value class convenient scheme vector length class element zero except element take value instance class pattern class would given target vector interpret value probability class model alternative choice target variable representation sometimes prove convenient chapter three distinct approach cation prob discriminant function directly vector class powerful approach however model conditional probability distribution inference stage us distribution make optimal decision separating inference decision gain numerous bene section different approach conditional probability technique model directly example parametric model parameter training alternatively adopt generative approach model density given together prior probability class compute posterior probability theorem shall discus example three approach chapter linear regression model considered chapter model prediction given linear function parameter case model also linear input variable therefore take form real number cation problem however wish predict discrete class label generally posterior probability range achieve consider generalization model transform linear function nonlinear function machine learning literature known activation function whereas inverse link function statistic literature decision surface correspond constant constant hence sion surface linear function even function nonlinear reason class model generalized linear model note however contrast model used regression longer linear parameter presence nonlinear function lead complex analytical property linear regression model nevertheless model still relatively simple general nonlinear model studied subsequent chapter algorithm chapter equally applicable make nonlinear transformation input variable vector basis function regression model chapter begin consider cation directly original input space section shall convenient switch notation basis function consistency later chapter
['discriminant', 'functions'] discriminant function discriminant function take input vector class chapter shall restrict attention linear namely decision surface simplify consider case class investigate extension class
['two', 'classes'] class representation linear discriminant function linear function input vector weight vector bias confused bias statistical sense negative bias sometimes threshold input vector assigned class class otherwise decision boundary therefore relation dimensional hyperplane within input space consider point decision surface hence vector orthogonal every vector lying within decision surface orientation decision surface similarly point decision surface normal distance origin decision surface given therefore bias parameter location decision surface property case figure furthermore note value give measure distance point decision surface consider figure illustration geometry linear discriminant function dimension decision surface shown displacement origin bias parameter also orthogonal distance eral point decision surface given arbitrary point orthogonal projection onto decision surface multiplying side result making result figure linear regression model chapter sometimes convenient compact notation introduce additional dummy input value case decision surface passing origin dimensional expanded input space
['multiple', 'classes'] multiple class consider extension linear class might build discriminant combining number discriminant function however lead serious hart show consider problem separating point particular class point class known example figure show figure construct class discriminant class lead region shown green left example designed distinguish point class point class right example three discriminant function used separate pair class example three class approach lead region input space ambiguously alternative introduce binary discriminant function every possible pair class known point according majority vote amongst discriminant however run problem ambiguous region diagram figure avoid considering single discriminant linear function form point class decision boundary class class therefore given hence dimensional hyperplane form decision boundary case section analogous geometrical property apply decision region discriminant always singly connected convex consider point inside decision region figure point lie line expressed form figure illustration decision region linear discriminant decision boundary shown point inside decision point lie line point must also hence decision region must singly connected convex linearity discriminant function inside hence also lie inside thus singly connected convex note class either employ formalism based discriminant function else simpler equivalent formulation section based single discriminant function explore three approach learning parameter linear nant function based least square fisher linear discriminant tron algorithm
['least', 'squares', 'classification'] least square cation chapter considered model linear function minimization error function simple solution parameter value therefore tempting apply formalism cation problem consider general cation problem class binary scheme target vector cation least square context conditional expectation target value given input vector binary scheme conditional expectation given vector posterior class probability unfortunately however probability typically rather poorly indeed approximation value outside range limited linear model shall shortly class linear model conveniently group together vector matrix whose column dimensional vector corresponding augmented input vector dummy input representation detail section input assigned class output determine parameter matrix error function regression chapter consider training data matrix whose vector together matrix whose error function written setting derivative respect zero obtain solution form matrix section obtain discriminant function form interesting property solution multiple target variable every target vector training linear constraint constant model prediction value satisfy constraint exercise thus scheme class prediction made model property element value however summation constraint alone allow model output probability constrained within interval approach give exact solution nant function parameter however even discriminant function make decision directly dispense probabilistic interpretation severe problem already seen solution section lack robustness outlier equally cation application figure additional data point right hand produce cant change location decision boundary even though point would correctly original decision bound error function prediction correct long correct side decision figure left plot show data class cross blue circle together decision boundary found least square magenta curve also logistic regression model green curve later section plot show corresponding result extra data point added bottom left diagram showing least square highly sensitive outlier unlike logistic regression boundary section shall consider several alternative error function cation shall suffer however problem least square severe simply lack robustness figure show synthetic data drawn three class input space property decision boundary give excellent separation class indeed technique logistic regression later chapter give tory solution seen plot however solution give poor result small region input space assigned green class failure least square surprise recall maximum likelihood assumption conditional distribution whereas binary target vector clearly distribution appropriate probabilistic model shall obtain cation technique much better property least square moment however continue explore alternative method setting parameter linear cation model
['fisher', 'linear', 'discriminant'] fisher linear discriminant view linear cation model term dimensionality reduction consider case class suppose take figure example synthetic data three class training data point green blue line denote decision boundary background colour denote respective class decision region left result discriminant region input space assigned green class small point class right result logistic regression section showing correct cation training data dimensional input vector project dimension place threshold class otherwise class obtain standard linear previous section general projection onto dimension lead considerable loss class well original space become strongly dimension however weight vector select projection class separation begin consider problem point class point class mean vector class given measure separation class onto separation class mean might choose maximize figure left plot show sample class blue along histogram resulting projection onto line joining class mean note considerable class overlap space right plot show corresponding projection based fisher linear discriminant showing greatly class separation mean data class however expression made arbitrarily large simply increasing magnitude solve problem could constrain unit length multiplier perform constrained maximization appendix still problem approach however exercise figure show class well original dimensional space considerable overlap onto line joining mean strongly nondiagonal covariance class distribution idea fisher maximize function give large separation class mean also giving small variance within class thereby class overlap projection formula data point space variance data class therefore given total variance whole data simply fisher criterion ratio variance variance given make dependence explicit rewrite fisher criterion form exercise covariance matrix given total covariance matrix given respect always direction furthermore care magnitude direction drop scalar factor multiplying side obtain note covariance isotropic proportional unit matrix proportional difference class mean result known fisher linear discriminant although strictly discriminant rather choice direction projection data dimension however data subsequently used construct discriminant choosing threshold point belonging belonging otherwise example model density distribution technique section parameter distribution maximum likelihood found class formalism section give expression optimal threshold cation assumption come central limit theorem random variable
['relation', 'least', 'squares'] relation least square approach determination linear discriminant based goal making model prediction close possible target value contrast fisher criterion derived maximum class separation output space interesting relationship approach particular shall show problem fisher criterion special case least square considered target value however adopt slightly different target scheme solution weight becomes equivalent fisher solution hart particular shall take target class number pattern class total number pattern target value reciprocal prior probability class class shall take target number pattern class error function written setting derivative respect zero obtain respectively making choice target scheme obtain expression bias form used mean total data given straightforward algebra making choice second equation becomes exercise substituted bias note always direction thus write irrelevant scale factor thus weight vector found fisher criterion addition also found sion bias value given tell vector belonging class class otherwise
['fisher', 'discriminant', 'multiple', 'classes'] fisher discriminant multiple class consider generalization fisher discriminant class shall assume dimensionality input space greater number class next introduce linear feature feature value conveniently grouped together form vector similarly weight vector considered column matrix note bias parameter generalization covariance matrix case class give number pattern class order generalization covariance matrix follow hart consider total covariance matrix mean total data total number data point total covariance matrix decomposed covariance matrix given plus additional matrix identify measure covariance covariance matrix original similar matrix wish construct scalar large covariance large covariance small many possible choice criterion example given criterion explicit function projection matrix form maximization criterion straightforward though somewhat involved length weight value determined correspond eigenvalue important result common criterion worth note composed trice outer product vector therefore rank addition matrix independent result constraint thus rank equal nonzero eigenvalue show projection onto dimensional subspace alter value therefore unable linear feature mean
['perceptron', 'algorithm'] algorithm another example linear discriminant model important place history pattern recognition model input vector nonlinear transformation give feature vector used construct generalized linear model form nonlinear activation function given step function form vector typically include bias component discussion cation problem target scheme appropriate context probabilistic model however convenient target value class class match choice activation function algorithm used determine parameter easily error function minimization natural choice error would total number pattern however lead simple learning algorithm error piecewise constant function discontinuity wherever change cause decision boundary move across data point method based error function applied gradient zero almost everywhere therefore consider alternative error function known derive note seeking weight vector pattern class whereas pattern class target scheme would like pattern satisfy criterion associate zero error pattern correctly whereas pattern try minimize quantity criterion therefore given frank important role history chine learning initially computer early built hardware provided direct implementation learning many idea principle dynamic theory brain work whose objection book book widely misinter time showing neural network fatally could learn solution linearly separable problem fact proved limitation case network merely correctly applied general network model unfortunately however book substantial decline research situation reversed today many hundred thousand application neural network widespread example area handwriting recognition information retrieval used routinely million people pattern contribution error associated particular pattern linear function region space pattern zero region correctly total error function therefore piecewise linear apply stochastic gradient descent algorithm error function section change weight vector given learning rate parameter integer index step algorithm function unchanged multiply constant learning rate parameter equal without generality note weight vector training pattern change learning algorithm simple interpretation cycle training pattern turn pattern evaluate function pattern correctly weight vector remains unchanged whereas incorrectly class vector onto current estimate weight vector class subtract vector learning algorithm figure consider effect single update learning algorithm contribution error pattern reduced made course imply contribution error function pattern reduced furthermore change weight vector previously correctly pattern become thus learning rule reduce total error function stage however convergence theorem state solution word training data linearly separable learning algorithm exact solution step proof theorem found example block hertz bishop note however number step achieve vergence could still substantial practice convergence able distinguish problem simply slow converge even data linearly separable many solution found depend parameter presentation data point furthermore data set linearly separable learning algorithm never converge figure illustration convergence learning algorithm showing data point class blue feature space left plot show initial parameter vector shown black arrow together corresponding decision boundary black line arrow point towards decision region belonging class data point circled green feature vector added current weight vector giving decision boundary shown right plot bottom left plot show next point considered green circle feature vector added weight vector giving decision boundary shown bottom right plot data point correctly figure illustration mark hardware photograph left show input simple camera system input scene case printed character illuminated powerful light image onto array cadmium sulphide photocell giving primitive image also patch board shown middle photograph different input feature tried often wired random demonstrate ability learn without need precise wiring contrast modern digital computer photograph right show rack adaptive weight weight rotary variable resistor also potentiometer driven electric motor thereby value weight automatically learning algorithm aside learning algorithm probabilistic output generalize readily class important limitation however fact common model chapter previous based linear basis function detailed discussion limitation found bishop analogue hardware implementation built based variable resistor implement adaptive parameter figure input simple camera system based array basis function could chosen variety way example based simple function randomly chosen subset input image typical application involved learning discriminate simple shape character time closely related system short adaptive linear element functional form model different approach training adopted lehr
['probabilistic', 'generative', 'models'] probabilistic generative model turn next probabilistic view cation show model linear decision boundary arise simple assumption distribution data section distinction discriminative generative approach cation shall adopt generative figure plot logistic sigmoid function shown together scaled function shown dashed blue factor chosen derivative curve equal approach model density well class prior compute posterior probability theorem consider case class posterior probability class written logistic sigmoid function plotted figure term sigmoid mean type function sometimes also function map whole real axis interval logistic sigmoid already chapter play important role many cation algorithm following symmetry property easily veri inverse logistic sigmoid given known function ratio probability class also known odds note simply posterior probability equivalent form appearance logistic sigmoid seem rather however provided take simple functional form shall shortly consider situation linear function case posterior probability generalized linear model case class known exponential generalization logistic sigmoid quantity exponential also known function version function investigate consequence choosing form class conditional density looking continuous input variable case discrete input
['continuous', 'inputs'] continuous input assume density explore resulting form posterior probability start shall assume class share covariance matrix thus density class given consider case class quadratic term exponent density assumption common covariance matrix leading linear function argument logistic sigmoid result case input space figure resulting figure plot show density class blue right corresponding posterior probability given logistic sigmoid linear function surface plot proportion given proportion blue given decision boundary correspond surface along posterior probability constant given linear function therefore decision boundary linear input space prior probability enter bias parameter change prior effect making parallel shift decision boundary generally parallel contour constant posterior probability general case class linear function consequence cancel lation quadratic term covariance resulting decision boundary corresponding minimum cation rate occur posterior probability equal linear function generalized linear model relax assumption covariance matrix allow class conditional density covariance matrix cancellation longer occur obtain quadratic function rise quadratic discriminant linear quadratic decision boundary figure figure plot show density three class distribution green blue green class covariance matrix plot show corresponding posterior probability colour vector posterior probability respective three class decision boundary also shown notice boundary green class covariance matrix linear whereas pair class quadratic
['maximum', 'likelihood', 'solution'] maximum likelihood solution parametric functional form density determine value parameter together prior class probability maximum likelihood data observation along corresponding class label consider case class density covariance matrix suppose data class class denote prior class probability data point class hence similarly class hence thus likelihood function given usual convenient maximize likelihood function consider maximization respect term likelihood function depend setting derivative respect equal zero obtain total number data point class total number data point class thus maximum likelihood estimate simply fraction point class result easily generalized case maximum likelihood estimate prior probability associated class given fraction training point assigned class exercise consider maximization respect pick likelihood function term depend giving setting derivative respect zero obtain simply mean input vector assigned class similar argument corresponding result given mean input vector assigned class finally consider maximum likelihood solution covariance matrix term likelihood function depend standard result maximum likelihood solution weighted average covariance matrix associated class separately result easily extended class problem obtain corresponding maximum likelihood solution parameter density covariance matrix note approach exercise distribution class robust outlier maximum likelihood estimation robust section
['discrete', 'features'] discrete feature consider case discrete feature value simplicity begin looking binary feature value discus extension general discrete feature shortly input general would correspond table number class independent variable summation constraint number feature might seek restricted make naive assumption feature value section independent conditioned class thus distribution form contain independent parameter class substituting give linear function input value case class alternatively consider logistic sigmoid formulation given anal result discrete variable take state exercise
['exponential', 'family'] exponential family seen distributed discrete input posterior class probability given generalized linear model logistic sigmoid class class activation function particular case general result assuming density member exponential family distribution form member exponential family distribution written form restrict attention subclass distribution make introduce scaling parameter obtain restricted exponential family density form note class parameter vector assuming class share scale parameter problem substitute expression density posterior class probability given logistic sigmoid acting linear function given similarly problem substitute density pression give linear function
['probabilistic', 'discriminative', 'models'] probabilistic discriminative model cation problem seen posterior probability class written logistic sigmoid acting linear function wide choice distribution similarly case posterior probability class given transformation linear function choice density used maximum likelihood determine parameter density well class prior used theorem posterior class probability however alternative approach functional form generalized linear model explicitly determine parameter directly maximum likelihood shall algorithm solution known iterative least square indirect approach parameter generalized linear model density class prior separately figure illustration role nonlinear basis function linear cation model left plot show original input space together data point class blue basis function space shown green cross contour shown green circle plot show corresponding feature space together linear decision boundary given logistic regression model form section nonlinear decision boundary original input space shown black curve plot theorem example generative could take model generate synthetic data drawing value marginal distribution direct approach likelihood function conditional distribution form discriminative training advantage discriminative approach typically adaptive parameter determined shall shortly also lead predictive performance particularly density assumption give poor approximation true
['fixed', 'basis', 'functions'] fixed basis function chapter considered cation model work original input vector however algorithm equally applicable make nonlinear transformation input vector basis function resulting decision boundary linear feature space correspond nonlinear decision boundary original space figure class linearly separable feature space need linearly separable original observation space note discussion linear model regression basis function typically constant correspond parameter play role bias remainder chapter shall include basis function transformation highlight useful similarity regression model chapter many problem practical interest cant overlap density posterior probability least value case solution posterior probability accurately standard decision theory chapter note nonlinear transformation remove class overlap indeed increase level overlap create overlap none original observation space however suitable choice make process posterior probability easier basis function model important limitation section resolved later chapter basis function adapt data notwithstanding limitation model nonlinear basis function play important role application discussion model many concept understanding complex counterpart
['logistic', 'regression'] logistic regression begin treatment generalized linear model considering problem cation discussion generative approach section rather general assumption posterior probability class written logistic sigmoid acting linear function feature vector logistic sigmoid function terminology statistic model known logistic regression although model cation rather regression feature space model adjustable parameter contrast class conditional density maximum likelihood would used parameter mean parameter covariance matrix together class prior give total parameter quadratically contrast linear dependence number parameter logistic regression large value clear advantage working logistic regression model directly maximum likelihood determine parameter logistic regression model shall make derivative logistic function conveniently expressed term sigmoid function exercise data likelihood function written usual error function taking negative logarithm likelihood give cross entropy error function form taking gradient error function respect obtain exercise made factor derivative logistic sigmoid leading form gradient likelihood particular contribution gradient data point given error target value prediction model time basis function vector furthermore comparison show take precisely form gradient error function linear regression model section desired could make result give sequential algorithm pattern time weight vector term worth maximum likelihood exhibit severe data set linearly separable maximum likelihood hyperplane corresponding equivalent separate class magnitude go case logistic sigmoid function becomes steep feature space corresponding step function every training point class assigned posterior probability furthermore typically continuum exercise solution separating hyperplane give rise probability training data point seen later figure maximum likelihood solution another solution found practice depend choice optimization parameter note problem arise even number data point large number parameter model long training data linearly separable singularity inclusion prior solution equivalently regularization term error function
['iterative', 'reweighted', 'least', 'squares'] iterative least square case linear regression model chapter likelihood solution assumption noise model lead solution consequence quadratic dependence likelihood function parameter vector logistic regression longer solution logistic sigmoid function however departure quadratic form substantial precise error function concave shall shortly hence unique minimum furthermore error function iterative technique based iterative optimization scheme us local quadratic approximation likelihood function update function take form fletcher bishop wold matrix whose element comprise second derivative respect component apply method linear regression model error function gradient error function given design matrix whose given newton section update take form wold wold recognize standard solution note error case quadratic hence formula give exact solution step apply update error function logistic regression model gradient error function given made also diagonal matrix element longer constant weight matrix corresponding fact error function longer quadratic property form logistic sigmoid function arbitrary vector matrix positive error function concave function hence unique minimum exercise update formula logistic regression model come wold wold vector element wold update formula take form normal equation weighted problem weighing matrix constant parameter vector must apply normal equation iteratively time weight vector compute weighing matrix reason algorithm known iterative least square weighted problem element diagonal weighting matrix variance mean variance logistic regression model given used property fact interpret solution problem space variable quantity element given simple interpretation effective target value space making local linear approximation logistic sigmoid function around current operating point wold wold
['multiclass', 'logistic', 'regression'] logistic regression discussion generative model cation section seen large class distribution posterior probability given transformation linear function feature variable activation given used maximum likelihood determine separately density class prior found corresponding posterior probability theorem thereby implicitly parameter consider maximum likelihood determine parameter model directly require derivative respect activation given exercise element identity matrix next write likelihood function easily done scheme target vector feature vector belonging class binary vector element zero except element equal likelihood function given matrix target variable element taking negative logarithm give known error function cation problem take gradient error function respect param vector making result derivative function obtain exercise made form gradient found error function linear model error logistic regression model namely prod error time basis function could formulate sequential algorithm pattern time weight vector seen derivative likelihood function linear sion model respect parameter vector data point took form error time feature vector similarly combination logistic sigmoid activation function error function activation function error function obtain simple form example general result shall section batch algorithm appeal update obtain corresponding algorithm problem evaluation matrix block size block given problem matrix logistic sion model positive error function unique minimum exercise practical detail case found bishop
['probit', 'regression'] regression seen broad range distribution exponential family resulting posterior class probability given logistic transformation acting linear function feature vari however choice density give rise simple form posterior probability instance density mixture might worth exploring type discriminative probabilistic model purpose chapter however shall return case remain within frame work generalized linear model activation function motivate alternative choice link function consider noisy threshold model input evaluate target value according otherwise figure schematic example probability density shown blue curve given example mixture along cumulative distribution function shown curve note value blue curve point vertical green line slope curve point conversely value curve point area blue curve shaded green region stochastic threshold model class label take value value threshold take value equivalent activation function given cumulative distribution function value drawn probability density corresponding activation function given cumulative distribution function figure example suppose density given zero mean unit variance corresponding cumulative distribution function given known function sigmoidal shape logistic sigmoid function figure note eral distribution change model equivalent linear many numerical package provide evaluation closely related function known function error function confused error function machine learning model related function exercise generalized linear model based activation function known regression determine parameter model maximum likelihood straightforward extension idea practice result found regression tend similar logistic regression shall however another model discus treatment logistic regression section issue occur practical application outlier arise instance error measuring input vector belling target value point long wrong side ideal decision boundary seriously distort note logistic regression model behave differently respect tail logistic sigmoid decay asymptotically like whereas activation function decay like model sensitive outlier however logistic model assume data correctly effect easily incorporated probabilistic model probability target value wrong value leading target value distribution data point form activation function input vector advance whose value data
['canonical', 'link', 'functions'] canonical link function linear regression model noise distribution error function corresponding negative likelihood given take derivative respect parameter vector contribution error function data point take form error time feature vector similarly combination logistic sigmoid activation function error function activation function error function obtain simple form show general result assuming conditional distribution target variable exponential family along corresponding choice activation function known canonical link function make restricted form exponential family note assumption exponential family target variable contrast section applied input vector therefore consider conditional distribution target variable form line argument derivation result conditional mean denote given thus must related denote relation following generalized linear model nonlinear function linear combination input feature variable known activation function machine learning literature known link function statistic consider likelihood function model function given assuming observation share common scale parameter noise variance distribution instance independent derivative likelihood respect model parameter given used together result considerable cation choose particular form link function given give hence also hence case gradient error function whereas logistic model
['laplace', 'approximation'] approximation section shall discus treatment logistic regression shall complex treatment linear regression model section particular integrate exactly parameter vector since posterior distribution longer therefore necessary introduce form approximation later book shall consider range technique based analytical approximation chapter numerical sampling chapter introduce simple widely used framework proximation aim approximation probability density continuous variable consider case single variable suppose distribution normalization shall suppose value unknown method goal mode distribution step mode word point equivalently distribution property logarithm quadratic function variable therefore consider expansion mode note term expansion appear since local maximum distribution taking exponential obtain obtain distribution making standard result normalization approximation figure note approximation well precision word stationary point must local maximum second derivative point negative figure illustration approximation applied distribution logistic sigmoid function left plot show distribution yellow together approximation mode right plot show negative logarithm corresponding curve extend method approximate distribution space stationary point gradient vanish expanding around stationary point matrix gradient operator taking exponential side obtain distribution proportional appropriate normalization found inspection standard result giving determinant distribution well provided precision matrix given positive stationary point must local maximum minimum saddle point order apply approximation need mode evaluate matrix mode practice mode found running form numerical optimization algorithm bishop many distribution practice different approximation according mode considered note normalization constant true need known order apply method result central limit theorem posterior distribution model become increasingly better number data point would expect approximation useful situation number data point relatively large major weakness approximation since based distribution directly applicable real variable case possible apply approximation transformation variable instance consider approximation serious limitation framework however based purely aspect true distribution value variable fail capture important global property chapter shall consider alternative approach adopt global perspective
['model', 'comparison', 'bic'] model comparison well distribution also obtain normalization constant approximation noted integrand made standard result distribution result obtain approximation model evidence section play central role model comparison consider data model parameter model likelihood function introduce prior parameter interested model various model omit keep notation uncluttered theorem model evidence given result obtain exercise factor value mode posterior distribution matrix second derivative negative posterior term right hand side likelihood parameter three term comprise factor model complexity assume prior distribution parameter broad full rank approximate roughly exercise number data point number parameter additive constant known information criterion schwarz criterion schwarz note given model complexity heavily complexity measure virtue easy evaluate also give misleading result particular assumption matrix full rank often valid since many parameter result obtain accurate estimate section model evidence starting approximation illustrate context neural network section
['bayesian', 'logistic', 'regression'] logistic regression turn treatment logistic regression exact infer logistic regression intractable particular evaluation posterior distribution would require normalization product prior distribution likelihood function product logistic sigmoid function every data point evaluation predictive distribution similarly intractable consider application approximation problem logistic regression
['laplace', 'approximation'] approximation recall section approximation mode posterior distribution mode evaluation second derivative posterior equivalent matrix seek representation posterior distribution natural begin prior write general form posterior distribution given taking side substituting prior distribution likelihood function obtain obtain approximation posterior maximize posterior distribution give maximum posterior solution mean covariance given inverse matrix second derivative negative likelihood take form approximation posterior distribution therefore take form approximation posterior distribution remains task respect distribution order make prediction
['predictive', 'distribution'] predictive distribution predictive distribution class given feature vector respect posterior distribution distribution corresponding probability class given evaluate predictive distribution note function projection onto delta function obtain evaluate delta function linear constraint form marginal distribution joint distribution grating direction orthogonal know section marginal distribution also evaluate mean covariance distribution taking moment order integration used result variational posterior distribution similarly vara note distribution take form predictive distribution linear regression model noise variance zero thus variational approximation predictive distribution becomes result also derived directly making result marginal distribution given section exercise integral convolution logistic analytically however obtain good barber bishop making close similarity logistic sigmoid function function order obtain best approximation logistic function need axis approximate suitable value function slope origin give similarity logistic sigmoid function exercise choice figure advantage function convolution expressed analytically term another function show exercise apply approximation function side equation leading following approximation logistic sigmoid result obtain approximate predictive distribution form respectively note decision boundary corresponding given decision boundary value thus decision criterion based rate equal prior probability however complex decision criterion play important role logistic sigmoid model approximation posterior distribution context variational inference figure exercise given data point convex hull point given consider second point together corresponding convex hull set point linearly separable vector scalar show convex hull intersect set point linearly separable conversely linearly separable convex hull intersect consider minimization error function suppose target vector training satisfy linear constraint matrix show consequence constraint element model prediction given solution also satisfy constraint assume basis function corresponding parameter play role bias extend result exercise show multiple linear constraint simultaneously target vector constraint also prediction linear model show maximization class separation criterion given respect multiplier enforce constraint lead result making show fisher criterion written form covariance matrix given respectively together choice target value section show expression error function written form show logistic sigmoid function property inverse given derive result posterior class probability generative model density verify result parameter consider generative cation model class prior class probability general density input feature vector suppose given training data binary target vector length us scheme component pattern class assuming data point drawn independently model show solution prior probability given number data point assigned class consider cation model exercise suppose density given distribution matrix show maximum likelihood solution mean distribution class given mean feature vector assigned class similarly show maximum likelihood solution covariance matrix given thus given weighted average covariance data associated class weighting given prior probability class consider cation problem class feature vector component take discrete state value component binary scheme suppose conditioned class component independent density respect feature vector component show quantity given appear argument function posterior class probability linear function component note example naive model section verify relation derivative logistic sigmoid making result derivative logistic show derivative error function logistic regression model given show linearly separable data maximum likelihood solution logistic regression model vector whose decision boundary separate class taking magnitude show matrix logistic regression model given positive diagonal matrix element output logistic regression model input vector hence show error function concave function unique minimum consider binary cation problem observation known belong class corresponding suppose procedure training data imperfect training point sometimes every data point instead value class label instead value probability given probabilistic model write likelihood function appropriate data show derivative activation function given result derivative activation function show gradient error given write expression gradient likelihood well corresponding matrix regression model quantity would train model show matrix logistic regression problem positive note full matrix problem size number parameter number class prove positive property consider product arbitrary vector length apply inequality show function function related result derive expression model approximation exercise derive result starting approximation model evidence given show prior parameter form model evidence approximation take form matrix second derivative likelihood assume prior broad small second term side furthermore consider case independent identically distributed data term data point show model evidence written approximately form expression result section derive result marginal logistic regression model respect posterior parameter suppose wish approximate logistic sigmoid scaled function show chosen derivative function equal exercise prove relation convolution function distribution show derivative left hand side respect equal derivative side integrate side respect show constant integration note side convenient introduce change variable given integral integral differentiate side relation obtain integral analytically network chapter considered model regression cation linear combination basis function model useful analytical computational property practical applicability limited curse dimensionality order apply model large scale problem necessary adapt basis function data support vector machine chapter address basis function training data point subset training advantage although training nonlinear optimization objective function convex solution optimization problem relatively straightforward number basis function resulting model generally much smaller number training point although often still relatively large typically increase size training relevance vector machine section also subset basis function typically result much model unlike also produce probabilistic output although expense optimization training alternative approach number basis function advance allow adaptive word parametric form basis parameter value training successful model type context pattern recognition neural network also known chapter fact really misnomer model layer logistic regression model continuous rather multiple discontinuous many application resulting model compact hence faster evaluate support vector machine generalization performance price compactness relevance vector machine like function form basis network training longer convex function model parameter practice however often worth substantial computational resource training phase order obtain compact model fast data term neural network origin attempt mathematical information biological system indeed used broadly cover wide range different model many subject exaggerated claim regarding biological perspective practical application pattern recognition ever biological realism would impose entirely unnecessary constraint focus chapter therefore neural network model statistical pattern recognition particular shall restrict attention class network proven practical value namely begin considering functional form network model basis function discus prob network parameter within maximum likelihood frame work solution nonlinear optimization problem evaluation derivative likelihood function respect work parameter shall technique error shall also show framework extended allow derivative matrix next discus various approach neural network training relationship also consider extension neural network model particular describe eral framework conditional probability distribution known mixture density network finally discus treatment neural work additional background neural network model found bishop
['feed', 'forward', 'network', 'functions'] network function linear model regression cation chapter based linear combination nonlinear basis function take form nonlinear activation function case cation identity case regression goal extend model making basis function depend parameter allow parameter along training course many way construct parametric nonlinear basis function neural network basis function follow form basis function nonlinear function linear combination input linear combination adaptive parameter lead basic neural network model series functional transformation first construct linear combination input variable form superscript corresponding param layer network shall refer parameter weight parameter bias following nomenclature chapter quantity known activation differentiable nonlinear activation function give quantity correspond output basis function context neural network hidden unit nonlinear function generally chosen sigmoidal function logistic sigmoid tanh function following value linearly combined give output exercise unit activation total number output transformation second layer network bias parameter finally output unit activation appropriate activation function give network output choice activation function determined nature data assumed distribution target variable figure network diagram layer neural network input hidden output variable node weight parameter link node bias link coming additional input hidden variable arrow denote information network forward propagation hidden unit input output consideration linear model chapter thus standard regression problem activation function identity similarly multiple binary cation problem output unit activation logistic sigmoid function finally problem activation function form used choice output unit activation function detail combine various stage give overall network function sigmoidal output unit activation function take form weight bias parameter grouped together vector thus neural network model simply nonlinear function input variable output variable vector adjustable parameter function form network diagram shown figure process forward propagation information network diagram represent probabilistic graphical model kind chapter internal node represent deterministic variable rather stochastic one reason adopted slightly different graphical notation kind model shall later give probabilistic interpretation neural network section bias parameter absorbed weight parameter additional input variable whose value take form similarly absorb bias weight overall network function becomes seen figure neural network model stage model section reason neural network also known difference however neural work us continuous sigmoidal hidden unit whereas us mean neural network differentiable respect network parameter property play central role network training activation function hidden unit network taken linear network always equivalent network without hidden unit fact composition successive linear transformation linear transformation however number hidden unit smaller either number input output unit network generate general possible linear formation input output information lost dimensionality reduction hidden unit section show network linear unit give rise principal component analysis general however little interest network linear unit network architecture shown figure commonly used practice however easily generalized instance considering additional layer weighted linear combination form transformation nonlinear activation note confusion literature regarding terminology counting number layer network thus network figure layer network count number layer unit treat input unit sometimes network count number layer hidden unit recommend terminology figure network number layer weight important network property another generalization network architecture include associated corresponding adaptive parameter figure example neural network general topology note hidden output unit associated bias parameter clarity input output instance network would directly input output principle network sigmoidal hidden unit always mimic skip layer bounded input value small weight operating range hidden unit effectively linear large weight value hidden unit output practice however advantageous include connection explicitly furthermore network sparse possible connection within layer present shall example sparse network architecture consider convolutional neural network section direct correspondence network diagram mathematical function develop general network complex network diagram however must restricted architecture word closed directed cycle ensure output deterministic function input simple example figure hidden output unit network function given run unit send connection unit bias param included summation given value applied input network successive application activation unit network output unit approximation property network widely stud white cotter found general neural network therefore said universal example network linear output uniformly approximate continuous function compact input domain arbitrary racy provided network large number hidden unit result hold wide range hidden unit activation function excluding although theorem reassuring problem suitable parameter value given training data later section chapter figure illustration approximate four different step function case data point shown blue dot pled uniformly interval corresponding data point used train layer network hidden unit tanh activation function linear output unit resulting network function shown curve output three hidden unit shown three dashed curve show exist effective solution problem based maximum likelihood approach capability network model broad range function figure also show individual hidden unit work collaboratively approximate function role hidden unit simple cation problem figure synthetic cation data appendix
['weight', 'space', 'symmetries'] symmetry property network play role consider model comparison multiple distinct choice weight vector give rise function input output consider network form shown figure hidden unit tanh activation function full connectivity layer change sign weight bias feeding particular hidden unit given input pattern sign activation hidden unit reversed tanh function tanha tanha transformation exactly sign weight leading hidden unit thus sign particular group weight bias function network unchanged found different weight vector give rise function hidden unit sign figure example solution simple class cation problem synthetic data neural network input hidden unit tanh activation function single output logistic sigmoid function dashed blue line show contour hidden unit line show decision surface work comparison green line optimal decision boundary distribution used generate data symmetry thus given weight vector equivalent weight vector similarly imagine interchange value weight bias leading particular hidden unit corresponding value weight bias associated different hidden unit clearly leaf network function unchanged different choice weight vector hidden unit given weight vector belong equivalent weight vector associated inter change symmetry corresponding different hidden unit network therefore overall symmetry factor network layer weight total level symmetry given product factor layer hidden unit turn factor account symmetry weight space except possible accidental symmetry choice weight furthermore existence symmetry particular property tanh function wide range activation function many case symmetry weight space little consequence although section shall encounter situation need take account
['network', 'training'] network training neural network general class parametric nonlinear function vector input variable vector output variable simple approach problem network parameter make analogy discussion polynomial curve section therefore minimize error function given training input vector together corresponding target vector minimize error function however provide much general view network training giving probabilistic interpretation network output already seen many advantage probabilistic prediction section also provide clearer motivation choice output unit choice error function start regression problem moment consider single target variable take real value following discussion section assume distribution dependent mean given output neural network precision inverse variance noise course somewhat restrictive assumption section shall extend approach allow general conditional distribution conditional distribution given take output unit activation function identity network approximate continuous function given data independent identically distributed observation along corresponding target value construct corresponding likelihood function taking negative logarithm obtain error function used learn parameter section shall cuss treatment neural network consider maximum likelihood approach note neural network literature usual sider minimization error function rather maximization likelihood shall follow convention consider nation likelihood function equivalent error function given additive multiplicative constant value found maximum likelihood solution practice network function cause error practice local maximum likelihood found corresponding local minimum error function section found value found negative likelihood give note iterative optimization multiple target variable assume inde pendent conditional noise precision conditional distribution target value given following argument single target variable maximum likelihood weight determined error function noise precision given exercise number target variable assumption independence expense slightly complex optimization problem exercise recall section natural error function given negative likelihood output unit activation function regression case view network output activation function identity corresponding error function property shall make error section consider case binary cation single target variable class class following discussion canonical link function section consider network single output whose activation function logistic sigmoid interpret conditional probability given conditional distribution target given input distribution form consider training independent observation error function given negative likelihood error function form note analogue noise precision target value assumed correctly however model easily extended allow error found exercise error function instead cation problem lead faster training well generalization separate binary cation perform work output logistic sigmoid activation function associated output binary class label assume class label independent given input vector conditional distribution target taking negative logarithm corresponding likelihood function give following error function exercise derivative error function activation particular output unit take form exercise regression case interesting contrast neural network solution problem corresponding approach based linear cation model kind chapter suppose standard network kind shown figure weight parameter layer network various output whereas linear model cation problem independently layer network nonlinear feature extraction feature different output save computation also lead generalization finally consider standard cation problem input assigned mutually exclusive class binary target variable scheme class network output leading following error function figure geometrical view error function surface sitting weight space point local minimum global minimum point local gradient error surface given vector following discussion section output unit activation function canonical link given function note unchanged constant added causing error function constant direction weight space degeneracy removed appropriate regularization term section added error function derivative error function respect activation particular output unit take familiar form exercise summary natural choice output unit activation function matching error function according type problem linear output error multiple independent binary cation logistic sigmoid output error cation output corresponding error function cation problem class single logistic sigmoid output alternatively network output output activation function
['parameter', 'optimization'] parameter optimization turn next task weight vector chosen function point useful geometrical picture error function view surface sitting weight space shown figure first note make small step weight space change error function vector point direction rate increase error function error smooth continuous function value occur point weight space gradient error function otherwise could make small step direction thereby reduce error point gradient stationary point minimum maximum saddle point goal vector take value ever error function typically highly nonlinear dependence weight bias parameter many point weight space gradient numerically small indeed discussion point local minimum point weight space equivalent minimum instance work kind shown figure hidden unit point weight space member family equivalent point section furthermore typically multiple inequivalent stationary point particular multiple inequivalent minimum minimum value error function weight vector said global minimum minimum corresponding higher value error function said local minimum successful application neural network necessary global minimum general known whether global minimum found necessary compare several local minimum order good solution clearly hope analytical solution resort iterative numerical procedure optimization continuous nonlinear function widely studied problem tensive literature solve technique involve choosing initial value weight vector moving weight space succession step form label iteration step different algorithm involve different choice weight vector update many algorithm make gradient information therefore require update value weight vector order understand importance gradient information useful consider local approximation error function based expansion
['local', 'quadratic', 'approximation'] local quadratic approximation insight optimization problem various technique considering local quadratic approximation error function consider expansion around point weight space cubic higher term gradient matrix element corresponding local approximation gradient given point close expression give reasonable approximation error gradient consider particular case local quadratic approximation around point minimum error function case linear term becomes order interpret geometrically consider eigenvalue equation matrix form complete appendix expand linear combination form transformation system origin point ax rotated align orthogonal matrix whose column detail appendix substituting error function written form matrix said positive figure error function quadratic contour error ellipsis whose ax matrix length inversely proportional square root correspond form complete arbitrary vector written form positive eigenvalue positive exercise system whose basis vector given contour constant ellipsis origin exercise figure weight space stationary point minimum corresponding result matrix positive exercise
['use', 'gradient', 'information'] gradient information shall section possible evaluate gradient error function mean procedure gradient information lead cant improvement speed minimum error function quadratic approximation error function given error surface quantity contain total independent element matrix symmetric exercise dimensionality total number adaptive parameter network location minimum quadratic approximation therefore parameter expect able locate minimum independent piece information make gradient information would expect perform function evaluation would require step thus computational effort minimum approach would compare algorithm make gradient information evaluation item information might hope minimum function gradient evaluation shall error evaluation take step minimum found step reason gradient information form basis practical algorithm training neural network
['gradient', 'descent', 'optimization'] gradient descent optimization approach gradient information choose weight update comprise small step direction negative gradient parameter known learning rate update gradient weight vector process repeated note error function respect training step entire training order evaluate technique whole data batch method step weight vector direction rate decrease error function approach known gradient descent descent although approach might intuitively seem reasonable fact turn poor algorithm reason bishop batch optimization method conjugate method much robust much faster simple gradient descent gill fletcher wright unlike gradient descent algorithm property error function always decrease iteration unless weight vector local global minimum order good minimum necessary algorithm multiple time time different randomly starting point resulting performance independent vali dation however version gradient descent proved useful practice training neural network large data set error function based maximum likelihood independent observation comprise term data point gradient descent also known sequential gradient descent stochastic gradient descent make update weight vector based data point time update repeated cycling data either sequence point random replacement course intermediate scenario update based batch data point advantage method batch method former handle redundancy data much consider example take data double size every data point note simply error function factor equivalent original error function batch method require double computational effort evaluate batch error function gradient whereas line method unaffected another property gradient descent possibility local minimum since stationary point respect error function whole data generally stationary point data point individually nonlinear optimization algorithm practical application neural work training detail bishop
['error', 'backpropagation'] error goal section technique gradient error function neural network shall local message passing scheme information sent alternately forward backwards network known error sometimes simply noted term used neural literature mean variety different thing instance architecture sometimes network term also used describe training gradient descent applied error function order clarify terminology useful consider nature training process care fully training algorithm involve iterative procedure minimization error function adjustment weight made sequence step step distinguish distinct stage stage derivative error function respect weight must shall important contribution technique method derivative stage error backwards network shall term describe evaluation derivative second stage derivative used compute adjustment made weight technique originally considered gradient descent important recognize stage distinct thus stage namely propagation backwards network order evaluate derivative applied many kind network also applied error function simple derivative matrix shall later chapter similarly second stage weight adjustment calculated derivative tackled variety optimization scheme many substantially powerful simple gradient descent
['evaluation', 'error', 'function', 'derivatives'] evaluation derivative derive algorithm general network topology arbitrary differentiable nonlinear activation function broad class error function resulting formula simple layered network structure single layer sigmoidal hidden unit together error many error function practical interest instance likelihood data comprise term data point training shall consider problem term error function used directly sequential optimization result training case batch method consider simple linear model output linear input variable together error function particular input pattern take form gradient error function respect weight given local computation product error signal associated output link variable associated input link section similar formula logistic sigmoid activation function together cross entropy error function similarly activation function together matching error function shall simple result complex setting network general network unit weighted input form activation unit input connection unit weight associated connection section bias included extra unit input activation therefore need deal bias explicitly nonlinear activation function give activation unit form note variable could input similarly unit could output pattern training shall suppose corresponding input vector network calculated activation hidden output unit network successive application process often forward propagation forward information network consider evaluation derivative respect weight output various unit depend particular input pattern however order keep notation uncluttered shall omit subscript network variable first note weight summed input unit therefore apply chain rule partial derivative give introduce useful notation often error reason shall shortly write substituting obtain equation tell derivative simply multiplying value unit output weight value unit input weight case bias note take form simple linear model considered start section thus order evaluate derivative need calculate value hidden output unit network apply seen already output unit figure illustration calculation hidden unit unit unit connection blue arrow direction information forward propagation arrow indicate backward propagation error information provided canonical link activation function evaluate hidden unit make chain rule partial derivative run unit unit connection arrange unit weight figure note unit could include hidden unit output unit writing making fact variation give rise variation error variation variable substitute given make obtain following formula tell value particular hidden unit backwards unit higher network figure note summation taken index corresponding backward propagation information network whereas forward propagation equation taken second index already know value output unit evaluate hidden unit network regardless topology procedure therefore error apply input vector network forward propagate network activation hidden output unit evaluate output unit obtain hidden unit network evaluate derivative batch method derivative total error step pattern training pattern derivation implicitly assumed hidden output unit network activation function derivation easily general however allow different unit individual activation function simply keeping track form go unit
['simple', 'example'] simple example derivation procedure general form error function activation function network topology order illustrate application algorithm shall consider particular example chosen simplicity practical importance cause many application neural network literature make type network shall consider network form figure together error output unit linear activation function hidden unit logistic sigmoid activation function given tanha tanha useful feature function derivative expressed simple form also consider standard error function pattern error given activation output unit corresponding target particular input pattern pattern training turn perform forward propagation next compute output unit obtain hidden unit finally derivative respect weight given
['efficiency', 'backpropagation'] important aspect computational understand examine number computer operation evaluate derivative error function scale total number weight bias network single evaluation error function given input pattern would require operation large fact except network sparse connection number weight typically much greater number unit bulk computational effort forward propagation concerned sum evaluation activation function small overhead term multiplication addition leading overall computational cost alternative approach derivative error function difference done perturbing weight turn derivative expression simulation accuracy approximation derivative making smaller numerical problem arise accuracy difference method symmetrical central difference form case correction cancel veri expansion exercise side residual correction number computational step however roughly doubled main problem numerical differentiation highly desirable scaling lost forward propagation step figure illustration modular pattern recognition system matrix used error signal output lier module system weight network must perturbed individually overall scaling however numerical differentiation play important role practice comparison derivative calculated central difference powerful check correctness implementation algorithm training network tice derivative give accuracy numerical however result numerical differentiation test case order check correctness implementation
['jacobian', 'matrix'] matrix seen derivative error function respect weight propagation error backwards network technique also applied calculation consider evaluation matrix whose element given derivative network output respect input derivative input matrix play useful role system built number distinct module figure module comprise adaptive function linear nonlinear long differentiable suppose wish minimize error function respect parameter figure derivative error function given matrix module figure middle term matrix measure local sensitivity output change input variable also known error associated input trained network order estimate contribution error output relation valid provided small general network trained neural network nonlinear element matrix constant depend particular input vector used thus valid small perturbation input must input vector matrix procedure similar derived derivative error function respect weight start writing element form made run unit input unit connection example unit hidden layer layered topology considered write recursive formula determine derivative run unit unit connection corresponding index made start output unit derivative found directly functional form activation function instance individual sigmoidal activation function output unit whereas output summarize procedure matrix apply input vector corresponding point input space matrix found forward propagate usual obtain activation hidden output unit network next matrix corresponding output unit recursive relation starting hidden unit network finally input also alternative forward propagation formalism derived analogous approach given exercise implementation algorithm checked differentiation form forward propagation network input
['hessian', 'matrix'] matrix shown technique used obtain derivative error function respect weight network back propagation also used evaluate second derivative error given note sometimes convenient consider weight bias parameter element single vector case second derivative form element matrix total number weight bias play important role many aspect neural following several nonlinear optimization algorithm used training neural network based consideration property error surface matrix bishop form basis fast procedure network following small change training data bishop inverse used identify least cant weight network part network pruning algorithm play central role approximation neural network section inverse used determine distribution trained network eigenvalue determine value determinant used evaluate model evidence various approximation scheme used evaluate matrix neural network however also calculated exactly extension technique important consideration many application parameter weight bias network matrix dimension computational effort evaluate scale like pattern data shall method whose scaling indeed
['diagonal', 'approximation'] diagonal approximation application matrix require inverse rather reason interest diagonal approximation word simply element zero inverse trivial evaluate shall consider error function term pattern data considering pattern time result pattern diagonal element pattern written second derivative side found chain rule differential calculus give equation form neglect element term obtain becker note number computational step evaluate approximation total number weight bias parameter network full also used diagonal approximation term evaluation exact sion diagonal term note longer scaling major problem diagonal approximation however practice typically found strongly nondiagonal approximation driven mainly computational convenience must care
['outer', 'product', 'approximation'] outer product approximation neural network applied regression problem common error function form considered case single output order keep notation simple extension several output straightforward write exercise matrix form network trained data output happen close target value second term small generally however appropriate neglect term following argument recall section optimal function loss conditional average target data quantity random variable zero mean assume value uncorrelated value second derivative term side whole term average zero summation exercise second term arrive approximation outer product approximation matrix built outer product vector given activation function output unit simply identity evaluation outer product approximation straightforward derivative error function step standard element matrix found step simple multiplication important emphasize approximation likely valid network trained appropriately general network second derivative term side typically negligible case error function network logistic sigmoid activation function corresponding approximation given exercise analogous result network output unit activation function exercise
['inverse', 'hessian'] inverse approximation develop procedure inverse stork first write approximation matrix notation contribution gradient output unit activation data point derive sequential procedure building data point time suppose already inverse data point separating contribution data point obtain order evaluate inverse consider matrix identity unit matrix simply special case identity identify obtain data point sequentially absorbed whole data result therefore procedure inverse single data initial matrix chosen small quantity algorithm actually inverse result particularly sensitive precise value extension algorithm network output straightforward exercise note matrix sometimes calculated indirectly part network training algorithm particular nonlinear algorithm gradually build approximation inverse training algorithm detail bishop
['finite', 'differences'] finite difference case derivative error function second derivative difference accuracy limited numerical precision perturb possible pair weight turn obtain symmetrical central difference formulation ensure residual error rather element matrix evaluation element four forward propagation needing operation pattern approach require operation evaluate complete therefore poor scaling property although practice useful check soft ware implementation method version numerical differentiation found central difference derivative error function calculated give weight perturbed gradient step method give operation
['exact', 'evaluation', 'hessian'] exact evaluation considered various approximation scheme matrix inverse also exactly work arbitrary topology extension technique back propagation used evaluate derivative share many desirable feature computational bishop bishop applied differentiable error function expressed function network output network arbitrary differentiable activation number computational step evaluate scale like similar algorithm also considered consider case network layer weight equation easily derived shall index exercise denote input index hidden unit index denote output contribution error data point matrix network considered three separate block weight second layer weight layer weight layer element identity matrix weight bias term corresponding expression simply setting appropriate activation inclusion connection straightforward exercise
['fast', 'multiplication', 'hessian'] fast multiplication many application quantity interest matrix product vector seen evaluation take operation also storage vector wish calculate however element instead intermediate step instead approach directly operation note gradient operator weight space write standard equation evaluation apply equation give equation evaluation acting original back propagation equation differential operator used notation denote operator shall follow convention analysis straightforward make usual rule differential calculus together result technique best simple example choose network form shown figure linear output unit error function consider contribution error function pattern data vector usual contribution pattern separately network equation given equation operator obtain forward propagation equation form element vector weight quan form variable whose value found equation considering error function standard expression equation operator obtain equation form finally usual equation derivative error acting operator obtain expression element vector implementation algorithm introduction additional variable hidden unit output unit input pattern value quantity found result element given elegant aspect technique equation mirror closely standard forward backward propagation extension compute product typically straightforward desired technique used evaluate full matrix choosing vector given successively series unit vector form pick column lead formalism analytically equivalent procedure bishop section though loss redundant calculation
['regularization', 'neural', 'networks'] regularization neural network number input output unit neural network generally determined dimensionality data whereas number hidden unit free parameter give best predictive performance note control number parameter weight bias network might expect maximum likelihood setting optimum value give best generalization performance corresponding optimum balance figure show example effect different value sinusoidal regression problem generalization error however simple function presence local minimum error function figure effect choosing multiple random weight vector range value overall best validation performance case particular solution practice approach choosing fact plot graph kind shown figure choose solution validation error however way control complexity neural network model order avoid discussion polynomial curve chapter alternative approach choose relatively large value control complexity addition regularization term error function regularizer quadratic giving error figure example network trained data point drawn sinusoidal data graph show result network hidden unit respectively error function scaled algorithm form regularizer also known weight decay length chapter effective model complexity determined choice regularization seen previously regularizer negative logarithm prior distribution weight vector
['consistent', 'gaussian', 'priors'] consistent prior limitation simple weight decay form inconsistent certain scaling property network illustrate consider network layer weight linear output unit input variable output variable activation hidden unit hidden layer figure plot error polynomial data number hidden unit network random start network size showing local minimum start weight vector initial sampling isotropic distribution mean zero variance take form activation output unit given suppose perform linear transformation input data form arrange network unchanged making corresponding linear transformation weight bias input unit hidden layer form exercise similarly linear transformation output variable network form making transformation weight bias train network original data network data input target variable linear consistency obtain equivalent network differ linear transformation weight given regularizer consistent property otherwise arbitrarily solution another equivalent clearly simple weight decay treat weight bias equal footing satisfy property therefore look regularizer invariant linear formation require regularizer invariant weight shift bias regularizer given weight layer weight second layer bias summation regularizer remain unchanged weight transformation provided regularization parameter regularizer prior form note prior form improper bias parameter unconstrained improper prior lead regularization model comparison within framework corresponding evidence zero therefore common include separate prior bias break shift invariance illustrate effect resulting four drawing sample prior plotting corresponding network function shown figure generally consider prior weight divided number group special case prior choose group correspond set weight associated input unit optimize marginal likelihood respect corresponding parameter obtain automatic relevance determination section
['early', 'stopping'] early stopping alternative regularization effective complexity network procedure early stopping training nonlinear network model iterative reduction error function training data many optimization algorithm used network training conjugate gradient error nonincreasing function iteration index however error measured respect independent data generally validation often show decrease crease network start training therefore stopped point error respect validation data figure order obtain network good generalization performance behaviour network case sometimes qualitatively term effective number degree freedom network number start small training process corresponding steady increase effective complexity model halting training figure illustration effect governing prior distribution weight bias network single input single linear output hidden unit tanh activation function prior four represent precision distribution bias weight bias weight respectively parameter vertical scale function note different vertical axis range diagram horizontal scale variation function value horizontal range variation occur parameter whose effect range vertical offset function minimum training error limiting effective network complexity case quadratic error function verify insight show early stopping exhibit similar behaviour regularization term understood figure ax weight space rotated parallel matrix absence weight decay weight vector start origin proceeds training along path local negative gradient weight vector move initially parallel axis point corresponding roughly move towards minimum error shape error surface widely eigenvalue stopping point near therefore similar weight decay relationship early stopping weight decay made quan thereby showing quantity iteration index exercise learning rate parameter play role reciprocal regularization figure illustration behaviour training error left validation error right typical training session function iteration step sinusoidal data goal best generalization performance training stopped point shown vertical dashed line corresponding minimum validation error parameter effective number parameter network therefore course training
['invariances'] invariance many application pattern recognition known prediction unchanged invariant transformation input vari example cation object image digit particular object assigned cation irrespective position within image translation invariance size scale invariance transformation produce cant change data expressed term intensity image give rise output cation system similarly speech recognition small level nonlinear warping along time axis preserve temporal change interpretation signal large number training pattern available adaptive model neural network learn invariance least approximately within training large number example effect various transformation thus translation invariance training include example object many different position approach impractical however number training example limited several invariant number combination transformation exponentially number transformation therefore seek alternative approach encouraging adaptive model exhibit invariance broadly divided four category training augmented replica training pattern formed according desired invariance instance digit example could make multiple copy example figure schematic illustration early stopping give similar result weight decay case quadratic error ellipse show tour constant error minimum function weight vector start origin move cording local negative direction follow path shown curve stopping training early weight vector found similar simple training error seen figure digit different position image regularization term added error function change model output input lead technique tangent propagation section invariance built feature transformation subsequent regression cation system us feature input necessarily also respect invariance option build invariance property structure network kernel function case technique relevance vector machine achieve local receptive eld weight context convolutional neural network section approach often relatively easy implement used encourage plex invariance figure sequential training algorithm done transforming input pattern model pattern different transformation drawn appropriate distribution added time batch method similar effect data point number time transforming copy independently augmented data lead cant improvement generalization although also costly approach leaf data unchanged error function addition regularizer section shall show approach closely related approach figure illustration synthetic warping digit original image shown left right show three example warped digit corresponding displacement eld shown bottom displacement eld sampling random displacement smoothing convolution width respectively advantage approach correctly extrapolate well beyond range transformation included training however cult feature invariance also discard information useful discrimination
['tangent', 'propagation'] tangent propagation regularization encourage model invariant transformation input technique tangent propagation consider effect transformation particular input vector provided transformation continuous translation rotation mirror instance pattern sweep manifold within input space figure case simplicity suppose transformation single parameter might rotation angle instance subspace swept figure illustration input space showing effect continuous particular input vector dimensional transformation continuous variable applied cause sweep manifold locally effect transformation tangent vector vector result acting transformation tangent curve given directional derivative tangent vector point given transformation input vector network output vector general change derivative output respect given element matrix section result used modify standard error function local invariance data point addition original error function regularization function give total error function form regularization regularization function zero network function variant transformation pattern vector value parameter balance training data learning invariance property practical implementation tangent vector difference original vector corresponding vector transformation small value dividing figure regularization function network weight formalism derivative respect network weight easily extension exercise technique section transformation parameter case translation combined rotation image manifold dimensionality corresponding regularizer given term form transformation several transformation considered time network made invariant separately locally invariant combination transformation figure illustration showing original image hand written digit tangent vector corresponding clockwise rotation result small contribution tangent vector original image giving degree true image rotated comparison related technique tangent distance used build invariance property method
['training', 'transformed', 'data'] training data seen encourage invariance model formation expand training version original input pattern show approach closely related technique tangent propagation bishop section shall consider transformation single parameter function shall also consider error function error function untransformed input written data limit form section considered network single output order keep notation uncluttered consider number copy data point perturbed transformation parameter drawn distribution error function expanded data written assume distribution zero mean small variance considering small transformation original input vector expand transformation function series power give second derivative respect expand model function give substituting mean error function expanding distribution transformation zero mean also shall denote term average error function becomes original error regularization term take form integration simplify regularization term section function error given average target value error equal plus term network function total error form thus leading order term regularizer left equivalent tangent propagation regularizer consider special case transformation input simply addition random noise regularizer take form exercise known regularization bishop derivative regularizer respect network weight found extended algorithm bishop small noise amplitude regularization related addition random noise input shown improve generalization appropriate circumstance
['convolutional', 'networks'] convolutional network another approach model invariant certain transformation input build invariance property structure neural work basis convolutional neural network widely applied image data consider task digit input image intensity value desired output posterior distribution digit class know identity digit invariant translation scaling well small rotation furthermore network must also exhibit invariance subtle transformation elastic deformation kind figure simple approach would treat image input fully connected network kind shown figure given large training network could principle yield good solution problem would learn appropriate invariance example however approach property image nearby strongly correlated distant many modern approach computer vision exploit property local feature depend small image information feature later stage order detect feature input image convolutional layer layer figure diagram part convolutional neural network showing layer unit layer unit several successive pair layer used ultimately yield information image whole also local feature useful region image likely useful region image instance object interest notion incorporated convolutional neural network three mechanism local receptive eld weight structure convolutional network figure convolutional layer unit organized plane feature unit feature take input small subregion image unit feature constrained share weight value instance feature might consist unit grid unit taking input patch image whole feature therefore adjustable weight parameter plus adjustable bias parameter input value patch linearly combined weight bias result sigmoidal think unit feature detector unit feature detect pattern different location input image weight evaluation activation unit equivalent convolution image intensity kernel weight parameter input image activation feature amount otherwise unchanged basis approximate invariance network output translation distortion input image typically need detect multiple feature order build effective model generally multiple feature map convolutional layer weight bias parameter output convolutional unit form input layer network feature convolutional layer plane unit layer unit take input small receptive corresponding feature convolutional layer unit perform instance unit might take input unit region corresponding feature would compute average input adaptive weight addition adaptive bias parameter sigmoidal nonlinear activation function receptive eld chosen contiguous nonoverlapping half number row column layer convolutional layer response unit layer relatively insensitive small shift image corresponding region input space practical architecture several pair convolutional sampling layer stage degree invariance input formation previous layer several feature map given convolutional layer plane unit previous layer gradual reduction spatial resolution number feature layer network would typically fully connected fully adaptive layer output case cation whole network trained error minimization evaluate gradient error function slight cation usual algorithm ensure constraint local receptive eld number weight exercise network smaller network fully connected furthermore number independent parameter learned data much smaller still substantial number constraint weight
['soft', 'weight', 'sharing'] soft weight reduce effective complexity network large number weight constrain weight within certain group equal technique weight section building translation invariance network used image interpretation cable however particular problem form constraint advance consider form soft weight hard constraint equal weight form regularization group weight similar value furthermore division weight group mean weight value group spread value within group determined part learning process recall simple weight decay regularizer given negative prior distribution weight weight value form several group rather group instead probability distribution mixture section variance component well considered adjustable parameter determined part learning process thus probability density form taking negative logarithm lead regularization function form total error function given regularization error respect weight respect parameter mixture model weight constant parameter mixture model could determined algorithm chapter however weight learning process avoid instability joint optimization simultaneously weight parameter done standard optimization algorithm conjugate gradient method order minimize total error function necessary able evaluate derivative respect various adjustable parameter regard prior probability introduce corresponding posterior probability following given theorem form derivative total error function respect weight given exercise effect regularization term therefore pull weight towards force proportional posterior probability given weight precisely kind effect seeking derivative error respect also easily give exercise simple intuitive interpretation push towards aver weight value weighted posterior probability respective weight parameter component similarly derivative respect variance given exercise drive towards weighted average squared deviation weight around corresponding weighting given posterior probability weight component note practical implementation variable minimization respect sures parameter remain positive also effect discouraging pathological solution go zero corresponding component onto weight parameter value solution detail context mixture model section derivative respect need take account constraint follow interpretation prior probability done term auxiliary variable function given derivative error function respect take form exercise figure left show robot determined uniquely joint angle length arm know forward kinematics tice joint angle give rise desired effector position shown right inverse kinematics solution correspond elbow elbow elbow elbow therefore driven towards average posterior probability ponent
['mixture', 'density', 'networks'] mixture density network goal learning model conditional distribution many simple regression problem chosen however practical machine learning problem often distribution arise example inverse problem distribution multimodal case assumption lead poor simple example inverse problem consider kinematics robot figure forward problem exercise position given joint angle unique solution however practice wish move effector robot position must appropriate joint angle therefore need solve inverse problem solution seen figure forward problem often causality physical system unique solution instance pattern symptom human body presence particular disease pattern however typically solve inverse problem trying predict presence disease given symptom forward problem inverse problem multiple instance several different disease result symptom example kinematics geometrical equation multimodality readily apparent however many machine learning problem presence multimodality particularly problem space high obvious tutorial purpose however shall consider simple problem easily visualize multimodality data problem sampling variable uniformly interval give value corresponding target value figure left data simple forward problem curve show result neural network error function corresponding inverse problem shown right role work trained error function give poor data multimodality data function uniform noise interval inverse problem keeping data point role figure show data set forward inverse problem along result neural network hidden unit single linear output unit error function least square maximum likelihood assumption lead poor model highly inverse problem therefore seek general framework conditional probability distribution mixture model well component density function input vector giving rise mixture density network given value mixture model general formalism arbitrary conditional density function provided consider network framework arbitrary conditional shall develop model explicitly component example model since noise variance data function input vector instead component distribution target variable binary rather continuous also specialized case isotropic variance component although mixture density network readily extended allow general covariance matrix covariance factorization even isotropic component conditional distribution assume factorization respect component contrast standard regression model consequence mixture distribution take various parameter mixture model namely mean variance figure mixture density network represent general conditional probability density considering parametric mixture model distribution whose parameter determined output neural network take input vector output conventional neural network take input structure mixture density network figure mixture density network closely related mixture expert section principle difference mixture density network function used predict parameter component density well nonlinear hidden unit amongst function neural network figure example network sigmoidal tanh hidden unit component mixture model component network output unit activation determine output determine kernel width output determine component kernel total number network output given usual output network simply conditional mean target variable must satisfy constraint output similarly variance must satisfy term exponential corresponding network activation finally mean real component directly network output activation adaptive parameter mixture density network comprise vector weight bias neural network maximum likelihood equivalently error function negative logarithm likelihood independent data error function take form made dependency explicit order minimize error function need calculate derivative error respect component standard procedure provided obtain suitable sion derivative error respect activation represent error signal pattern output unit back hidden unit error function derivative usual error function composed term training data point consider derivative particular pattern derivative pattern dealing mixture distribution convenient view prior probability introduce corresponding posterior probability given derivative respect network output activation governing given exercise similarly derivative respect output activation ponent mean given exercise finally derivative respect output activation compo variance given exercise figure plot function three kernel function mixture density network trained data shown figure model three compo us layer tanh unit hidden layer nine output corresponding mean variance gaus component small large value conditional probability density target data unimodal high value prior probability intermediate conditional trimodal three comparable value plot mean colour plot contour corresponding probability density data mixture network plot proximate conditional mode shown point conditional density illustrate mixture density network ample inverse problem shown figure plot mean conditional density contour corresponding shown figure output neural network hence parameter mixture model necessarily continuous function input variable however figure model able produce conditional density unimodal value trimodal value amplitude component mixture density network trained predict conditional density function target data given value input vector conditional density complete description generator data problem value output vector concerned density function calculate quantity interest different application mean corresponding conditional average target data given used standard network trained least square conditional mean mixture density network reproduce conventional result special case course already noted multimodal distribution conditional mean limited value similarly evaluate variance density function average give exercise used general corresponding result variance function seen multimodal distribution conditional mean give poor representation data instance simple robot shown figure need pick possible joint angle setting order achieve desired location whereas average solution solution case conditional mode value conditional mode mixture density network simple analytical solution would require numerical iteration simple alternative take mean probable component value shown data figure
['bayesian', 'neural', 'networks'] neural network discussion neural network maximum like determine network parameter weight bias likelihood maximum posterior approach regularizer logarithm prior parameter however treatment need marginalize distribution parameter order make prediction section solution simple linear regression model assumption noise posterior could exactly predictive could also found closed form case network highly nonlinear dependence network function parameter value mean exact treatment longer found fact distribution corresponding multiple local minimum error function technique variational inference chapter applied neural network approximation posterior distribution camp also full covariance barber bishop barber bishop complete treatment however based approximation form basis discussion given approximate posterior distribution mode true posterior furthermore shall assume covariance gaus small network function approximately linear respect parameter region parameter space posterior probability nonzero approximation obtain model analogous linear regression cation model chapter exploit result make evidence framework provide point estimate compare alternative model example network different number unit start shall discus regression case later consider cation cation task
['posterior', 'parameter', 'distribution'] posterior parameter distribution consider problem single continuous target variable vector input extension multiple target straightforward shall suppose conditional distribution mean given output neural network model precision inverse variance similarly shall choose prior distribution weight form data observation corresponding target value likelihood function given resulting posterior distribution consequence nonlinear dependence approximation posterior distribution approximation must local maximum posterior must done iterative numerical optimization usual convenient maximize logarithm posterior written form error function assuming moment maximum posterior denote standard nonlinear optimization algorithm conjugate gradient error evaluate derivative found mode build local approximation matrix second derivative negative posterior given matrix second derivative square error function respect component algorithm section corresponding approximation posterior given similarly predictive distribution respect posterior distribution however even approximation posterior integration still analytically intractable network function function make progress assume posterior distribution small variance characteristic scale make series expansion network function around retain linear term approximation model distribution whose mean linear function form therefore make general result marginal give exercise variance given predictive distribution whose mean given network function parameter value variance term intrinsic noise target variable whereas second term express uncertainty uncertainty model parameter corresponding predictive distribution linear regression model given
['hyperparameter', 'optimization'] optimization assumed known make evidence framework section together approximation posterior obtain practical procedure choosing value marginal likelihood evidence network weight easily making approximation result exercise taking logarithm give total number parameter error function take form corresponding result linear regression model evidence framework make point estimate consider maximization respect done analogy linear regression case section eigenvalue equation matrix second derivative square error function analogy obtain effective number parameter section note result exact linear regression case nonlinear neural network however fact change cause change turn change eigenvalue therefore implicitly term derivative respect similarly evidence respect give formula linear model need alternate hyper parameter posterior distribution situation neural network model complex however multimodality posterior distribution consequence solution found posterior depend solution differ consequence interchange sign reversal symmetry hidden unit section identical prediction concerned irrelevant equivalent solution found however inequivalent solution well generally yield different value order compare different model example neural network differ number hidden unit need evaluate model evidence taking substituting value iterative optimization careful evaluation making bishop either case necessary evaluate determinant matrix problematic practice determinant unlike trace sensitive small eigenvalue often cult determine accurately approximation based local quadratic expansion around mode posterior distribution weight seen section given mode network member equivalent mode differ interchange symmetry hidden unit network different number unit taken account multiplying evidence factor
['bayesian', 'neural', 'networks', 'classification'] neural network cation used approximation develop treat neural network regression model discus cation framework arise applied cation shall sider network single logistic sigmoid output corresponding cation problem extension network output straightforward shall build extensively analogous result linear exercise cation model section encourage reader familiarize material section likelihood function model given target value note data point assumed correctly prior taken isotropic form stage framework model initialize determine parameter vector posterior distribution equivalent error function error combined standard algorithm section found solution weight vector next step matrix second derivative negative likelihood function done instance exact method outer product approximation given second derivative negative posterior written form approximation posterior given optimize maximize marginal likelihood easily shown take form exercise error function evidence function respect lead equation given evidence procedure determine figure synthetic data appendix finally need predictive distribution integration intractable network function figure illustration evidence framework applied synthetic data green curve show optimal boundary black curve show result network hidden unit maximum hood curve show regularizer evidence starting initial value note evidence dure greatly network approximation assume posterior distribution narrow hence make approximation improve however taking account variance posterior distribution case linear approximation network output used case regression would inappropriate logistic sigmoid output unit activation function output range instead make linear approximation output unit activation form vector found approximation posterior distribution model linear function appeal result section distribution output unit activation value induced distribution network weight given approximation posterior distribution given section distribution mean variance finally obtain predictive distribution must marginalize figure illustration approximation neural network hidden unit tanh activation function single output unit weight parameter found scaled conjugate gradient evidence framework left result simple approximation based point estimate parameter green curve show decision boundary contour correspond output probability right corresponding result note effect spread contour make prediction dent input point posterior probability towards contour unaffected convolution logistic sigmoid intractable therefore apply approximation giving recall function figure show example framework applied synthetic cation data appendix exercise consider network function form hidden unit nonlinear activation function given logistic sigmoid function form show equivalent network exactly hidden unit activation function given tanha tanh hint relation tanha show parameter network differ linear transformation show likelihood function conditional distribution neural network equivalent error function consider regression problem multiple target variable assumed distribution target conditioned input vector form output neural network input vector weight vector covariance assumed noise target given independent observation write error function must order maximum likelihood solution assume known assume also determined data write expression maximum likelihood solution note optimization coupled contrast case independent target variable section consider binary cation problem target value network output suppose probability class label training data point incorrectly assuming independent identically distributed data write error function corresponding negative likelihood verify error function note error function make model robust incorrectly data contrast usual error function show likelihood neural network model network output interpretation equivalent minimization error function show derivative error function respect activation output unit logistic sigmoid activation function show derivative error function respect activation output unit activation function derivative logistic sigmoid activation function expressed term function value derive corresponding result tanh activation function error function binary cation problem network output activation function data target value derive correspond error function consider network output target value class class would appropriate choice output unit activation function consider matrix equation setting vector equal turn show positive eigenvalue positive consider quadratic error function matrix eigenvalue equation given show tour constant error ellipsis whose ax length inversely proportional square root corresponding eigenvalue considering local expansion error function stationary point show necessary condition stationary point local minimum error function matrix positive show consequence symmetry matrix number independent element quadratic error function given making expansion verify term cancel side section derived procedure matrix neural network procedure derive alternative formalism based forward propagation equation outer product approximation matrix neural network error function given extend result case multiple output consider squared loss function form parametric function neural network result show function error given conditional expectation given result show second derivative respect element vector given note sample obtain consider network form shown figure addition extra parameter corresponding connection directly input output extending discussion section write equation derivative error function respect additional parameter derive expression outer product approximation matrix network single output logistic sigmoid activation function error function corresponding result error function derive expression outer product approximation matrix network output activation function error function corresponding result square error function extend expression outer product approximation matrix case output unit hence derive recursive expression analogous number pattern similar sion number output result together identity sequential update expression analogous inverse extra pattern extra output derive result element matrix network application chain rule extend result section exact network include connection directly input output verify network function invariant transformation applied input provided weight bias simultaneously similarly show network output according weight bias consider quadratic error function form minimum matrix positive constant suppose initial weight vector chosen origin simple gradient descent step number learning rate assumed small show step component weight vector parallel written eigenvalue respectively show give provided suppose training number step show component weight vector parallel satisfy compare result discussion section regularization simple weight decay hence show analogous regularization param result also show effective number parameter network training progress consider arbitrary topology trained tangent propagation error function function given show regularization term written pattern term form differential operator acting forward propagation equation operator show forward propagation following equation variable show derivative respect weight network written form write equation hence derive back propagation equation evaluation consider framework training data special case transformation simply addition random noise distribution zero mean unit covariance following argument analogous section show resulting regularizer form consider neural network convolutional network section multiple weight constrained value discus standard algorithm must order ensure constraint derivative error function respect adjustable parameter network verify result verify result verify result show derivative respect auxiliary parameter given hence making constraint derive result write pair equation express robot shown figure term joint angle length link assume origin system given attachment point lower equation forward kinematics robot derive result derivative error function respect network output activation mixture density network derive result derivative error function respect network output activation component mean mixture density network derive result derivative error function respect network output activation component variance mixture density network verify result conditional mean variance mixture density network model general result derive predictive distribution approximation neural network model make approximation result show evidence function neural network model outline cation framework neural network section handle problem network activation function following analogous step given section regression network derive result marginal likelihood case work error function function method chapter considered linear parametric model regression cation form input output vector adaptive parameter learning phase training data used either obtain point estimate parameter vector determine posterior distribution vector training data prediction input based purely learned parameter vector approach also used nonlinear parametric model neural network chapter however class pattern recognition technique training data point subset kept used also prediction phase instance probability density model comprised linear combination section kernel function training data point similarly section simple technique cation nearest involved test vector label example training example method involve entire training order make prediction future data point typically require metric measure similarity vector input space generally fast train slow making prediction test data point many linear parametric model recast equivalent dual prediction also based linear combination kernel function training data point shall model based nonlinear feature space kernel function given relation kernel symmetric function argument kernel concept tern recognition context method potential function analogy electrostatics although many year machine learning context large margin boser giving rise technique support vector machine since considerable interest topic chapter term theory application cant development extension kernel handle symbolic object thereby greatly expanding range problem example kernel function considering identity feature space case shall refer linear kernel concept kernel inner product feature space build interesting extension many algorithm making kernel trick also known kernel substitution general idea algorithm input vector form scalar product replace scalar product choice kernel instance technique kernel substitution applied principal component analysis order develop nonlinear variant section example kernel substitution include kernel fisher discriminant numerous form kernel function common shall counter several example chapter many property function difference argument known stationary kernel invariant translation input space specialization homogeneous kernel also known dial basis function depend magnitude distance typically section argument recent textbook kernel method
['dual', 'representations'] dual representation many linear model regression cation term dual representation kernel function naturally concept play important role consider support vector machine next chapter consider linear regression model whose parameter determined error function given gradient respect equal zero solution take form linear combination vector function form design matrix whose given vector instead working parameter vector reformulate least square algorithm term parameter vector giving rise dual substitute obtain gram matrix symmetric matrix element kernel function term gram matrix error function written setting gradient respect zero obtain following substitute back linear regression model obtain following prediction input vector element thus dual formulation solution problem expressed entirely term kernel function known dual formulation solution expressed linear combination element recover original formulation term parameter vector note prediction given linear combination exercise target value training fact already result slightly different notation section dual formulation determine parameter vector matrix whereas original parameter space formulation invert matrix order determine typically much dual formulation seem particularly useful however advantage dual formulation shall expressed entirely term kernel function therefore work directly term kernel avoid explicit introduction feature vector implicitly feature space high even dimensionality existence dual representation based gram matrix property many linear model section develop dual exercise probabilistic linear model regression technique process duality also play important role discus support vector machine chapter
['constructing', 'kernels'] kernel order exploit kernel substitution need able construct valid kernel function approach choose feature space corresponding kernel figure kernel function input space basis function alternative approach construct kernel function directly case must ensure function choose valid kernel word scalar product perhaps dimensional feature space simple example consider kernel function given figure illustration construction kernel function starting corresponding basis column lower plot show kernel function plotted function upper plot show corresponding basis function given polynomial left column column logistic right column take particular case input space expand term thereby identify corresponding nonlinear feature feature take form therefore possible second order term weighting tween generally however need simple test whether function valid kernel without construct function explicitly necessary condition function valid kernel gram matrix whose element given positive possible choice note positive matrix thing matrix whose element appendix powerful technique kernel build simpler kernel building block done following property technique kernel given valid kernel following kernel also valid constant function polynomial function valid kernel symmetric positive matrix variable necessarily disjoint valid kernel function respective space property embark construction complex kernel appropriate application require kernel symmetric positive express appropriate form similarity according intended application consider common example kernel function extensive discus sion kernel engineering simple polynomial kernel term degree consider slightly generalized kernel corresponding feature linear term well term order similarly order instance image kernel particular weighted possible product image second image similarly include term degree considering result combining kernel valid kernel function another commonly used kernel take form often kernel note however context probability density hence normalization valid kernel expanding square give making together validity linear kernel note feature vector kernel dimensionality exercise kernel restricted distance kernel substitution replace nonlinear kernel obtain important contribution arise kernel viewpoint sion input symbolic rather simply vector real number kernel function object diverse graph set string text consider instance space possible subset subset simple choice kernel would intersection set number subset valid kernel function shown correspond inner product feature space exercise powerful approach construction kernel start probabilistic generative model apply generative model discriminative setting generative model deal naturally missing data case hidden model handle sequence length contrast discriminative model generally give better performance discriminative task generative model therefore interest combine approach combine generative model kernel kernel discriminative approach given generative model kernel clearly valid kernel function interpret inner product feature space say input similar high probability extend class kernel considering sum product different probability distribution positive weighting form equivalent overall multiplicative constant mixture distribution component factorize index role latent variable input give large value kernel function section hence appear similar cant probability range different component taking limit also consider kernel form continuous latent variable suppose data ordered sequence length observation given popular generative model sequence hidden model express distribution section corresponding sequence hidden state approach kernel function measuring similarity sequence extending mixture representation give sequence hidden sequence model easily extended allow sequence length alternative technique generative model kernel function known fisher kernel consider parametric generative model vector parameter goal kernel measure similarity input vector induced generative model consider gradient respect vector feature space dimensionality particular consider fisher score fisher kernel fisher information matrix given expectation respect distribution perspective information geometry differential geometry space model parameter note presence fisher information matrix cause kernel invariant nonlinear density model exercise practice often infeasible evaluate fisher information matrix approach simply replace expectation fisher sample average giving covariance matrix fisher score fisher kernel whitening score simply omit fisher section information matrix altogether kernel application fisher kernel document retrieval given example kernel function sigmoidal kernel given tanh whose gram matrix general positive form kernel however used practice possibly give kernel expansion support vector machine super resemblance neural network model shall limit number basis function neural network appropriate prior process thereby providing link neural network kernel method section
['radial', 'basis', 'function', 'networks'] radial basis function network chapter regression model based linear combination basis function although discus detail form basis function might take choice widely used radial basis function property basis function radial distance typically historically radial basis function purpose exact interpolation given input vector along corresponding target value goal smooth function every target value exactly linear combination radial basis function every data point value found least square number constraint result function every target value exactly pattern recognition application however target value generally noisy exact interpolation undesirable solution expansion radial basis function also arise regularization theory bishop error function regularizer term differential operator optimal solution given expansion green function operator analogous discrete matrix basis function data point differential operator isotropic green function depend radial distance corresponding data point presence regularizer solution longer training data exactly another motivation radial basis function come consideration interpolation problem input rather target variable noisy bishop noise input variable variable distribution error function becomes calculus variation optimize respect function appendix give exercise basis function given basis function every data point known model derived different perspective section noise distribution isotropic function basis function radial note basis function value effect normalization shown figure normal sometimes used practice region input space basis function take small value would necessarily lead region either small purely bias parameter another situation expansion radial basis function arise application kernel density estimation problem regression shall discus section basis function associated every data point model costly evaluate making prediction data point model therefore moody darken retain sion radial basis function number basis function smaller number data point typically number basis function location determined based input data alone basis function kept determined least square usual linear equation section figure plot basis function left together corresponding basis function right way choosing basis function randomly chosen subset data point systematic approach orthogonal least square sequential selection process step next data point chosen basis function give reduction error value expansion determined part algorithm clustering algorithm also used give basis function section longer coincide training data point
['nadaraya', 'watson', 'model'] model section prediction linear regression model input take form linear combination training target value given equivalent kernel equivalent kernel summation constraint motivate kernel regression model different perspective starting kernel density estimation suppose training density estimator model joint distribution section component density function component data point expression regression function corresponding conditional average target variable conditioned input variable given assume simplicity component density function zero mean value simple change variable obtain kernel function given result known model kernel regression kernel function prop giving weight data point close note kernel summation constraint figure illustration kernel regression model isotropic kernel sinusoidal data original sine function shown green curve data point shown blue isotropic kernel resulting regression function given mean shown line along region conditional distribution shown shading blue ellipse around data point show standard deviation contour corresponding kernel appear noncircular different scale horizontal vertical ax fact model conditional expectation also full conditional distribution given expectation illustration consider case single input variable given isotropic variable variance corresponding conditional distribution given gaus mixture shown together conditional mean sinusoidal exercise synthetic data figure obvious extension model allow form gaus component instance different variance parameter input target variable generally could model joint distribution mixture model trained technique chapter mani jordan corresponding conditional distribution latter case longer representation term kernel training data point however number component mixture model smaller number training point resulting model faster evaluate test data point thereby accepted computational cost training phase order model faster making prediction
['gaussian', 'processes'] process section kernel concept duality probabilistic model regression extend role kernel discriminative model leading framework process shall thereby kernel arise naturally setting chapter considered linear regression model form vector parameter vector nonlinear basis function depend input vector prior distribution induced corresponding prior distribution function given training data posterior distribution thereby corresponding posterior distribution regression function turn addition noise predictive distribution input vector process viewpoint dispense parametric model instead prior probability distribution function directly sight might seem cult work distribution uncountably space function however shall training need consider value function discrete input value corresponding training test data point practice work space model equivalent process widely studied many eld instance literature process regression known similarly moving aver model radial basis function network form process model review process machine learning perspective found comparison process model alternative approach given also recent textbook process
['linear', 'regression', 'revisited'] linear regression order motivate process viewpoint return linear regression example predictive distribution working term distribution function provide example process consider model term linear combination basis function given element vector input vector weight vector consider prior distribution given isotropic form precision inverse variance distribution given value function probability distribution therefore probability distribution function practice wish function value example training data point therefore interested joint distribution function denote vector element vector given design matrix element distribution first note linear combination distributed variable given element hence gaus therefore need mean covariance given exercise gram matrix element kernel function model particular example process eral process probability distribution function value arbitrary point jointly distribution case input vector also known random generally stochastic process giving joint probability distribution value consistent manner point stochastic process joint distribution variable completely statistic namely mean covariance application prior knowledge mean symmetry take zero equivalent choosing mean prior weight value zero basis function viewpoint cation process giving covariance value given kernel function case process linear regression model weight prior kernel function given also kernel function directly rather indirectly choice basis function figure show sample function drawn gaus process different choice kernel function kernel form second exponential kernel given process originally describe motion figure sample gaus process left exponential kernel right
['gaussian', 'processes', 'regression'] process regression order apply process model problem regression need take account noise target value given random noise variable whose value chosen inde pendently observation shall consider noise process distribution precision noise noise independent data point joint distribution target value conditioned value given isotropic form unit matrix process marginal distribution given whose mean zero whose covariance gram matrix kernel function typically chosen express property point similar corresponding value strongly correlated dissimilar point notion similarity depend application order marginal distribution conditioned input value need integrate done making result section model marginal distribution given covariance matrix element result fact source randomness namely associated associated independent covariance simply widely used kernel function process regression given exponential quadratic form addition constant linear term give note term parametric model linear function input variable sample prior plotted various value parameter figure figure show point pled joint distribution along corresponding value used process viewpoint build model joint distribution set data point goal regression however make prediction target variable input given training data suppose corresponding input value comprise training goal predict target variable input vector evaluate predictive note distribution conditioned also variable however keep notation simple show variable explicitly conditional distribution begin writing joint distribution vector apply result section obtain conditional figure joint distribution given covariance matrix element given joint distribution apply result section conditional distribution partition covariance matrix covariance matrix element given vector element scalar figure sample process prior covariance function title plot result distribution distribution mean covariance given result process regression vector function test point input value predictive whose mean variance depend example process regression shown figure restriction kernel function covariance matrix given must positive eigenvalue corresponding eigenvalue therefore kernel matrix positive pair point eigenvalue zero still give rise positive eigenvalue restriction kernel function exploit technique section construct figure illustration sampling data point process blue curve show sample process prior function point show value function value correspond value shown green independent noise suitable kernel note mean predictive distribution written form component thus kernel function distance obtain expansion radial basis function result predictive distribution ce regression arbitrary kernel function particular case kernel function term basis function derive result previously section linear regression starting process viewpoint exercise model therefore obtain predictive distribution either taking parameter space viewpoint linear regression result taking function space viewpoint process result central computational operation process involve inversion matrix size standard method require computation contrast basis function model invert matrix size computational complexity note viewpoint matrix inversion must given training test point method require multiply cost process case linear basis model number basis function smaller number data point work basis function figure illustration mechanism process regression case training point test point lip show contour joint training data point condition value correspond vertical blue line tain shown function green curve framework however advantage process viewpoint consider covariance function expressed term number basis function large training data set however direct application process method become infeasible range approximation scheme better scaling training size exact approach practical issue application process bishop process regression case single variable extension formalism multiple target variable known straightforward various extension gaus exercise figure illustration process applied sinusoidal data figure three rightmost data point green curve show sinusoidal function data point shown blue sampling addition noise line show mean process predictive shaded region plus minus standard deviation notice uncertainty increase right data point process regression also considered purpose distribution manifold unsupervised learning bishop solution stochastic differential equation
['learning', 'hyperparameters'] learning prediction process model depend part choice covariance function practice rather covariance function prefer parametric family function infer parameter value data parameter govern thing length scale correlation precision noise correspond standard parametric model technique learning based evaluation likelihood function ce model approach make point estimate likelihood function regression problem analogous type maximum like procedure linear regression model maximization likelihood section done optimization algorithm conjugate gradient fletcher wright bishop likelihood function process regression model easily standard form distribution giving nonlinear optimization also need gradient likelihood respect parameter vector shall assume evaluation derivative straightforward would case covariance considered chapter making result derivative together result derivative obtain general function multiple straightforward introduce prior maximize method fully treatment need evaluate weighted product prior likelihood general however exact intractable must resort approximation process regression model give predictive distribution whose mean variance function input vector however assumed contribution predictive variance additive noise parameter constant problem known noise variance also depend model extend figure sample prior process kernel function given left plot right plot process framework second process represent dependence input variance hence process model
['automatic', 'relevance', 'determination'] automatic relevance determination previous section maximum likelihood could used termine value correlation parameter process technique usefully extended separate parameter input variable result shall optimization parameter maximum likelihood relative importance different input data exam process context automatic relevance determination originally framework neural network neal mechanism appropriate input preferred section consider process input space kernel function form sample resulting prior function shown different setting precision parameter figure parameter becomes small function becomes relatively insensitive corresponding input variable parameter data maximum likelihood becomes possible detect input variable little effect predictive distribution corresponding value small useful practice input simple synthetic data three input figure target variable sampling value function figure illustration automatic determination gaus process synthetic prob three input curve show corresponding value green blue number iteration marginal likelihood detail given text note logarithmic scale vertical axis noise value given corresponding value noise value independent thus good predictor noisy predictor chance correlation marginal likelihood process parameter scaled conjugate gradient algorithm figure relatively large value much smaller value becomes small irrelevant framework easily incorporated kernel give following form kernel function found useful application process range regression problem dimensionality input space
['gaussian', 'processes', 'classification'] process cation probabilistic approach cation goal model posterior probability target variable input vector given training data probability must interval whereas process model make prediction entire real axis however easily adapt process cation problem transforming output process appropriate nonlinear activation function consider problem target variable process function transform function logistic sigmoid given obtain stochastic process function case input space figure probability figure left plot show sample process prior function right plot show result transforming sample logistic sigmoid function target variable given distribution usual denote training input corresponding target variable also consider single test point target value goal determine predictive distribution left input variable implicit introduce process prior vector compo turn process training data obtain predictive process prior take form unlike regression case covariance matrix longer noise term assume training data point correctly ever numerical reason convenient introduce term parameter covariance matrix positive thus covariance matrix element given positive kernel function kind considered section value typically advance shall assume kernel function vector parameter shall later discus learned training data problem predict value given predictive distribution given integral analytically intractable method neal alternatively consider technique based analytical approximation section derived approximate formula convolution logistic sigmoid distribution result evaluate integral provided approximation posterior distribution usual cation approximation posterior distribution true posterior tend number data point increase consequence central limit theorem case process number variable section number data point argument apply directly however consider increasing number data point falling region space corresponding uncertainty function decrease leading asymptotically barber three different approach approximation considered technique based variational inference section make local variational bound logistic sigmoid product sigmoid function product thereby approach also yield lower bound likelihood function variational framework process cation also extended problem approximation function second approach us expectation propagation section true posterior distribution unimodal shall shortly expectation propagation approach give good result
['laplace', 'approximation'] approximation third approach process cation based approximation consider detail order evaluate predictive section distribution seek approximation posterior distribution theorem given used conditional distribution result ce regression give therefore evaluate integral approximation posterior distribution standard result convolution distribution prior given process covariance data term assuming independence data point given obtain approximation expanding logarithm additive normalization constant given quantity first need mode posterior distribution evaluate gradient given vector element simply mode setting gradient zero resort iterative scheme based method give rise iterative least square algorithm second section derivative also require approximation anyway given diagonal matrix element used result derivative logistic sigmoid function note diagonal element range hence positive matrix hence inverse positive construction positive matrix also positive exercise matrix positive posterior distribution convex therefore single mode global maximum posterior distribution however function formula iterative update equation given exercise anew equation converge mode denote mode gradient vanish hence satisfy found mode posterior evaluate matrix given element proximation posterior distribution given nana combine hence evaluate integral model general result give exercise varan distribution approximate integral result logistic regression model section interested decision boundary correspond need consider mean ignore effect variance also need determine parameter covariance function approach maximize likelihood function given need expression likelihood gradient desired suitable regularization term also added leading maximum likelihood solution likelihood function integral analytically intractable make result obtain following approximation likelihood function also need evaluate gradient respect parameter vector note change cause change leading additional term gradient thus differentiate respect obtain set term dependence covariance matrix rest dependence term explicit dependence found together result given compute term dependence note approximation zero gradient give contribution gradient result dependence leaf following contribution derivative respect component used result together evaluate derivative respect differ relation respect give give combining evaluate gradient likelihood function used standard nonlinear optimization order determine value illustrate application approximation ce synthetic data shown figure extension appendix approximation process class activation function straightforward barber figure illustration process cation showing data left together optimal decision boundary true distribution green decision boundary process black right posterior probability blue class together process decision boundary
['connection', 'neural', 'networks'] connection neural network seen range function neural network number hidden unit large network approximate given function arbitrary accuracy framework maximum likelihood number hidden unit need limited level dependent size training order avoid however perspective make little sense limit number parameter network according size training neural network prior distribution parameter vector conjunction network function produce prior distribution function vector network output neal shown broad class prior distribution distribution function neural network tend process limit noted however limit output variable neural network become independent great merit neural network output share hidden unit borrow statistical strength weight associated hidden unit output variable property therefore lost process limit seen process determined covariance kernel function given explicit form covariance case choice hidden unit activation function kernel function nonstationary expressed function difference consequence weight prior zero break translation invariance weight space working directly covariance function implicitly marginal distribution weight weight prior value determine length scale distribution function understood example figure case number hidden unit note marginalize analytically must instead resort technique kind section exercise consider dual formulation least square linear regression problem given section show solution component vector expressed linear combination element vector vector show dual dual formulation given original representation term parameter vector exercise develop dual formulation learning algorithm learning rule show learned weight vector written linear combination vector denote linear combination derive formulation learning algorithm predictive function term show feature vector form kernel function section input vector class nearest input vector training case distance metric rule term scalar product making kernel formulate general nonlinear kernel appendix give example matrix positive element negative eigenvalue hence positive find example converse property namely matrix positive eigenvalue least negative element verify result valid kernel verify result valid kernel verify result valid kernel verify result valid kernel verify result valid kernel show excellent choice kernel learning function given showing linear learning machine based kernel always solution proportional making expansion expanding middle factor power series show kernel expressed inner product feature vector consider space possible subset given show kernel function inner product feature space dimensionality subset element indexed subset given otherwise either subset equal show fisher kernel remains invariant make nonlinear transformation parameter vector function invertible differentiable write form fisher kernel case distribution mean covariance considering determinant gram matrix show positive kernel function inequality consider parametric model parameter vector together data input value nonlinear feature suppose dependence error function take form monotonically increasing function writing form show value take form linear combination basis function consider error function data noisy input distribution noise calculus vari minimize error function respect function hence show optimal solution given expansion form basis function given consider model input variable target variable component isotropic covariance variance matrix given unit matrix write expression conditional density conditional mean variance term kernel function another viewpoint kernel regression come consideration problem input variable well target variable corrupted additive noise suppose target value usual taking function point noise value directly however noise corrupted version random variable distribution consider observation together error function distribution input noise give respect function calculus variation appendix show optimal solution given kernel regression solution form kernel form verify result consider process regression model kernel function term nonlinear basis function show predictive distribution identical result section linear regression model note model predictive distribution necessary show conditional mean variance mean make matrix identity variance make matrix identity consider regression problem training input vector test input vector suppose process prior function derive expression joint predictive given value show marginal distribution test observation given usual process regression result consider process regression model target variable dimensionality write conditional distribution test input vector given training input vector corresponding target observation show diagonal matrix whose element satisfy positive show positive matrix positive formula derive iterative update formula mode posterior distribution process cation model result derive expression mean variance posterior distribution process cation model derive result likelihood function framework process cation similarly derive result term gradient likelihood machine previous chapter variety learning algorithm based linear kernel cant limitation many algorithm kernel function must possible pair training point infeasible training lead excessive computation time making prediction data point chapter shall look algorithm sparse solution prediction input depend kernel function subset training data point begin looking detail support vector machine popular year problem cation regression novelty detection important property support vector machine determination model parameter convex optimization prob local solution also global optimum discussion support vector machine make extensive multiplier reader review concept covered appendix additional support vector machine found muller decision machine provide posterior probability already bene probability alternative sparse kernel technique known relevance vector machine based formulation posterior section output well typically much solution maximum margin begin discussion support vector machine cation problem linear model form transformation made bias parameter explicit note shall shortly introduce dual representation expressed term kernel function work explicitly feature space training data input vector corresponding target value data point according sign shall assume moment training data linearly separable feature space least choice parameter function form point point training data point course exist many solution separate class exactly section algorithm solution number step solution however dependent arbitrary initial value chosen well order data point multiple solution training data exactly give generalization error support vector machine approach problem concept margin distance decision boundary sample figure support vector machine decision boundary chosen margin maximum margin solution computational learning theory also known statistical learning theory section ever simple insight origin maximum margin given tong koller consider framework cation based hybrid generative discriminative approach model distribution vector class density estimator kernel margin figure margin perpendicular distance decision boundary data point shown left margin lead particular choice decision boundary shown right location boundary determined subset data point known support vector circle common parameter together class prior decision boundary however instead optimal boundary determine best hyperplane probability error relative learned density model limit optimal hyperplane shown maximum margin intuition behind result reduced hyperplane increasingly dominated nearby data point relative distant one limit hyperplane becomes independent data point support vector shall figure respect prior parameter approach simple linearly separable data lead decision boundary lie middle region separating data point large margin solution similar behaviour recall figure perpendicular distance point hyper plane take form given furthermore interested solution data point thus distance point decision surface given margin given perpendicular distance point data wish optimize parameter order maximize distance thus maximum margin solution found taken factor outside optimization depend direct solution optimization problem would complex shall convert equivalent problem much easier solve note make distance point decision surface given unchanged freedom point surface case data point satisfy constraint known canonical representation decision hyperplane case data point equality hold constraint said active whereas remainder said inactive always least active constraint always point margin least active constraint optimization problem simply maximize equivalent solve optimization problem subject constraint given factor included later convenience example quadratic problem trying minimize quadratic function subject linear inequality constraint bias parameter however determined implicitly constraint require change change shall work shortly order solve constrained optimization problem introduce multiplier multiplier constraint giving appendix function note minus sign front multiplier term respect respect setting derivative respect equal zero obtain following condition condition give dual representation maximum margin problem maximize respect subject constraint kernel function take form quadratic problem optimize quadratic function subject inequality constraint shall discus technique quadratic problem section solution quadratic problem variable general computational complexity going dual formulation turned original optimization problem involved variable dual problem variable basis function whose number smaller number data point move dual problem disadvantageous however model kernel maximum margin applied feature space whose dimensionality number data point feature space kernel formulation also make clear role constraint kernel function positive function bounded giving rise well optimization problem order data point trained model evaluate sign expressed term parameter kernel function substituting give although widely considered mathematician born nineteen already made important contribution royal artillery school many year worked hard persuade move berlin eventually director mathematics berlin academy later life thanks personal intervention chemist discovered oxygen self later executed guillotine made contribution calculus variation foundation dynamic appendix show constrained optimization form condition case require following three property hold thus every data point either data point appear hence play role making prediction data point data point support vector satisfy correspond point maximum margin feature space figure property central practical applicability support vector machine model trained cant proportion data point support vector quadratic problem found value determine value threshold parameter support vector give index support vector although solve equation arbitrarily chosen support vector numerically stable solution multiplying making equation support vector give total number support vector later comparison alternative model express maximum margin term minimization error function simple quadratic regularizer form function zero otherwise constraint note long regularization parameter precise value play role figure show example cation resulting training port vector machine simple synthetic data kernel figure example synthetic data class dimension showing contour constant support vector machine gaus kernel function also shown decision boundary margin boundary port vector form although data linearly separable data space linearly separable nonlinear feature space implicitly nonlinear kernel function thus training data point perfectly original data space example also geometrical insight origin sparsity maximum margin hyperplane location support vector data point around freely long remain side margin region without decision boundary solution independent data point
['overlapping', 'class', 'distributions'] class distribution assumed training data point linearly separable feature space resulting support vector machine give exact separation training data original input space although corresponding decision boundary nonlinear practice however distribution overlap case exact separation training data lead poor generalization therefore need modify support vector machine allow training point case separable class implicitly used error function gave error data point zero error correctly model parameter maximize margin modify approach data point wrong side margin boundary penalty increase distance boundary subsequent optimization problem convenient make penalty linear function distance introduce slack variable slack variable training data point data point inside correct margin boundary point thus data point decision boundary point figure illustration slack variable data point circle around support vector exact cation constraint slack variable constrained satisfy data point correctly either margin correct side margin point inside margin rect side decision boundary data point wrong side decision boundary sometimes hard margin constraint give soft margin training data point note slack variable allow class distribution framework still sensitive outlier penalty cation increase linearly goal maximize margin softly point wrong side margin boundary therefore minimize parameter control slack variable penalty margin point upper bound number point parameter therefore analogous inverse regularization control training error model complexity limit recover support vector machine separable data wish minimize subject constraint together corresponding given multiplier corresponding condition given appendix optimize making give result eliminate obtain dual form identical separable case except constraint somewhat different constraint note multiplier furthermore together therefore minimize respect dual variable subject known constraint quadratic problem substitute prediction data point made interpret resulting solution subset data point case contribute predictive model data point constitute support vector hence must satisfy hence point margin point inside margin either correctly determine parameter note support vector hence satisfy numerically stable solution give index data point alternative equivalent formulation support vector machine known subject constraint approach advantage parameter upper bound fraction margin error point hence wrong side margin boundary lower bound fraction support vector example applied synthetic data shown figure kernel form used although prediction input made support vector training phase determination parameter make whole data important algorithm figure illustration applied data dimension support vector circle quadratic problem note objective function given quadratic local optimum also global optimum provided constraint convex region linear direct solution quadratic problem traditional technique often infeasible demanding computation memory requirement practical approach need found tech exploit fact value unchanged remove row column kernel matrix corresponding multiplier value zero full quadratic problem broken series smaller one whose goal eventually identify nonzero multiplier discard conjugate gradient although size matrix quadratic function number data point squared approximately number nonzero multiplier squared even memory cation decomposition method also solve series smaller quadratic problem designed size technique applied arbitrarily large data set however still numerical solution quadratic problematic expensive popular approach training support vector machine sequential minimal optimization take concept extreme limit multiplier time case subproblem thereby numerical quadratic altogether heuristic given choosing pair multiplier considered step practice found scaling number data point somewhere linear quadratic depending particular application seen kernel function correspond inner product feature space high even dimensionality working directly term kernel function without feature space explicitly might fore seem support vector machine somehow manage avoid curse case however constraint amongst section feature value restrict effective dimensionality feature space consider simple polynomial kernel expand term component kernel function therefore inner product feature space dimension input space feature space vector function however weighting different feature constrained form thus point original space would constrained exactly nonlinear manifold feature space already fact support vector machine provide probabilistic output instead make cation decision vector discus cation allow false positive false negative error ever wish module probabilistic system probabilistic prediction class label input address issue logistic sigmoid output previously trained support vector machine conditional probability assumed form value parameter found error function training pair value data used sigmoid need independent used train original order avoid severe stage approach equivalent assuming output support vector machine belonging class training procedure intended encourage give poor approximation posterior probability tipping
['relation', 'logistic', 'regression'] relation logistic regression separable case recast term minimization error function also allow highlight similarity difference logistic regression model section seen data point correct side margin boundary therefore satisfy figure plot hinge error function used support vector machine shown blue along error function logistic regression factor pass point shown also shown cation error black squared error green point thus objective function written overall multiplicative constant form hinge error function positive part hinge error function shape plotted figure approximation cation error error function ideally would like minimize also shown figure considered logistic regression model section found convenient work target variable comparison support vector machine reformulate maximum likelihood logistic regression target variable note given logistic sigmoid function used property logistic sigmoid function write construct error function taking negative logarithm likelihood function quadratic regularizer take form exercise comparison error function divide error function pass point error function also plotted figure similar form support vector error function difference region lead sparse solution logistic error hinge loss continuous cation error another continuous error function sometimes used solve cation problem squared error plotted figure property however increasing emphasis data point correctly long decision boundary correct side point strongly weighted expense point objective minimize cation rate monotonically decreasing error function would better choice
['multiclass', 'svms'] support vector machine fundamentally practice however often tackle problem class various therefore combining multiple order build commonly used approach construct separate model trained data class positive example data class negative example known approach however figure decision individual lead inconsistent result input assigned multiple class simultaneously problem sometimes making prediction input unfortunately heuristic approach problem different trained different task guarantee real valued quantity different appropriate scale another problem approach training set instance class equal number training data point individual trained data set negative example positive example symmetry original problem lost variant scheme modify target value positive class target negative class target single objective function training simultaneously based margin class however result much training instead separate optimization problem data point overall cost single optimization problem size must giving overall cost another approach train different class possible pair class test point according class high number vote approach sometimes figure lead ambiguity resulting cation also large approach training time approach similarly evaluate test point computation latter problem pairwise directed acyclic graph confused probabilistic graphical model leading class total test point pairwise need particular used depending path graph traversed different approach cation based code applied support vector machine generalization voting scheme approach general partition class used train individual class particular set response chosen together suitable scheme give robustness error ambiguity output individual although application cation problem remains open issue practice approach widely used spite formula practical limitation also support vector machine solve learning problem related probability density estimation instead density data however method smooth boundary region high density boundary chosen represent density probability data point drawn distribution land inside region given number advance restricted problem full density application approach problem support vector machine algorithm try hyperplane separate fraction training data origin time distance margin hyperplane origin look sphere feature space fraction data point kernel function algorithm equivalent
['svms', 'regression'] regression extend support vector machine regression problem time property simple linear regression section figure plot insensitive error function error increase early distance beyond region also shown quadratic error function green minimize error function given obtain sparse solution quadratic error function insensitive error function give zero error absolute difference tween prediction target simple example insensitive error function linear cost associated error outside insensitive region given otherwise figure therefore minimize error function given given convention inverse regularization parameter front error term optimization problem slack variable data point need slack variable point point figure condition target point inside tube slack variable point outside tube provided slack variable nonzero corresponding condition figure illustration regression showing regression curve together insensitive tube also shown exam slack variable point tube point tube point inside tube error function support vector regression written must subject constraint well multiplier substitute derivative respect zero giving result eliminate corresponding variable dual problem exercise anam respect kernel constrained maximization constraint note multiplier also together require constraint together condition substituting prediction input made expressed term kernel function corresponding condition state solution product dual variable constraint must vanish given obtain several useful result first note nonzero data point either lie upper boundary tube lie upper boundary similarly nonzero value point must either lower boundary tube furthermore constraint incompatible easily seen together strictly positive every data point either must zero support vector data point contribute prediction given word either point boundary tube outside tube point within tube sparse solution term predictive model involve support vector parameter found considering data point must must therefore satisfy obtain used obtain analogous result considering point practice better average estimate cation case alternative formulation regression parameter governing complexity intuitive interpretation particular instead width insensitive region instead parameter bound fraction point lying outside tube anam subject constraint shown data point falling outside insensitive tube least data point support vector either tube outside support vector machine solve regression problem sinusoidal data figure parameter appendix chosen hand practice value would typically determined cross validation figure illustration applied sinusoidal synthetic data kernel regression curve shown line insensitive tube shaded region also data point shown green support vector blue circle
['computational', 'learning', 'theory'] computational learning theory historically support vector machine largely theoretical framework known computational learning theory also time statistical learning theory rani origin valiant probably approximately correct learning framework goal framework understand large data need order give good generalization also give bound computational cost learning although consider suppose data size drawn joint distribution input variable class label restrict attention noise free situation class label determined unknown deterministic function learning function drawn space function basis training good generalization error rate threshold indicator function expectation respect quantity side random variable training framework hold probability greater data drawn randomly another parameter terminology probably mately correct come requirement high probability greater error rate small given choice model space given parameter learning aim provide bound minimum size data meet criterion quantity learning dimension dimension measure complexity space function framework extended space number function bound derived within framework often worst case apply choice distribution long training test example drawn independently choice function long application machine learning deal distribution cant example large region input space carry class label consequence lack assumption form distribution bound conservative word strongly overestimate size data set achieve given generalization performance reason bound found practical application attempt improve tightness bound framework distribution space function somewhat analogous prior treatment still possible choice although bound still conservative
['relevance', 'vector', 'machines'] relevance vector machine support vector machine used variety cation sion application nevertheless suffer number limitation several already chapter particular output represent decision rather posterior probability also originally class extension class prob complexity parameter well parameter case regression must found holdout method finally prediction expressed linear combination kernel function training data point positive relevance vector machine tipping sparse technique regression cation share many characteristic whilst principal limitation additionally typically lead much model resulting correspondingly faster performance test data whilst comparable generalization error contrast shall convenient introduce sion form consider extension cation task
['rvm', 'regression'] regression relevance vector machine regression linear model form studied chapter prior result sparse solution model conditional distribution target variable given input vector take form noise precision inverse noise variance mean given linear model form nonlinear basis function typically include constant term corresponding weight parameter bias relevance vector machine instance model mirror structure support vector machine particular basis function given kernel kernel associated data point training general expression take form bias parameter number parameter case form predictive model except subsequent analysis valid arbitrary choice basis function generality shall work form contrast restriction positive kernel basis function tied either number location training data point suppose given observation input vector denote collectively data matrix whose corresponding target value given thus likelihood function given next introduce prior distribution parameter vector chapter shall consider prior however differ introduce separate weight parameter instead single thus weight prior take form precision corresponding parameter shall maximize evidence respect cant proportion corresponding weight parameter posterior distribution concentrated zero basis function associated parameter therefore play role prediction made model effectively resulting sparse model result linear regression model posterior distribution weight take form mean covariance given design matrix element note case model symmetric kernel matrix element value determined type maximum likelihood also known evidence approximation maximize marginal section hood function weight parameter convolution readily exercise give marginal likelihood form matrix given goal maximize respect small cation result section evidence approximation linear regression model identify approach simply derivative marginal likelihood zero obtain following equation exercise component posterior mean quantity measure well corresponding parameter determined data section diagonal component posterior covariance given learning therefore proceeds choosing initial value mean covariance posterior respectively alternately posterior mean covariance suit able convergence criterion second approach algorithm approach value maximize evidence formally equivalent numerically however found exercise direct optimization approach corresponding give faster convergence tipping result optimization proportion driven large principle value weight parameter section corresponding posterior distribution mean variance zero thus parameter corresponding basis removed model play role making prediction input case model form input corresponding nonzero weight relevance vector mechanism automatic relevance determination support vector worth however mechanism sparsity probabilistic model automatic determination quite general applied model expressed adaptive linear combination basis function found value maximize marginal likelihood evaluate predictive distribution input given exercise thus predictive mean given equal posterior mean variance predictive distribution given given value familiar result context linear regression recall basis function predictive variance linear regression model becomes small region input space basis function case basis function data point model therefore become increasingly certain prediction outside domain data course undesirable predictive distribution process regression section figure illustration regression data kernel function used figure regression model mean predictive shown line standard deviation predictive distribution shown shaded region also data point shown green relevance tor blue circle note vector port vector suffer problem however computational cost making prediction process typically much higher figure show example applied sinusoidal regression data noise precision parameter also determined evidence maximization number relevance vector smaller number support vector used wide range regression cation task found give model typically order magnitude compact corresponding support vector machine resulting cant improvement speed test data remarkably greater sparsity little reduction generalization error corresponding principal disadvantage training function training time longer comparable model basis function inversion matrix size general computation case model noted technique training whose cost roughly quadratic course case always option starting smaller number basis function relevance vector machine parameter governing complexity noise variance determined automatically single training whereas support vector machine parameter generally found multiple training run furthermore next section shall derive alternative procedure training relevance vector machine training speed
['analysis', 'sparsity'] analysis sparsity noted mechanism automatic relevance determination cause subset parameter driven zero examine detail figure illustration mechanism sparsity linear regression model showing training vector target value given cross model basis vector poorly target data vector left model isotropic noise corresponding probable value right model value case ellipse unit distance taking value plot dashed green circle show contrition noise term value probability data probable solution basis vector removed mechanism sparsity context relevance vector machine process arrive faster procedure hyper parameter direct technique given proceeding mathematical analysis give informal insight origin sparsity linear model consider data observation together model single basis function along isotropic noise marginal likelihood given covariance matrix take form vector similarly notice process model covariance given particular observation goal marginal likelihood figure poor alignment direction training data vector corresponding driven basis vector model value always assign lower probability data thereby decreasing value density optimal value value would cause distribution elongated direction away data thereby increasing probability mass region away data hence reducing value density target data vector general case basis vector similar intuition hold namely particular basis vector poorly data vector likely model investigate mechanism sparsity mathematical general case basis function motivate analysis note result parameter term side also function result therefore resent implicit solution iteration would even determine single different approach optimization problem make explicit dependence marginal likelihood particular determine stationary point explicitly tipping tipping pull contribution matrix give column word vector element contrast matrix matrix contribution basis function removed matrix identity determinant inverse written result write marginal likelihood function form exercise simply marginal likelihood basis function quantity dependence quantity sparsity known quality shall large value relative value mean basis function figure plot marginal likelihood versus showing left single maximum right maximum likely model sparsity measure extent basis function overlap basis vector model quality measure alignment basis vector error training value vector prediction would result model vector tipping stationary point marginal likelihood respect occur derivative equal zero possible form solution solution conversely solve obtain solution figure relative size quality sparsity term whether particular basis vector model complete analysis tipping based second derivative marginal likelihood solution indeed unique maximum exercise note approach solution given value well providing insight origin sparsity analysis also lead practical algorithm cant speed advantage us candidate basis vector cycle turn decide whether vector included model resulting sequential sparse learning algorithm sequential sparse learning algorithm regression problem initialize initialize basis function included model evaluate along basis function select candidate basis function basis vector already included model update model evaluate remove basis function model regression problem update terminate otherwise note basis function already model action practice convenient evaluate quantity quality variable expressed form note write exercise involve basis vector correspond stage computation therefore scale like number active basis vector model typically much smaller number training pattern
['rvm', 'classification'] cation extend relevance vector machine framework cation prob prior weight probabilistic linear cation model kind studied chapter start consider prob binary target variable model take form linear combination basis function logistic sigmoid function logistic sigmoid function introduce prior weight vector obtain model considered already chapter difference model us prior separate precision associated weight parameter contrast regression model longer integrate analytically parameter vector follow tipping proximation applied closely related problem logistic section regression section begin vector given value build approximation posterior distribution thereby obtain approximation marginal likelihood maximization mate marginal likelihood lead value process repeated convergence consider approximation model detail value mode posterior distribution done iterative least square section need gradient vector matrix posterior distribution given exercise diagonal matrix element vector design matrix element used property derivative logistic sigmoid function convergence algorithm negative inverse covariance matrix approximation posterior distribution mode resulting approximation posterior distribution mean approximation setting zero giving mean covariance approximation form approximation evaluate marginal likelihood general result integral substitute derivative marginal likelihood respect equal zero obtain exercise give identical formula regression write approximate marginal likelihood form take form regression case apply analysis sparsity obtain fast learning algorithm fully optimize single step figure show relevance vector machine applied synthetic cation data relevance vector tend region appendix decision boundary contrast support vector machine consistent discussion sparsity basis function data point near boundary vector poorly training data vector potential advantage relevance vector machine make probabilistic prediction example used help construct emission density nonlinear extension linear dynamical system face video sequence section considered binary cation problem class make probabilistic approach section linear model form figure example relevance vector machine applied synthetic data plot show decision boundary data point relevance vector circle comparison result shown figure corresponding support vector machine show give much model plot show posterior probability given output proportion blue probability point belonging blue class combined function give output likelihood function given target value data point matrix element approximation used optimize tipping model found give approach cation pairwise method used support vector machine also prediction data point principal disadvantage matrix size number active basis function give additional factor computational cost training principal disadvantage relevance vector machine relatively long training time offset however avoidance run model complexity parameter furthermore yield model computation time test point usually important consideration practice typically much exercise suppose data input vector corresponding target value suppose model density input tor within class separately kernel density estimator kernel write minimum decision rule assuming class equal prior probability show also kernel chosen cation rule simply input vector class mean finally show kernel take form cation based mean feature space show side constraint arbitrary constant solution maximum margin hyperplane unchanged show irrespective dimensionality data space data data point class determine location hyperplane show value margin hyper plane given given subject constraint show value previous exercise also satisfy similarly show consider logistic regression model target variable given show negative likelihood addition quadratic regularization term take form consider regression support vector machine setting derivative respect zero back substituting eliminate corresponding variable show dual given regression support vector machine considered section show training data point similarly point verify result mean covariance posterior distribution weight regression derive result marginal likelihood function regression integral technique square exponential repeat exercise time make general result show direct maximization marginal likelihood regression relevance vector machine lead equation evidence framework regression formula marginal likelihood given extend approach inclusion given gamma distribution form obtain corresponding formula corresponding posterior probability respect derive result predictive distribution relevance vector machine regression show predictive variance given result show marginal likelihood written form sparsity quality factor respectively taking second derivative marginal likelihood regression respect show stationary point given maximum marginal likelihood together matrix identity show quantity written form show gradient vector matrix distribution cation relevance vector machine given verify maximization approximate marginal likelihood function cation relevance vector machine lead result model probability play central role modern pattern recognition seen chapter probability theory expressed term simple equation corresponding rule product rule probabilistic infer learning manipulation book matter complex amount repeated application equation could therefore proceed formulate solve complicated probabilistic model purely algebraic however shall highly advantageous augment analysis diagrammatic representation probability distribution probabilistic graphical model offer several useful property provide simple visualize structure probabilistic model used design motivate model insight property model conditional independence property inspection graph complex computation perform inference learning model expressed term graphical manipulation underlying mathematical expression carried along implicitly graph node also vertex connected link also known edge arc probabilistic graphical model node random variable group random variable link express probabilistic relation ship variable graph capture joint distribution random variable decomposed product factor depending subset variable shall begin network also known directed graphical model link graph particular arrow major class graphical model random eld also known undirected graphical model link carry arrow directional directed graph useful causal relationship random variable whereas undirected graph better soft random variable purpose inference problem often convenient convert directed undirected graph different representation factor graph chapter shall focus aspect graphical model application pattern recognition machine learning general treat graphical model found book jordan jordan
['bayesian', 'networks'] network order motivate directed graph describe probability distribution consider arbitrary joint distribution three variable note stage need specify anything vari whether discrete continuous indeed powerful aspect graphical model graph make probabilistic statement broad class distribution application product rule probability write joint distribution form second application product rule time second term right hand side give note decomposition hold choice joint distribution represent side term simple graphical model first introduce node random variable associate node corresponding conditional distribution side figure directed graphical model joint distribution three variable correspond decomposition side conditional distribution directed link arrow graph node corresponding variable distribution conditioned thus factor link node node whereas factor incoming link result graph shown figure link going node node node parent node node child node note shall make formal distinction node variable simply symbol refer interesting point note side symmetrical respect three variable whereas side indeed making decomposition implicitly chosen particular namely chosen different would different decomposition hence different graphical representation shall return point later moment extend example figure considering joint distribution variable given repeated application product rule probability joint distribution written product conditional distribution variable given choice represent directed graph node conditional distribution side node incoming link lower node graph fully connected link every pair node worked completely general joint distribution decomposition representation fully connected graph choice distribution shall shortly absence link graph interesting information property class distribution graph consider graph shown figure fully connected graph instance link shall graph corresponding representation joint probability distribution written term product conditional node graph conditional distribution conditioned parent corresponding node graph stance conditioned joint distribution variable figure example directed acyclic graph joint distribution variable corresponding decomposition joint distribution given therefore given reader take moment study carefully correspondence figure state general term relationship given directed graph corresponding distribution variable joint distribution graph given product node graph conditional distribution node conditioned variable corresponding parent node graph thus graph node joint distribution given parent equation express factorization property joint distribution directed graphical model although considered node correspond single variable equally well associate set variable variable node graph easy show representation right hand side always correctly provided individual conditional distribution exercise directed graph considering subject important namely must directed cycle word closed path within graph move node node along link follow direction arrow back starting node graph also directed acyclic graph dag equivalent statement exercise node link node lower node
['example', 'polynomial', 'regression'] example polynomial regression illustration directed graph describe probability consider polynomial regression model figure directed graphical model joint distribution corresponding polynomial regression model random variable model vector polynomial data addition model input data noise variance precision prior parameter model rather random variable random variable moment joint distribution given product prior conditional distribution joint distribution graphical model shown figure start deal complex model later book shall inconvenient write multiple node form explicitly figure therefore introduce graphical notation multiple node expressed compactly draw single representative node surround plate node kind graph figure obtain graph shown figure shall sometimes helpful make parameter model well stochastic variable explicit case becomes correspondingly make explicit graphical representation shall adopt convention random variable open circle deterministic parameter smaller solid circle take graph figure include deterministic parameter obtain graph shown figure apply graphical model problem machine learning pattern recognition typically random variable figure alternative compact representation graph shown figure plate node single example shown explicitly figure show model figure deterministic parameter shown explicitly smaller solid node value example variable training case polynomial curve graphical model denote variable shad corresponding node thus graph corresponding figure variable shown figure note value example latent variable also known hidden variable variable play crucial role many probabilistic model form focus chapter value desired evaluate posterior polynomial section moment note straightforward application theorem deterministic parameter order keep uncluttered general model parameter little direct interest ultimate goal make prediction input value suppose given input value wish corresponding probability fort conditioned data graphical model problem shown figure corresponding joint distribution random variable model conditioned deterministic parameter given figure figure node shaded indicate corresponding random vari training value figure polynomial regression model corresponding figure showing also input value together corresponding model prediction predictive distribution rule probability model parameter implicitly setting random variable value data detail calculation chapter
['generative', 'models'] generative model many situation wish draw sample given prob ability distribution although shall devote whole chapter detailed discussion sampling method instructive outline technique ancestral sampling particularly relevant graphical model consider joint distribution variable according corresponding directed acyclic graph shall suppose variable ordered link node lower node word node higher number parent goal draw sample joint distribution start node draw sample distribution call work node node draw sample conditional distribution parent variable value note stage parent value always available correspond lower node already technique sampling distribution detail chapter pled variable objective sample joint distribution obtain sample marginal corresponding subset variable simply take value node ignore value node example draw sample distribution simply sample full joint distribution retain value discard value figure graphical model process image object identity object discrete variable position orientation object continuous variable independent prior probability image vector intensity probability distribution dependent identity object well position orientation image object orientation position practical application probabilistic model typically higher variable corresponding terminal node graph represent observation node corresponding latent variable primary role latent variable allow complicated distribution variable term model simpler typically exponential family conditional distribution interpret model process data arose instance consider object recognition task data point image vector intensity object case latent variable might interpretation position orientation object given particular image goal posterior distribution object integrate possible position orientation represent problem graphical model form show figure graphical model capture causal process pearl data reason model often generative model contrast polynomial regression model figure generative probability distribution associated input variable possible generate synthetic data point model could make generative suitable prior distribution expense complex model hidden variable probabilistic model need however physical interpretation simply allow complex joint distribution simpler component either case technique ancestral sampling applied generative model mimic creation data would therefore give rise fantasy data whose probability distribution model perfect representation reality would data practice synthetic observation generative model prove informative understanding form probability distribution model
['discrete', 'variables'] discrete variable importance probability distribution member exponential family seen family many well section known distribution particular case although distribution relatively simple form useful building block complex probability figure graph general discrete variable total parameter dropping link node number parameter reduced distribution framework graphical model useful building block linked together model particularly nice property choose relationship tween pair directed graph conjugate shall several example shortly case particularly worthy note namely parent child node correspond discrete variable correspond variable case relationship extended hierarchically construct arbitrarily complex directed acyclic graph begin examining discrete case probability distribution single discrete variable possible state representation given parameter constraint value need order distribution suppose discrete variable state wish model joint distribution denote probability observing parameter component similarly joint distribution written parameter subject constraint parameter easily seen total number parameter must arbitrary joint distribution variable therefore exponentially number variable product rule factor joint distribution form graph link going node node shown figure marginal distribution parameter similarly conditional distribution cation parameter possible value total number parameter must joint distribution therefore suppose variable independent corresponding graphical model shown figure variable figure chain discrete node state cation parameter linearly length chain contrast fully graph node would param exponentially separate multinomial distribution total number parameter would distribution independent discrete variable state total number parameter would therefore linearly number variable graphical perspective reduced number parameter dropping link graph expense restricted class distribution generally discrete variable model joint distribution directed graph variable corresponding node conditional distribution node given subject usual normalization constraint graph fully connected completely general distribution parameter whereas link graph joint distribution product total number parameter graph level connectivity allow general distribution fully parameter general joint distribution illustration consider chain node shown figure marginal distribution parameter whereas distribution parameter give total parameter count quadratic linearly rather exponentially length chain alternative reduce number independent parameter model parameter also known tying parameter instance chain example figure arrange conditional distribution parameter together parameter governing distribution give total parameter must order joint distribution turn graph discrete variable model prior parameter graphical point view node additional parent distribution associated corresponding discrete node chain model figure corresponding model governing conditional distribution shown figure another exponential growth number parameter model discrete variable model conditional distribution instead complete table conditional probability value idea consider graph figure node represent binary variable parent variable single figure extension model figure include prior param governing discrete distribution figure figure parameter amongst conditional distribution probability giving parameter total parent node conditional distribution however would require parameter probability possible setting parent variable thus general number parameter specify conditional distribution grow exponentially tain parsimonious form conditional distribution logistic sigmoid function acting linear combination parent variable giving section logistic sigmoid dimensional vector parent state augmented additional variable whose value vector parameter restricted form conditional distribution general case number parameter linearly sense analogous choice restrictive form covariance matrix example diagonal matrix distribution motivation logistic sigmoid representation section figure graph parent child used illustrate idea conditional distribution discrete variable
['linear', 'gaussian', 'models'] model previous section construct joint probability distribution discrete variable variable node directed acyclic graph show expressed directed graph corresponding model component vari impose interesting structure distribution general diagonal covariance opposite several widely used technique example model probabilistic principal component analysis factor analysis linear system shall make extensive result section later chapter consider technique detail consider arbitrary directed acyclic graph variable node single continuous random variable distribution mean distribution taken linear combination state parent node node parameter governing mean variance conditional distribution joint distribution product node graph hence take form term independent quadratic function component hence joint distribution determine mean covariance joint distribution variable conditional state parent distribution form zero mean unit variance random variable satisfying element identity matrix taking expectation figure directed graph three variable missing link thus component starting node working graph assume node node higher number parent similarly obtain element covariance matrix form recursion relation covariance similarly starting node consider extreme case first suppose link graph therefore isolated node case parameter parameter parameter recursion relation mean given covariance matrix diagonal form joint distribution total parameter inde pendent distribution consider fully connected graph node lower node parent matrix entry hence lower triangular matrix entry leading diagonal total number parameter taking number element matrix account absence element lead diagonal dividing matrix element diagonal giving total total number independent parameter covariance matrix therefore corresponding general symmetric covariance matrix section graph intermediate level complexity correspond joint gaus distribution partially constrained covariance matrix consider ample graph shown figure link missing variable recursion relation mean covariance joint distribution given exercise readily extend graphical model case node graph represent variable case write conditional distribution node form matrix different easy verify joint distribution variable note already example relationship conjugate prior mean section variable distribution joint distribution therefore simple graph node parent node mean distribution parameter prior value unknown treat perspective prior sometimes given distribution type construction extended principle level illustration hierarchical model shall encounter example later chapter
['conditional', 'independence'] conditional independence important concept probability distribution multiple variable conditional independence consider three variable suppose conditional distribution given depend value conditionally independent given expressed slightly different consider joint distribution conditioned write form used product rule probability together thus conditioned joint distribution prod marginal distribution marginal distribution conditioned say variable statistically independent given note conditional independence require figure three example graph three variable used discus conditional independence property directed graphical model equivalently must hold every possible value value shall sometimes shorthand notation conditional independence conditionally independent given equivalent conditional independence property play important role model pattern recognition structure model computation perform inference learning model shall example shortly given expression joint distribution variable term product conditional distribution mathematical representation underlying directed graph could principle test whether conditional independence property hold repeated application product rule probability practice approach would time important elegant feature graphical model conditional independence property joint distribution read directly graph without perform analytical manipulation general framework stand directed pearl shall motivate concept give general state criterion formal proof found
['three', 'example', 'graphs'] three example graph begin discussion conditional independence property directed graph considering three simple example graph three node together motivate illustrate concept three example shown figure joint distribution corresponding graph easily written general result give none variable investigate whether independent side respect give general factorize product figure figure conditioned value variable empty symbol mean conditional inde property hold general course hold particular distribution virtue numerical value associated various conditional probability follow general structure graph suppose condition variable graph figure easily write conditional distribution given form obtain conditional independence property provide simple graphical interpretation result considering path node node node said path node connected tail arrow presence path node cause node pendent however condition node figure conditioned node block path cause become conditionally independent similarly consider graph shown figure joint distribution corresponding graph general formula give first suppose none variable test independent give figure second three example node graph used motivate conditional framework directed graphical model figure figure node general factorize suppose condition node shown figure theorem together obtain obtain conditional independence property interpret result graphically node said respect path node node path node render dependent observe figure observation block path obtain conditional independence property finally consider third node example shown graph figure shall subtle behaviour previous graph joint distribution written general result give consider case none variable side obtain figure last three example node graph used explore conditional independence property model graph rather different property previous example figure figure value node graph independent variable contrast previous example write result suppose condition figure conditional given general factorize product thus third example opposite behaviour graphically node respect path head arrow node unobserved block path variable independent however path render dependent subtlety associated third example need consider first introduce terminology node node path step path direction arrow shown path become unblocked either node descendant exercise summary node node leaf path unblocked unless case block path contrast node block path unobserved node least descendant path becomes unblocked worth spending moment understand unusual behaviour graph figure consider particular instance graph corresponding problem three binary random variable fuel system shown figure variable state battery either state fuel tank either full fuel empty state electric fuel gauge either full empty figure example node graph used illustrate phenomenon explaining away three node represent state battery state fuel tank reading electric fuel gauge text detail battery either independently fuel tank either full empty prior probability given state fuel tank battery fuel gauge read full given rather unreliable fuel gauge probability determined requirement probability complete cation probabilistic model observe data prior probability fuel tank empty suppose observe fuel gauge discover read empty corresponding middle graph figure theorem evaluate posterior probability fuel tank empty first evaluate denominator theorem given similarly evaluate result thus observing gauge read empty make likely tank indeed empty would intuitively expect next suppose also check state battery state fuel gauge battery shown graph figure posterior probability fuel tank empty given observation fuel gauge battery state given prior probability numerator thus probability tank empty result observation state battery accord intuition battery away observation fuel gauge read empty state fuel tank battery indeed become dependent result observing reading fuel gauge fact would also case instead observing fuel gauge directly state descendant note probability greater prior probability observation fuel gauge read zero still evidence empty fuel tank
['separation'] give general statement property pearl directed graph consider general directed graph nonintersecting set node whose union smaller complete node graph wish ascertain whether particular conditional independence statement given directed acyclic graph consider possible path node node path said blocked node either arrow path meet either node node arrow meet node neither node descendant path blocked said joint distribution variable graph satisfy concept figure graph path blocked node node path blocked node although latter node descendant thus conditional independence statement follow graph graph path blocked node node conditional independence property figure illustration text detail distribution according graph note path also blocked node node neither descendant purpose parameter figure small circle behave node ever marginal distribution associated node consequently parameter node never parent path node always hence blocked consequently play role another example conditional independence provided concept independent identically distributed data consider problem posterior distribution mean distribution directed graph section shown figure joint distribution prior gether conditional distribution practice observe goal infer suppose moment condition consider joint distribution observation note unique path path respect node every path blocked observation independent given figure directed graph problem mean observation graph drawn plate notation figure graphical representation naive model cation conditioned class label component vector assumed independent however integrate observation general longer dent latent variable value another example model data graph figure corresponding polynomial regression stochastic node node respect path node following conditional independence property thus conditioned polynomial predictive distribution independent training data therefore training data determine posterior distribution discard training data posterior distribution make prediction input observation section related graphical structure approach cation naive model conditional independence assumption model structure suppose variable vector wish assign value class scheme represent class dimensional binary vector generative model multinomial prior class label component prior probability class together conditional distribution vector assumption naive model conditioned class distribution input variable dependent graphical representation model shown figure observation block path path node conditionally independent given however marginalize unobserved path longer blocked tell general marginal density factorize respect component simple application naive model context data different source medical diagnosis section given training input together class label naive model training data maximum likelihood assuming data drawn independently model solution model class separately correspondingly data example suppose probability density within class chosen case naive assumption covariance matrix diagonal contour constant density within class ellipsoid marginal density however given superposition diagonal weighting given class prior longer factorize respect component naive assumption helpful dimensionality input space high making density estimation full space chal also useful input vector discrete continuous variable since separately appropriate model distribution binary observation vari conditional independence assumption model clearly strong lead rather poor representation density nevertheless even assumption precisely model still give good cation performance practice decision boundary insensitive detail density figure seen particular directed graph decomposition joint probability distribution product conditional probability graph also express conditional independence statement criterion theorem really expression equivalence property order make clear helpful think directed graph suppose consider particular joint probability distribution variable corresponding node graph allow distribution expressed term factorization graph present possible distribution variable subset distribution directed factorization figure alternatively graph different kind listing conditional independence property criterion graph distribution property present possible distribution second kind theorem tell distribution precisely conditional independence property apply probabilistic model particular graph true instance whether variable discrete continuous combination particular graph scribing whole family probability distribution extreme fully connected graph exhibit conditional dependence property represent possible joint probability distribution given variable contain possible figure view graphical model case directed graph prob ability distribution directed factorization property possible probability distribution alternatively graph distribution according whether respect conditional independency property graph theorem say distribution second kind extreme fully disconnected graph link joint distribution factorize product marginal distribution variable node graph note given graph distribution include additional independence property beyond graph instance fully distribution always graph corresponding variable discussion conditional independence property exploring concept blanket boundary consider joint distribution directed graph node consider conditional distribution particular node variable conditioned variable factorization property express conditional distribution form integral summation case discrete variable observe factor functional dependence taken outside integral therefore cancel numerator denominator factor remain conditional distribution node together conditional distribution node node word parent conditional depend parent node whereas depend child figure blanket node parent child node property conditional distribution conditioned variable graph dependent variable blanket well word variable corresponding parent node node node parent child blanket figure think blanket node minimal node rest graph note include parent child node phenomenon explaining away mean observation child node block path must therefore observe coparent node also
['markov', 'random', 'fields'] random field seen directed graphical model specify factorization joint variable product local conditional distribution also conditional independence property must distribution according graph turn second class graphical model undirected graph specify factorization conditional independence relation random also known network undirected graphical model snell node variable group variable well link pair node link undirected carry arrow case undirected graph convenient begin discussion conditional independence property
['conditional', 'independence', 'properties'] conditional independence property case directed graph possible test whether section conditional independence property hold graphical test involved testing whether path set node blocked blocked however somewhat subtle presence path node might whether possible alternative graphical semantics probability distribution conditional independence determined simple graph separation indeed case undirected graphical model removing figure example undirected graph every path node node pass least node conditional independence property hold probability distribution graph link graph asymmetry parent child node removed subtlety associated node longer arise suppose undirected graph identify three set node consider conditional independence property test whether property probability distribution graph consider possible path connect node node path node path blocked conditional independence property hold however least path blocked property necessarily hold precisely exist least distribution corresponding graph satisfy conditional independence relation example figure note exactly except explaining away phenomenon testing conditional independence undirected graph therefore simpler directed graph alternative view conditional independence test imagine moving node graph together link connect node path node node path conditional independence property must hold blanket undirected graph take particularly simple form node conditionally independent node conditioned node figure
['factorization', 'properties'] factorization property seek factorization rule undirected graph correspond conditional independence test involve joint distribution product function set variable local graph therefore need decide appropriate notion locality case figure undirected graph blanket node node property conditional distribution conditioned variable graph dependent variable blanket consider node connected link variable must conditionally independent given node graph fact direct path node path node hence path blocked conditional independence property expressed variable removed factor joint distribution must therefore appear factor order conditional independence property hold possible distribution belonging graph lead consider graphical concept clique subset node graph link pair node subset word node clique fully connected furthermore maximal clique clique possible include node graph without clique concept undirected graph four variable shown figure graph clique node given well maximal clique given clique missing link therefore factor decomposition joint distribution function variable clique fact consider function maximal clique without loss generality clique must subset maximal clique thus maximal clique arbitrary function clique another factor subset variable would redundant denote clique variable clique figure undirected graph showing clique outlined green maximal clique outlined blue joint distribution written product potential function maximal clique graph quantity sometimes partition function normalization given distribution given correctly considering potential function satisfy ensure assumed discrete variable framework equally applicable continuous variable combination summation appropriate combination summation integration note restrict choice potential function probabilistic interpretation marginal conditional distribution contrast directed graph factor conditional corresponding variable conditioned state parent however special case instance undirected graph starting directed graph potential function indeed interpretation shall shortly consequence generality potential function product general correctly therefore explicit normalization factor given recall directed graph joint distribution automatically consequence normalization conditional distribution factorization presence normalization constant major limitation undirected graph model discrete node state evaluation normalization term state worst case exponential size model partition function parameter learning function parameter govern potential function however evaluation local conditional distribution partition function conditional ratio partition function cancel numerator ratio similarly local marginal work joint distribution normalize explicitly provided small number variable evaluation normalization feasible notion conditional independence based graph separation factorization joint distribution intended correspond conditional independence structure however made formal connection conditional independence factorization undirected graph need restrict attention function strictly positive never zero negative choice given restriction make precise relationship factorization conditional independence return concept graphical model figure consider possible distribution variable corresponding node particular undirected graph distribution consistent conditional independence statement read graph graph separation similarly distribution expressed factorization form respect maximal clique graph theorem state set identical restricted potential function strictly positive convenient express exponential energy function exponential representation distribution joint distribution product potential total energy energy maximal clique contrast factor joint distribution directed graph undirected graph probabilistic interpretation although give greater choosing potential function normalization constraint raise question motivate choice potential function particular application done view potential function local variable preferred global relatively high probability good balance satisfying possibly clique potential turn example illustrate undirected graph
['illustration', 'image', 'de', 'noising'] illustration image illustrate application undirected graph example noise removal binary image although simple example typical sophisticated application noisy image array binary value index run shall suppose image taking unknown image binary value randomly sign small probability example binary image together noise corrupted image sign probability shown figure given noisy image goal recover original image noise level small know strong correlation also know image strongly correlated prior knowledge figure illustration image random show original binary image left corrupted image randomly right bottom show image conditional model left algorithm right produce image agree original image whereas corresponding number random model whose undirected graph shown figure graph type clique variable clique form associated energy function express correlation variable choose simple energy function clique form positive constant desired effect giving lower energy thus encouraging higher probability sign higher energy opposite sign clique comprise pair variable index want energy lower sign opposite sign choose energy given positive constant potential function arbitrary function maximal clique multiply function subset clique figure undirected graphical model random image binary variable state unknown image corresponding value noisy image equivalently corresponding energy example extra term image term effect model towards value particular sign preference complete energy function model take form joint distribution given element value given noisy image implicitly conditional distribution noise free image example model widely studied statistical physic purpose image restoration wish image high probability ideally maximum probability shall simple iterative technique conditional mode simply application gradient ascent idea initialize variable simply setting take node time evaluate total energy possible state keeping node variable whichever state lower energy either leave probability unchanged unchanged increase variable simple local computation exercise repeat update another site suitable stopping criterion node systematic instance repeatedly raster scanning image choosing node random sequence update every site least change variable made algorithm figure example directed graph equivalent undirected graph local maximum probability need however correspond global maximum purpose simple illustration parameter note leaving simply mean prior probability state equal starting noisy image initial convergence leading image shown lower left panel figure note effectively remove link global probable solution given corresponding noisy image exercise later shall discus effective algorithm high probability algorithm typically lead better solution section although still global maximum posterior however certain class model given exist algorithm based graph cut global maximum lower right panel figure show result algorithm problem
['relation', 'directed', 'graphs'] relation directed graph graphical framework probability corresponding directed undirected graph instructive discus relation consider problem taking model directed graph trying convert undirected graph case straightforward simple example figure joint distribution directed graph given product form convert undirected graph representation shown undirected graph maximal clique simply pair neigh node wish write joint distribution form figure example simple directed graph moral graph easily done absorbed marginal node potential function note case partition function consider generalize construction convert distribution factorization directed graph factorization undirected graph clique potential undirected graph given conditional distribution directed graph order valid must ensure variable conditional distribution member least clique undirected graph node directed graph parent simply directed link undirected link however node directed graph parent node path discussion conditional independence consider simple directed graph node shown figure joint distribution directed graph take form factor four variable must belong single clique conditional distribution absorbed clique potential ensure extra link pair parent node anachronistically process marrying parent become known moralization resulting undirected graph dropping arrow moral graph important observe moral graph example fully connected exhibit conditional independence property contrast original directed graph thus general convert directed graph undirected graph additional undirected link pair parent node graph drop arrow original link give moral graph initialize clique potential moral graph take conditional distribution factor original directed graph multiply clique potential always exist least maximal clique variable factor result moralization step note case partition function given process converting directed graph undirected graph play important role exact inference technique junction tree algorithm section converting undirected directed representation much common general present problem normalization constraint going directed undirected representation discard conditional independence property graph course could always trivially convert distribution directed graph undirected graph simply fully connected undirected graph would however discard conditional independence property would vacuous process moralization add extra link maximum number independence property seen procedure conditional independence property different directed undirected graph turn type graph express different conditional independence property worth exploring issue detail return view directed undirected graph possible section distribution given variable could reduced subset respect conditional independency graph graph said dependency distribution every conditional independence statement distribution graph thus completely disconnected graph link trivial distribution alternatively consider distribution graph appropriate conditional independence property every conditional statement graph distribution graph said independence distribution clearly fully connected graph trivial distribution case every conditional independence property distribution graph vice graph said perfect figure diagram distribution given variable together distribution perfect directed graph perfect undirected graph figure directed graph whose conditional independence property expressed undirected graph three variable distribution perfect therefore consider distribution distribution directed graph perfect distinct distribution distribution undirected graph perfect addition distribution neither directed undirected graph offer perfect diagram figure figure show example directed graph perfect distribution satisfying conditional independence property corresponding undirected graph three vari perfect conversely consider undirected graph four variable shown graph exhibit property directed graph four variable conditional independence property graphical framework extended consistent graph include directed undirected link chain graph contain directed undirected graph considered special case although graph represent class distribution either directed undirected alone remain distribution even chain graph provide perfect chain graph book figure undirected graph whose conditional independence property expressed term directed graph variable
['inference', 'graphical', 'models'] inference graphical model turn problem inference graphical model node graph value wish compute posterior distribution subset node shall exploit graphical structure algorithm inference figure graphical representation theorem text detail make structure algorithm transparent shall many algorithm expressed term propagation local message around graph section shall focus primarily technique exact inference chapter shall consider number approximate inference algorithm start consider graphical interpretation theorem suppose decompose joint distribution variable product factor form directed graph shown figure suppose observe value shaded node figure view marginal distribution prior latent variable goal infer corresponding posterior distribution product rule probability evaluate used theorem calculate thus joint distribution expressed term graphical perspective joint distribution graph shown figure direction arrow reversed example inference problem graphical model
['inference', 'chain'] inference chain consider complex problem chain node form shown figure example foundation discussion exact inference general graph later section shall consider undirected graph figure already seen directed chain equivalent undirected chain directed graph node parent require addition extra link directed undirected version graph express exactly conditional inde statement joint distribution graph take form shall consider case node represent discrete vari state case potential function table joint distribution parameter consider inference problem marginal distribution node part along chain note moment node marginal joint distribution variable except naive implementation would evaluate joint distribution perform summation explicitly joint distribution number possible value variable state value evaluation storage joint distribution well obtain involve storage computation scale exponentially length chain however obtain much algorithm independence property graphical model substitute factor expression joint distribution rearrange order summation multiplication allow marginal much consider instance summation potential perform summation give function perform summation involve function together potential place similarly summation potential separately give function summation effectively remove variable distribution removal node graph group potential summation together express desired marginal form reader study carefully underlying idea form basis later discussion general algorithm concept multiplication distributive side three arithmetic operation whereas right hand side operation work computational cost marginal expression perform summation state function variable instance summation function table number table value cost resulting vector number matrix number summation multiplication kind total cost marginal linear length chain contrast exponential cost naive approach therefore able exploit many conditional independence property simple graph order obtain graph fully connected would conditional independence property would forced work directly full joint distribution give powerful interpretation calculation term passing local message around graph expression marginal product factor time normalization constant shall interpret message forward along chain node node similarly message backwards figure marginal distribution node along chain multiplying message message passing sage end chain ward node along chain node node note message value choice product sage pointwise multiplication element message give another value message therefore evaluate apply repeatedly reach desired node note carefully structure message passing equation outgoing message multiplying incoming message local potential node variable outgoing variable node variable similarly message starting node recursive message passing figure normalization easily side state operation computation graph form shown figure chain corresponding message passing equation represent example chapman equation process suppose wish evaluate every node chain simply procedure separately node computational cost however approach would wasteful computation instance need prop agate message node back node similarly evaluate need propagate message node back node involve much computation message identical case suppose instead launch message starting node propagate corresponding message back node suppose similarly launch message starting node propagate message forward node provided store intermediate message along node evaluate marginal computational cost twice marginal single node rather time much observe message direction across link graph note also normalization constant need convenient node node graph corresponding variable simply value summation note effect variable value expressed multiplying joint distribution copy additional function take value value otherwise function absorbed potential contain summation contain term suppose wish calculate joint distribution node chain similar evaluation marginal single node except variable summed moment thought show joint distribution written exercise form thus obtain joint distribution set variable potential directly message passing obtain useful result practice wish parametric form clique potential equivalently conditional distribution directed graph order learn parameter potential variable employ algorithm chapter turn local joint distribution clique conditioned data precisely step shall consider example detail chapter
['trees'] tree seen exact inference graph chain node time linear number node algorithm figure example tree structured graph showing undirected tree directed tree directed term message along chain generally inference local message passing class graph tree particular shall shortly generalize message passing formalism derived chain give algorithm framework exact inference graph case undirected graph tree graph path pair node graph therefore loop case directed graph tree single node root parent node parent convert directed tree undirected graph moralization step link node parent consequence corresponding graph undirected tree example undirected directed tree shown figure note distribution directed tree easily converted undirected tree vice exercise node directed graph parent still path direction arrow node graph figure graph node property parent furthermore corresponding undirected graph loop
['factor', 'graphs'] factor graph algorithm derive next section applicable undirected directed tree cast particularly simple general form introduce graphical construction factor graph directed undirected graph allow global function several vari expressed product factor subset variable factor graph make decomposition explicit additional node tor addition node variable also allow explicit detail factorization shall write joint distribution variable form product factor subset variable convenience shall denote figure example factor graph factorization individual variable however discussion comprise group variable vector matrix factor function corresponding variable directed graph whose factorization represent special case factor local conditional distribution similarly undirected graph given special case factor function maximal clique factor empty variable factor graph node usual circle every variable distribution case directed undirected graph also additional node small square factor joint finally undirected link factor node variable node factor consider example distribution expressed term factorization expressed factor graph shown figure note factor variable undirected graph product factor would simply together clique potential similarly could combined single potential factor graph however keep factor explicit able convey detailed information underlying factorization figure undirected graph single clique potential factor graph factor distribution undirected graph different factor graph distribution whose factor satisfy figure directed graph factorization factor graph distribution directed graph whose factor different factor graph distribution factor factor graph said bipartite consist distinct kind node link node opposite type general factor graph therefore always drawn row node variable node factor node bottom link row shown example figure situation however way laying graph intuitive example factor graph derived directed undirected graph shall given distribution expressed term undirected graph readily convert factor graph create variable node corresponding node original undirected graph create factor node corresponding maximal clique factor equal clique potential note several different factor graph correspond undirected graph concept figure similarly convert directed graph factor graph simply create variable node factor graph corresponding node directed graph create factor node corresponding conditional distribution appropriate link multiple factor graph correspond directed graph conversion directed graph factor graph figure already noted importance graph inference take directed undirected tree convert factor graph result tree word factor graph loop path node case directed conversion undirected graph result loop moralization step whereas conversion factor graph result tree figure fact local cycle directed graph link parent node removed conversion factor graph appropriate factor function shown figure seen multiple different factor graph represent undirected graph factor graph figure directed result converting undirected graph showing creation loop result converting factor graph tree structure precise form factorization figure show example fully connected undirected graph along different factor graph joint given general form whereas given factorization factorization correspond conditional independence property
['sum', 'product', 'algorithm'] algorithm shall make factor graph framework derive powerful class exact inference algorithm applicable graph shall focus problem local node subset node lead algorithm later shall modify technique allow probable state found giving rise algorithm also shall suppose variable model discrete sum framework however equally applicable model case integration shall consider example detail discus linear dynamical system section figure fragment graph cycle conversion fragment factor graph tree figure fully connected undirected graph factor graph undirected graph algorithm exact inference directed graph without loop known belief propagation pearl special case algorithm shall consider algorithm simpler derive apply well general shall assume original graph undirected tree directed tree corresponding factor graph tree structure convert original graph factor graph deal directed undirected model framework goal exploit structure graph achieve thing obtain exact inference algorithm situation several allow computation begin considering problem marginal variable node moment shall suppose variable hidden later shall modify algorithm incorporate evidence corresponding variable marginal ming joint distribution variable except variable variable idea substitute factor graph expression interchange summation product order obtain algorithm consider fragment graph shown figure tree structure graph partition factor joint distribution group group associated factor node variable node joint distribution written product form factor node variable connected variable node factor node figure fragment factor graph evaluation marginal product factor group associated factor substituting sum product tain function message factor node variable node marginal given product incoming message node order evaluate message turn figure note factor factor particular write convenience variable associated factor addition factorization figure note variable variable factor also notation substituting obtain figure illustration factorization factor node variable node factor node node removed following message variable node factor node therefore distinct kind message factor node variable node variable node factor node case message along link always function variable associated variable node link result say evaluate message sent factor node vari able node along link take product incoming message along link coming factor node multiply factor associated node marginalize variable associated incoming message figure important note factor node send message variable node received incoming message variable node finally derive expression message variable node factor node making factorization term associated node given product term associated factor node linked node excluding node product taken node except node note factor original graph precisely kind substituting figure illustration evaluation message sent variable node adjacent factor node obtain used message factor node variable node thus evaluate message sent variable node adjacent factor node along link simply take product incoming message along link note variable node computation simply pass message also note variable node send message factor node received incoming message factor node recall goal calculate marginal variable node marginal given product incoming message along link node message term message order start recursion view node root tree begin leaf node leaf node variable node message along link given figure similarly leaf node factor node message sent take form figure algorithm begin message sent leaf node pend whether leaf node variable node factor node figure point worth summarize particular version product algorithm marginal start variable node root factor graph message leaf graph message passing step applied message along every link root node received message node send message towards root received message root node received message marginal shall illustrate process shortly node always receive enough message able send message simple inductive argument clearly graph variable root node connected directly several factor leaf node algorithm trivially sending message form directly leaf root imagine building general graph node time suppose particular graph valid algorithm variable factor node added connected single link overall graph must remain tree node leaf node therefore message node linked turn therefore receive message order send message towards root valid algorithm thereby proof suppose wish every variable node graph could done simply running algorithm afresh node however would wasteful many computation would repeated obtain much procedure multiple message passing algorithm obtain general algorithm arbitrarily pick variable factor node designate root propagate message leaf root point root node received message therefore send message turn received message send message along link going away root message outwards root leaf message direction across every link graph every node received message simple inductive argument used verify validity message passing protocol every variable exercise node received message readily calculate marginal distribution every variable graph number message given twice number link graph twice computation involved single marginal comparison algorithm separately node amount computation would grow quadratically size graph note algorithm fact independent node root figure algorithm purely term message sent factor node factor node example outgoing message shown blue arrow taking product coming message shown green arrow factor variable indeed notion node special status convenient explain message passing protocol next suppose wish marginal distribution associated set variable belonging factor similar argument used easy marginal associated factor given exercise product message factor node local factor node complete analogy variable node factor function wish learn value parameter algorithm precisely quantity need calculate step shall detail discus hidden model chapter message sent variable node factor node seen simply product incoming message link wish view algorithm slightly different form message variable node factor node simply considering message sent factor node easily seen considering example figure rather issue normalization factor graph derived directed graph joint distribution already correctly algorithm similarly correctly however undirected graph general unknown normalization simple chain example figure easily handled working unnormal version joint distribution algorithm corresponding easily normalization done single variable rather entire variable would normalize directly point helpful consider simple example illustrate operation algorithm figure show simple node factor figure simple factor graph used illustrate algorithm graph whose joint distribution given order apply algorithm graph designate node root case leaf node starting leaf node following sequence message direction message figure sage propagation complete propagate message root node leaf node given figure flow message algorithm applied example graph figure leaf node towards root node root node towards leaf node message direction across link evaluate simple check verify marginal given correct expression substituting message result assumed variable graph hidden practical application subset variable wish posterior distribution conditioned observation node easily handled within algorithm suppose partition hidden variable variable value simply multiply joint distribution otherwise product hence version algorithm calculate posterior normalization whose value found local computation summation variable collapse single term assumed throughout section dealing discrete vari however nothing discrete variable either graphical framework probabilistic construction algorithm table example joint distribution binary variable maximum joint distribution variable value maximum continuous variable summation simply integration shall give example algorithm applied graph variable consider linear dynamical system section
['max', 'sum', 'algorithm'] algorithm algorithm take joint distribution expressed factor graph component variable common task setting variable prob ability value probability closely related algorithm application dynamic context graphical model simple approach latent variable value high probability would algorithm obtain variable marginal turn value marginal however would give value individually probable practice typically wish value jointly probability word vector joint distribution corresponding value joint probability given general value easily show simple example consider joint distribution binary variable given table joint distribution setting corresponding value however marginal value given similarly marginal given value joint distribution fact cult construct example individually probable value probability zero joint distribution exercise therefore seek algorithm value joint distribution allow obtain value joint distribution maximum address second problem shall simply write operator term component total number variable substitute expansion term product factor algorithm made distributive multiplication make analogous operator hold always case factor graphical model exchange product maximization consider simple example chain node evaluation probability maximum written calculation product operator result much computation easily inter term message node backwards along chain node readily generalize result arbitrary factor graph substituting expression factor graph expansion maximization product structure calculation identical algorithm simply translate result present context particular suppose designate particular variable node root graph start message inwards leaf tree towards root node sending message towards root received incoming message maximization product message root node give maximum value could algorithm identical algorithm except summation maximization note stage message sent leaf root direction practice product many small probability lead numerical problem convenient work logarithm joint logarithm monotonic function hence operator logarithm function distributive property thus taking logarithm simply effect product algorithm sum obtain algorithm result derived algorithm readily write algorithm term message passing simply product sum logarithm give initial message sent leaf node analogy given root node maximum probability analogy seen maximum joint distribution prop message leaf arbitrarily chosen root node result irrespective node chosen root turn second problem variable joint maximum value sent message leaf root process also give value probable value root node variable point might simply continue message passing send message root back leaf apply variable node however rather possible tiple give rise maximum value case strategy fail possible individual variable value product message node belong different giving overall longer maximum problem resolved rather different kind message passing root node leaf work return simple chain example variable state figure lattice trellis diagram show explicitly possible state diagram variable chain model illustration show direction message passing algorithm every state variable corresponding column gram function unique state previous variable black line path lattice correspond give global maximum joint probability distribution either found tracing back along black line opposite direction arrow corresponding graph shown figure suppose take node root node phase propagate message leaf node root node follow particular graph initial message sent leaf node simply probable value given need determine state previous variable correspond done keeping track value variable gave rise maximum state variable word quantity given understand better happening helpful represent chain vari term lattice trellis diagram shown figure note probabilistic graphical model node represent individual state variable variable column state state given variable unique state previous variable probability tie broken either systematically random corresponding function given line node know probable value node simply follow link back probable state node back initial node message back chain known note could several value give maximum value provided chose value assured globally consistent figure path shall suppose global maximum joint probability distribution represent possible value starting either state tracing back along black line obtain valid global maximum note forward message passing backward applied node separately could state path path giving overall global maximizer necessary instead keep track state forward function consistent solution extension general factor graph clear message sent factor node variable node maximization variable node node perform maximization keep record value variable gave rise maximum step found value sign consistent state algorithm give exact variable provided factor graph tree important application technique probable sequence hidden state hidden model case known algorithm section algorithm inclusion evidence form variable straightforward variable value maximization hidden vari shown formally identity function variable factor function algorithm interesting compare conditional mode algorithm page step simpler cause message node next comprise single value state node conditional distribution algorithm complex message function node variable hence comprise value state unlike however global maximum even graph
['exact', 'inference', 'general', 'graphs'] exact inference general graph algorithm provide exact solution inference problem graph many practical application however deal graph loop message passing framework generalized arbitrary graph giving exact inference procedure known junction tree algorithm jordan give brief outline step involved intended convey detailed understanding algorithm rather give various stage involved starting point directed graph converted undirected graph whereas starting undirected graph step next graph cycle four node extra link eliminate cycle instance graph figure cycle link could added alternatively note joint resulting graph still product potential function considered function expanded set variable next graph used construct undirected graph join tree whose node correspond maximal clique graph whose link connect pair clique vari common selection pair clique connect important done give maximal tree possible tree link clique chosen weight tree weight link number node clique weight tree weight link tree condensed clique subset another clique absorbed clique give junction tree consequence triangulation step resulting tree running intersection property mean variable clique must also every clique path inference variable consistent across graph finally message passing algorithm essentially equivalent algorithm applied junction tree order although junction tree algorithm sound complicated heart simple idea used already factorization property distribution allow sum product partial summation formed thereby work directly joint distribution role junction tree provide precise organize computation worth purely graphical operation junction tree exact arbitrary graph sense given graph general exist approach unfortunately algorithm must work joint distribution within node clique graph cost algorithm determined number variable clique grow exponentially number case discrete variable important concept graph term number variable clique fact size clique ensure tree general multiple different junction tree given starting graph junction tree clique variable original graph high junction tree algorithm becomes impractical
['loopy', 'belief', 'propagation'] loopy belief propagation many problem practical interest feasible exact need exploit effective approximation method important class approximation broadly variational method detail chapter deterministic approach wide range sampling method also monte method based stochastic numerical sampling distribution length chapter consider simple approach approximate inference graph loop build directly previous discussion exact inference tree idea simply apply algorithm even though guar yield good result approach known loopy belief possible message passing rule algorithm purely local however graph cycle information many time around graph model algorithm converge whereas order apply approach need message passing schedule assume message time given link given direction message sent node previous message sent direction across link function recent message received node previous step algorithm seen message sent across link node message received node across link loop graph raise problem initiate message passing algorithm resolve suppose initial message given unit function across every link direction every node position send message many possible way organize message passing schedule example schedule simultaneously pass message across every link direction time step whereas schedule message time serial schedule following variable factor node message pending link node node received message link since last time send message thus node message link pending message link pending message need message would simply duplicate previous message link graph tree structure schedule pending message eventually terminate message direction across every link point pending message product received exercise message every variable give exact marginal graph loop however algorithm never terminate might always pending message although practice generally found converge within reasonable time application algorithm stopped convergence approximate local product recently received incoming message variable node factor node every link application loopy belief propagation algorithm give poor whereas application proven effective particular algorithm certain kind code equivalent loopy belief propagation neal
['learning', 'graph', 'structure'] learning graph structure discussion inference graphical model assumed structure graph known however also interest beyond inference problem learning graph structure data koller space possible well measure used score structure viewpoint would ideally like compute posterior graph structure make prediction respect distribution prior graph indexed posterior distribution given data model evidence score model however evaluation evidence latent variable present computational problem many model exploring space structure also problematic number different graph structure exponentially number node often necessary resort heuristic good candidate exercise variable order show representation joint distribution directed graph correctly provided conditional distribution show property directed cycle directed graph statement ordered node node link going node table joint distribution three binary variable consider three binary variable joint distribution given table show direct evaluation distribution property marginally dependent become independent conditioned evaluate distribution corresponding joint distribution given table hence show direct evaluation draw corresponding directed graph draw directed probabilistic graphical model corresponding relevance vector machine model shown figure seen number parameter specify conditional distribution could reduced making logistic sigmoid alternative representation pearl given parameter represent probability additional parameter satisfying conditional distribution known show soft probabilistic form logical function function give whenever least discus interpretation recursion relation show mean joint distribution graph shown figure given respectively show criterion show conditional distribution node directed graph conditioned node blanket independent variable graph figure example graphical model used explore independence property path descendant namely node consider directed graph shown figure none variable show suppose observe variable show general consider example fuel system shown figure suppose instead observing state fuel gauge directly gauge seen driver report reading gauge report either gauge show full show empty driver unreliable expressed following probability suppose driver tell fuel gauge show empty word observe evaluate probability tank empty given observation similarly evaluate corresponding probability given also observation battery note second probability lower discus intuition behind result relate result figure show distinct undirected graph distinct random variable draw possibility case consider conditional mode minimize energy function given write expression difference value energy associated state particular variable variable show quantity local graph consider particular case energy function given show probable latent variable given show joint distribution node graph shown figure given expression form consider inference problem graph shown figure node show message passing algorithm section used solve discus message consider graph form shown figure node node show show message passing algorithm section applied result independent value show distribution directed tree trivially written equivalent distribution corresponding undirected tree also show distribution expressed undirected tree suitable clique potential written directed tree calculate number distinct directed tree given undirected tree apply algorithm derived section node model section show result special case consider message passing protocol algorithm factor graph message leaf arbitrarily chosen root node root node leaf proof induction show message order every step node must send message received incoming message necessary construct outgoing message show marginal distribution set variable associated factor factor graph found running message passing algorithm consider factor graph given subset variable node form connected variable node subset connected least variable node single factor node show algorithm used compute marginal distribution subset section marginal distribution variable node factor graph given product message node factor node form show marginal also written product incoming message along link outgoing message along link show marginal distribution variable factor factor graph running message passing written product message factor node along link time local factor form veri algorithm graph figure node root node give correct marginal show correct also similarly show result running algorithm graph give correct joint distribution consider factor graph discrete variable suppose wish evaluate joint distribution associated variable belong common factor procedure product algorithm evaluate joint distribution variable successively value consider discrete variable three possible state example construct joint distribution variable property value marginal along value marginal together probability zero joint distribution concept pending message algorithm factor graph section show graph cycle always least pending message irrespective long algorithm run show algorithm factor graph tree structure loop number message sent pending message joint distribution latent variable correspond distribution variable alone relatively complex marginal distribution variable term tractable joint distribution expanded space latent variable introduction latent variable thereby complicated distribution formed simpler component chapter shall mixture distribution mixture section term discrete latent variable continuous latent variable form subject chapter well providing framework building complex probability mixture model also used cluster data therefore begin discussion mixture distribution considering problem cluster data point approach technique algorithm introduce latent variable section view mixture distribution discrete latent variable assignment data point component mixture section eral technique maximum likelihood estimator latent variable model algorithm mixture distribution motivate algorithm fairly informal give careful treatment based latent variable viewpoint shall section algorithm particular limit applied mixture finally discus generality section mixture model widely used data mining pattern recognition machine learning statistical analysis many application parameter determined maximum likelihood typically algorithm however shall cant limitation maximum likelihood chapter shall show elegant treatment given framework variational inference little additional computation resolve principal likelihood also number component mixture automatically data
['k', 'means', 'clustering'] clustering begin considering problem group cluster data point multidimensional space suppose data observation random variable goal partition data number cluster shall suppose moment value given intuitively might think cluster group data point whose interpoint distance small distance point outside cluster formalize notion vector prototype associated cluster shall shortly think cluster goal assignment data point cluster well vector square distance data point vector minimum convenient point notation describe assignment data point cluster data point introduce corresponding binary indicator variable cluster data point assigned data point assigned cluster known scheme objective function sometimes distortion measure given square distance data point assigned vector goal value minimize iterative procedure iteration successive step corresponding successive optimization respect first choose initial value phase minimize respect keeping second phase minimize respect keeping optimization repeated convergence shall stage correspond respectively expectation maximization step algorithm emphasize shall section term step step context algorithm consider determination linear optimization easily give closed form solution term different independent optimize separately choosing whichever value give minimum value word simply assign data point cluster formally expressed otherwise consider optimization objective function quadratic function setting derivative respect zero giving easily solve give denominator expression equal number point assigned cluster result simple interpretation namely equal mean data point assigned cluster reason procedure known algorithm phase data point cluster mean repeated turn change assignment maximum number iteration phase value objective function convergence algorithm assured exercise ever converge local rather global minimum convergence property algorithm studied algorithm faithful data appendix purpose example made linear data known variable zero mean unit standard deviation example chosen figure illustration algorithm faithful data green point denote data space initial choice shown blue cross respectively initial step data point assigned either cluster blue cluster according cluster equivalent point according side perpendicular bisector cluster shown magenta line subsequent step cluster mean point assigned corresponding cluster show successive step convergence algorithm figure plot cost function given step blue point step point mean algorithm example shown figure third step cycle change either prototype vector case assignment data point nearest cluster equivalent cation data point according side perpendicular bisector cluster plot cost function given faithful example shown figure note deliberately chosen poor initial value cluster algorithm take several step convergence practice better procedure would choose cluster equal random subset data point also worth algorithm often used initialize parameter mixture model algorithm section direct implementation algorithm relatively slow step necessary compute every prototype vector every data point various scheme speeding algorithm based data structure tree nearby point approach make triangle inequality distance thereby unnecessary calculation considered batch version whole data used together update prototype vector also derive stochastic algorithm procedure section problem root regression function given derivative respect lead sequential update exercise data point turn update nearest prototype learning rate parameter typically made decrease mono tonically data point considered algorithm based squared distance measure dissimilarity data point prototype vector limit type data variable considered would inappropriate case variable represent categorical label instance also make determination cluster mean outlier section generalize algorithm general dissimilarity measure vector following distortion measure give algorithm step given cluster prototype data point cluster dissimilarity corresponding prototype computational cost case standard algorithm general choice measure step potentially complex common restrict cluster prototype equal data vector cluster algorithm choice dissimilarity measure long readily thus step cluster discrete search point assigned cluster evaluation notable feature algorithm iteration every data point assigned uniquely cluster whereas data point much closer particular data point roughly midway cluster latter case clear hard assignment nearest cluster appropriate shall next section probabilistic approach obtain soft assignment data point cluster level uncertainty appropriate assignment probabilistic formulation numerous bene
['image', 'segmentation', 'compression'] image segmentation compression illustration application algorithm consider related problem image segmentation image compression goal segmentation partition image region reasonably homogeneous visual appearance object part object ponce image point dimensional space intensity blue green channel segmentation algorithm simply treat image separate data point note strictly space channel intensity bounded interval nevertheless apply algorithm without illustrate result running convergence particular value image vector intensity triplet given assigned result various value shown figure given value algorithm image palette colour particularly sophisticated approach image segmentation least take account spatial proximity different image segmentation problem general extremely cult original image figure example application clustering algorithm image segmentation show initial image together segmentation various value also vector quantization data compression smaller value give higher compression expense image quality remains subject active research simply illustrate behaviour algorithm also result clustering algorithm perform data sion important distinguish lossless data compression goal able reconstruct original data exactly compressed representation data compression accept error reconstruction return higher level compression lossless case apply algorithm problem data compression data point store identity cluster assigned also store value typically data provided choose data point nearest data point similarly compressed nearest label instead original data vector framework often vector quantization vector vector image segmentation problem also illustration clustering data compression suppose original image value bit precision transmit whole image directly would cost bit suppose image data instead original intensity vector transmit identity nearest vector vector bit must also transmit code book vector bit total number bit transmit image rounding nearest integer original image shown figure bit transmit directly comparison compressed image require bit bit bit respectively transmit represent compression ratio original image respectively degree compression image quality note example illustrate algorithm aiming produce good image compressor would fruitful consider small block adjacent instance thereby exploit correlation exist natural image nearby
['mixtures', 'gaussians'] mixture section mixture model simple linear super position component providing class density single turn formulation mixture term discrete latent variable provide insight important distribution also serve motivate algorithm recall mixture distribution written linear superposition form introduce binary random variable particular element equal element equal value therefore satisfy possible state vector according element nonzero shall joint distribution term marginal conditional distribution corresponding graphical model figure marginal distribution term figure graphical representation mixture model joint distribution expressed form parameter must satisfy together order valid probability us representation also write distribution form similarly conditional distribution given particular value also written form joint distribution given marginal distribution joint distribution possible state give exercise made thus marginal distribution mixture form several observation marginal distribution form every data point corresponding latent variable therefore found equivalent formulation mixture explicit latent variable might seem much however able work joint distribution instead marginal distribution lead cant notably introduction algorithm another quantity play important role conditional probability given shall denote whose value found theorem shall view prior probability quantity corresponding posterior probability shall later also responsibility component take explain observation technique ancestral sampling generate random sample section distributed according mixture model generate value denote marginal distribution generate value conditional distribution technique sampling standard distribution chapter depict sample joint distribution plotting point corresponding value according value word according component responsible generating shown figure similarly sample marginal distribution taking sample joint distribution value figure plotting value without label also synthetic data illustrate responsibility every data point posterior probability component mixture distribution data particular represent value responsibility associated data point plotting corresponding point proportion blue green given respectively shown figure instance data point whereas equal proportion blue green appear cyan figure data point true identity component
['maximum', 'likelihood'] maximum likelihood suppose data observation wish model data mixture represent data figure example point drawn mixture shown figure sample joint distribution three state corresponding three component mixture green blue corresponding sample marginal distribution simply value plotting value data said complete whereas incomplete sample colour represent value responsibility associated data point plotting corresponding point proportion blue green given respectively matrix given similarly corresponding latent variable matrix row assume data point drawn independently distribution express mixture model data graphical representation shown figure likelihood function given maximize function worth cant problem associated maximum likelihood framework applied mixture model presence singularity consider mixture whose component covariance matrix given unit matrix although conclusion hold general covariance matrix suppose component mixture model component mean exactly equal data figure graphical representation mixture model data point corresponding latent point figure illustration singularity likelihood function arise mixture case single gaus shown figure singularity arise point value data point contribute term likelihood function form consider limit term go likelihood function also thus maximization likelihood function well problem singularity always present occur whenever component collapse onto data point recall problem arise case single distribution understand difference note single collapse onto data point contribute multiplicative factor likelihood function data point factor zero exponentially fast giving overall likelihood go zero rather however least component mixture component variance therefore assign probability data point component shrink onto data point thereby contribute ever increasing additive value likelihood figure singularity provide another example severe occur maximum likelihood approach shall occur adopt approach moment section however simply note maximum likelihood mixture model must take step avoid pathological solution instead seek local maximum likelihood function well hope avoid singularity suitable heuristic instance component mean randomly chosen value also covariance large value optimization issue maximum likelihood solution fact given maximum likelihood solution mixture total equivalent solution corresponding way set parameter component word given nondegenerate point space parameter value additional point give rise exactly distribution problem known ability berger important issue wish interpret parameter value discovered model ability also arise discus model continuous latent variable chapter however purpose good density model irrelevant equivalent solution good likelihood function mixture model turn complex problem case single presence summation inside logarithm logarithm function longer act directly derivative likelihood zero longer obtain closed form solution shall shortly approach apply optimization technique fletcher wright bishop although technique feasible indeed play important role discus mixture density network chapter consider alternative approach known algorithm broad applicability foundation discussion variational inference technique chapter
['em', 'gaussian', 'mixtures'] mixture elegant powerful method maximum likelihood solution model latent variable algorithm algorithm dempster later shall give general treatment shall also show generalized obtain variational inference framework initially shall motivate section algorithm giving relatively informal treatment context mixture model emphasize however broad applicability indeed context variety different model book begin writing condition must maximum likelihood function setting derivative respect mean component zero obtain made form distribution note posterior probability responsibility given appear naturally side multiplying assume nonsingular obtain interpret effective number point assigned cluster note carefully form solution mean component taking weighted mean point data weighting factor data point given posterior probability component responsible generating derivative respect zero follow similar line reasoning making result maximum likelihood solution covariance matrix single obtain section form corresponding result single data data point weighted corresponding probability denominator given effective number point associated corresponding component finally maximize respect must take account constraint multiplier appendix following quantity give appearance responsibility multiply side making constraint eliminate obtain component given average component take explaining data point worth result solution parameter mixture model responsibility depend parameter complex however result suggest simple iterative scheme solution maximum likelihood problem shall turn instance algorithm particular case mixture model choose initial value mean covariance alternate following update shall call step figure illustration algorithm faithful used illustration algorithm figure text detail step reason become apparent shortly expectation step step current value parameter evaluate posterior probability responsibility given probability maximization step step mean covariance result note evaluate mean value covariance keeping corresponding result single distribution shall show update parameter resulting step step increase likelihood function practice algorithm change section likelihood function alternatively parameter fall threshold illustrate algorithm mixture applied faithful data figure mixture used value algorithm figure precision matrix proportional unit matrix plot show data point green together initial mixture model contour component shown blue circle plot show result initial step data point proportion blue equal posterior probability blue ponent corresponding proportion given posterior probability component thus point cant probability belonging either cluster appear purple situation step shown plot mean blue mean data weighted probability data point belonging blue cluster word mass blue similarly covariance blue equal covariance blue analogous result hold component plot show result complete cycle respectively plot algorithm close convergence note algorithm take many iteration reach approximate convergence algorithm cycle computation therefore common order suitable mixture model subsequently covariance matrix conveniently sample covariance cluster found algorithm fraction data point assigned respective cluster approach like technique must employed avoid singularity likelihood function component collapse onto particular data point generally multiple local maximum likelihood function maximum algorithm mixture play important role summarize mixture given mixture model goal maximize likelihood function respect parameter mean covariance component initialize mean covariance evaluate initial value likelihood step evaluate responsibility current parameter value step parameter current responsibility evaluate likelihood check convergence either parameter likelihood convergence criterion return step
['alternative', 'view', 'em'] alternative view section present complementary view algorithm role latent variable discus approach abstract setting illustration consider case mixture goal algorithm maximum likelihood solution latent variable denote data similarly denote latent variable corresponding model parameter likelihood function given note discussion apply equally well continuous latent variable simply integral observation summation latent variable inside logarithm even joint distribution exponential family marginal distribution typically result presence logarithm acting directly joint distribution resulting complicated expression maximum likelihood solution suppose observation told corresponding value latent variable shall call complete data shall refer actual data incomplete figure likelihood function complete data simply take form shall suppose maximization likelihood function straightforward practice however given complete data incomplete data state knowledge value latent variable given posterior distribution likelihood consider instead value posterior distribution latent variable shall step algorithm subsequent step maximize expectation current estimate parameter pair successive step give rise estimate algorithm choosing starting value parameter expectation seem somewhat arbitrary however shall motivation choice give treatment section step current parameter value posterior distribution latent variable given posterior distribution expectation likelihood general parameter value expectation given step determine parameter estimate function note logarithm act directly joint distribution corresponding maximization position tractable general algorithm property shall show later cycle increase likelihood unless already local maximum section general algorithm given joint distribution variable latent vari parameter goal maximize likelihood respect choose initial setting parameter step evaluate step evaluate given check convergence either likelihood parameter value convergence criterion return step algorithm also used maximum posterior solution model prior parameter case exercise step remains maximum likelihood case whereas step quantity given suitable choice prior remove singularity kind figure considered algorithm maximize likelihood function discrete latent variable however also applied unobserved variable correspond missing value data distribution value taking joint distribution variable missing one used maximize corresponding likelihood function shall show example application technique context principal component analysis figure valid procedure data value missing random meaning mechanism causing value missing depend unobserved value many situation case instance sensor return value whenever quantity measuring threshold
['gaussian', 'mixtures', 'revisited'] mixture consider application latent variable view case mixture model recall goal maximize likelihood function data cult case single distribution presence summation inside logarithm pose addition data also given value corresponding discrete variable recall figure show data label showing component data point figure show corresponding incomplete data graphical model complete data shown figure figure show graph figure except suppose discrete variable well data variable consider problem likelihood complete data likelihood function take form component taking logarithm obtain comparison likelihood function incomplete data show summation logarithm act directly distribution member exponential family surprisingly lead much simpler solution maximum likelihood problem show consider maximization respect mean covariance element equal except single element value likelihood function simply independent mixture component thus maximization respect mean covariance exactly single except subset data point assigned component maximization respect note coupled different value virtue summation constraint enforced multiplier lead result equal fraction data point assigned corresponding component thus likelihood function trivially closed form practice however value latent variable consider expectation respect posterior distribution latent variable likelihood together theorem posterior distribution take form hence posterior distribution independent easily veri inspection directed graph figure exercise making criterion value indicator section variable posterior distribution given responsibility component data point value likelihood function therefore given proceed first choose initial value param evaluate responsibility step keep responsibility maximize respect step lead closed form solution given precisely algorithm exercise mixture derived shall gain insight role likelihood function give proof convergence algorithm section
['relation', 'k', 'means'] relation comparison algorithm algorithm mixture show close similarity whereas algorithm hard assignment data point cluster data point associated uniquely cluster algorithm make soft assignment based posterior probability fact derive algorithm particular limit mixture consider mixture model covariance matrix mixture component given variance parameter component identity matrix consider algorithm mixture form treat constant instead parameter posterior probability responsibility particular data point given consider limit denominator term zero slowly hence responsibility data point zero except term unity note hold independently value long none zero thus limit obtain hard assignment data point cluster algorithm data point thereby assigned cluster mean equation given result note formula simply reset value equal fraction data point assigned cluster although parameter longer play active role algorithm finally limit likelihood given becomes exercise thus limit likelihood equivalent distortion measure algorithm given note algorithm estimate covariance cluster mean version mixture model general covariance matrix known elliptical algorithm considered sung
['mixtures', 'bernoulli', 'distributions'] mixture distribution chapter distribution continuous vari mixture example mixture illustrate algorithm different context discus discrete binary variable distribution model also known latent class analysis henry peel well practical importance right discus sion mixture also foundation consideration hidden model discrete variable section consider binary variable distribution parameter individual variable independent given mean covariance distribution easily seen consider mixture distribution given mean covariance mixture distribution given exercise covariance matrix longer diagonal mixture distribution capture correlation vari unlike single distribution given data likelihood function model given appearance summation inside logarithm maximum likelihood solution longer closed form derive algorithm likelihood function mixture distribution introduce explicit latent variable associated instance case mixture binary variable single component equal component equal write conditional distribution given latent variable prior distribution latent variable mixture model form product marginalize recover exercise order derive algorithm write likelihood function given next take expectation likelihood respect posterior distribution latent variable give posterior probability responsibility component given data point step responsibility theorem take form consider responsibility enter term written effective number data point associated component step maximize likelihood respect parameter derivative respect equal zero rearrange term obtain exercise set mean component equal weighted mean data weighting given responsibility component take data point maximization respect need introduce multiplier enforce constraint following analogous step used mixture obtain exercise intuitively reasonable result ponent given effective fraction point data component note contrast mixture singularity likelihood function go seen likelihood function bounded exist exercise singularity likelihood function go zero found provided pathological starting point algorithm always increase value likelihood function local maximum found illustrate mixture model figure section model digit digit image turned binary vector setting element whose value exceed setting element data digit digit mixture distribution running iteration algorithm parameter random value chosen uniformly range satisfy constraint mixture distribution able three cluster data corresponding different digit conjugate prior parameter distribution given beta distribution seen beta prior equivalent figure illustration mixture model show example digit data converting value grey scale binary threshold bottom three image show parameter three component mixture model comparison also data single distribution maximum likelihood amount simply count shown rightmost image bottom additional effective observation similarly introduce prior section mixture model maximize posterior probability exercise straightforward extend analysis mixture case multinomial binary variable state making discrete exercise introduce prior model parameter desired
['em', 'bayesian', 'linear', 'regression'] linear regression third example application return evidence proximation linear regression section estimation equation evaluation evidence setting derivative resulting expression zero turn alternative approach based algorithm recall goal maximize evidence function given respect parameter vector regard latent variable hence optimize marginal likelihood function step compute posterior distribution given current ting parameter likelihood step maximize quantity respect already derived posterior distribution given likelihood function given likelihood prior given respectively given taking expectation respect posterior distribution give setting derivative respect zero obtain step equation exercise analogous result hold exercise note equation take slightly different form corresponding result derived direct evaluation evidence function however involve computation inversion decomposition matrix hence comparable computational cost iteration approach course converge result assuming local maximum evidence function veri quantity stationary point evidence function equation hence substitute give obtain precisely equation example consider closely related model namely relevance vector machine regression section used direct marginal likelihood derive equation hyper parameter consider alternative approach view weight vector latent variable apply algorithm step posterior distribution weight given step maximize likelihood expectation taken respect posterior distribution parameter value compute parameter value maximize respect give exercise equation formally equivalent direct exercise
['em', 'algorithm', 'general'] algorithm general expectation maximization algorithm algorithm general technique maximum likelihood solution probabilistic model latent vari dempster give general treatment algorithm process provide proof algorithm derived heuristically section mixture indeed maximize likelihood function hath away neal discussion also form basis derivation variational inference framework section consider probabilistic model collectively denote variable hidden variable joint distribution parameter goal maximize likelihood function given assuming discrete although discussion identical continuous variable combination discrete continuous variable summation integration appropriate shall suppose direct optimization cult likelihood function easier next introduce distribution latent variable serve choice following decomposition hold note functional appendix discussion distribution function parameter worth figure illustration decomposition given hold choice distribution divergence quan lower bound likelihood function carefully form expression particular differ sign also joint distribution conditional distribution given verify decomposition make product rule probability give exercise substitute expression give rise term cancel give likelihood distribution sum divergence posterior distribution recall vergence equality section therefore word lower bound decomposition algorithm iterative optimization technique maximum likelihood solution decomposition algorithm demonstrate indeed maximize likelihood suppose current value parameter vector step lower bound respect holding solution maximization problem easily seen value depend value occur divergence word equal posterior distribution case lower bound equal likelihood figure subsequent step distribution lower bound respect give value cause lower bound increase unless already maximum necessarily cause corresponding likelihood function increase distribution determined parameter value rather value step equal posterior distribution hence nonzero divergence increase likelihood function therefore greater increase lower bound figure illustration step algorithm distribution equal posterior distribution current parameter causing lower bound move value like function divergence vanishing shown figure substitute step lower bound take form constant simply negative entropy distribution fore independent thus step quantity expectation likelihood case note variable inside logarithm joint distribution member exponential family product member logarithm cancel exponential lead step typically much simpler maximization corresponding likelihood function operation algorithm also space schematically figure curve figure illustration step algorithm distribution lower bound respect parameter vector give value divergence cause likelihood increase least much lower bound figure algorithm alter lower bound likelihood rent parameter value bound obtain parameter value text full discussion complete data likelihood function whose value wish maximize start initial parameter value step evaluate distribution latent variable give rise lower bound whose value equal likelihood shown blue curve note bound make tangential contact likelihood curve gradient bound convex function unique exercise maximum mixture component exponential family step bound giving value give value hood subsequent step construct bound tangential shown green curve particular case independent identically distributed data comprise data point comprise corresponding latent variable independence assumption product rule posterior probability step take form posterior distribution also respect case mixture model simply say responsibility mixture component take particular data point value parameter mixture component value data point seen step algorithm value bound likelihood function complete cycle change model parameter cause likelihood increase unless already maximum case parameter remain unchanged also algorithm maximize posterior distribution model prior parameter note function making decomposition constant optimize side alternately respect optimization respect give rise step equation standard algorithm equation introduction prior term typically small cation standard maximum hood equation algorithm break potentially cult problem likelihood function stage step step often prove simpler implement nevertheless complex model case either step step indeed remain intractable lead possible extension algorithm generalized algorithm address problem intractable step instead aiming maximize respect seek instead change parameter increase value lower bound likelihood function complete cycle algorithm increase value likelihood unless parameter already correspond local maximum exploit approach would nonlinear optimization strategy conjugate gradient algorithm step another form algorithm known expectation conditional maximization algorithm making several constrained optimization within step meng instance parameter might partitioned group step broken multiple step subset remainder similarly generalize step algorithm partial rather complete optimization respect neal seen given value unique maximum respect posterior distribution choice bound equal likelihood function algorithm global maximum value also global maximum likelihood provided continuous function continuity local maximum also local maximum consider case independent data point corresponding latent variable joint distribution data point structure incremental form cycle data point time step instead responsibility data point responsibility data point might appear subsequent step would require computation responsibility data point ever mixture component member exponential family responsibility enter simple statistic consider instance case mixture suppose perform update data point corresponding value responsibility step statistic instance mean statistic obtain exercise together corresponding result covariance thus step step take time independent total number data point parameter data point rather waiting whole data incremental sion converge faster batch version step incremental algorithm increasing value shown algorithm local global maximum correspond local global maximum likelihood function exercise consider algorithm section show consequence number possible assignment discrete indicator variable assignment unique optimum algorithm must converge number iteration apply sequential estimation procedure problem root regression function given derivative respect show lead stochastic algorithm data point nearest prototype consider mixture model marginal distribution latent variable given conditional distribution variable given show marginal distribution possible value mixture form suppose wish algorithm maximize posterior parameter model latent variable data show step remains maximum likelihood case whereas step quantity given consider directed graph mixture model shown figure making criterion section show posterior distribution latent variable respect different data point consider special case mixture model matrix component constrained common value derive equation likelihood function model verify maximization likelihood mixture model lead result mean covariance component independently corresponding group data point given fraction point group show maximize respect keeping responsibility obtain closed form solution given show maximize respect keeping responsibility obtain closed form solution given consider density model given mixture distribution suppose partition vector part show conditional density mixture distribution expression component density section relationship mean mixture considering mixture model component covariance show limit complete data likelihood model given equivalent distortion measure algorithm given consider mixture distribution form element could discrete continuous combination denote mean covariance respectively show mean covariance mixture distribution given equation algorithm show distribution parameter value corresponding maximum likelihood function property hence show parameter model compo mean algorithm converge iteration choice initial solution property note degenerate case mixture model component identical practice avoid solution appropriate consider joint distribution latent variable distribution forming product given given show marginalize joint distribution respect obtain show maximize likelihood function mixture distribution respect obtain step equation show maximize likelihood function mixture distribution respect multiplier enforce summation constraint obtain step equation show consequence constraint discrete variable likelihood function mixture distribution bounded hence singularity likelihood go consider mixture model section together prior distribution parameter vector given beta distribution prior given derive algorithm posterior probability consider variable whose component multinomial variable degree binary vector component subject constraint suppose distribution variable mixture discrete multinomial distribution considered section parameter represent probability must satisfy together constraint value given data derive step equation algorithm component parameter distribution maximum likelihood show maximization likelihood function linear regression model lead step estimation result evidence framework section derive equation parameter linear regression model analogous result maximization likelihood derive step equation relevance vector machine regression section used direct maximization marginal like derive equation value regression similarly section used algorithm maximize marginal likelihood giving equation show set equation formally equivalent verify relation respectively show lower bound given gradient respect likelihood function point consider incremental form algorithm mixture responsibility data point starting formula derive result component mean derive formula covariance matrix mixture model responsibility analogous result mean inference central task application probabilistic model evaluation distribution latent variable given visible data variable evaluation expectation respect model might also contain deterministic parameter leave implicit moment fully model unknown parameter given prior distribution absorbed latent variable vector instance algorithm need evaluate expectation likelihood respect posterior distribution latent variable many model practical interest infeasible evaluate posterior distribution indeed compute respect distribution could dimensionality latent space high work directly posterior distribution highly complex form expectation analytically tractable case continuous variable integration analytical solution dimensionality space complexity integrand prohibit numerical integration discrete variable marginal involve possible hidden variable though always possible principle often practice exponentially many hidden state exact calculation prohibitively expensive situation need resort approximation scheme fall broadly class according whether rely stochastic approximation stochastic technique chain monte chapter widespread method across many domain generally property given computational resource generate exact result approximation amount processor time practice sampling method demanding often limiting problem also cult know whether sampling scheme generating independent sample distribution chapter introduce range deterministic approximation scheme scale well large application based analytical posterior distribution example assuming particular parametric form never generate exact result strength weakness complementary sampling method section approximation based local approximation mode maximum distribution turn family approximation technique variational inference vari global criterion widely applied conclude brief introduction alternative variational framework known expectation propagation
['variational', 'inference'] variational inference variational method origin century work calculus variation standard calculus concerned derivative function think function take value variable input return value function output derivative function output value make change input value similarly functional take function input return value functional output example would entropy take probability distribution input return quantity output introduce concept functional derivative press value functional change response change input function rule calculus variation mirror standard calculus appendix many problem expressed term optimization problem quantity functional solution exploring possible input function functional variational method broad applicability include area element method kapur maximum entropy schwarz although nothing intrinsically approximate variational method naturally lend approximate solution done range function optimization instance considering quadratic function considering function linear combination basis function linear combination vary case application probabilistic restriction example take form factorization assumption jordan consider detail concept variational optimization applied inference problem suppose fully model parameter given prior distribution model also latent variable well parameter shall denote latent variable parameter similarly denote variable example might independent identically distributed data probabilistic model joint distribution goal approximation posterior distribution well model evidence discussion decompose marginal probability discussion parameter vector longer parameter stochastic variable absorbed since chapter mainly interested continuous variable used integration rather summation decomposition ever analysis go unchanged variable discrete simply integration summation maximize lower bound optimization respect distribution equivalent divergence allow possible choice maximum lower bound diver equal posterior distribution figure illustration variational approximation example considered figure plot show original distribution yellow along variational green plot show negative logarithm corresponding curve however shall suppose model working true posterior distribution intractable therefore consider instead restricted family distribution seek member family divergence goal restrict family comprise tractable distribution time family rich provide good approximation true posterior distribution important emphasize restriction purely achieve tractability requirement rich family distribution possible particular associated highly approximation simply approach true posterior distribution closely restrict family distribution distribution parameter lower bound becomes function exploit standard nonlinear optimization technique determine optimal value parameter example approach variational distribution respect mean variance shown figure
['factorized', 'distributions'] distribution consider alternative restrict family suppose partition element disjoint group denote assume distribution respect group making assumption particular place restriction functional form individual factor form variational inference proximation framework physic mean theory amongst distribution form seek lower bound therefore wish make free form variational optimization respect distribution respect factor turn achieve substitute dissect dependence factor simply keep notation uncluttered obtain distribution relation notation expectation respect distribution variable suppose keep maximize possible form distribution easily done negative divergence thus equivalent swiss mathematician physicist worked berlin widely considered mathematician time certainly collected work volume amongst many contribution modern theory function together calculus variation discovered formula four important number mathematics last year life almost totally blind nearly half result period divergence minimum thus obtain general expression optimal solution given worth taking moment study form solution basis application variational method say optimal factor simply considering joint distribution hidden visible variable taking expectation respect factor additive constant distribution thus take exponential side normalize practice shall convenient work form instate normalization constant inspection become clear subsequent example equation given represent condition maximum lower bound subject factorization constraint however represent explicit solution sion side optimum expectation respect factor therefore seek consistent solution factor appropriately cycling factor turn estimate given side current estimate factor convergence bound convex respect factor
['properties', 'factorized', 'approximations'] property approximation approach variational inference based approximation true posterior distribution consider moment problem general distribution distribution begin discus problem distribution provide useful insight type inaccuracy approximation consider distribution correlated variable mean precision element symmetry precision matrix suppose wish approximate distribution form apply general result expression optimal factor useful note side need retain term functional dependence term absorbed normalization constant thus next observe side expression quadratic function identify distribution worth assume rather derived result variational optimization divergence possible distribution note also need consider additive constant explicitly normalization constant found inspection technique square identify section mean precision giving symmetry also written note solution coupled expectation respect vice general address treating variational solution equation cycling variable turn convergence criterion shall example shortly however note problem simple closed form solution found particular equation take easily shown solution provided nonsingular result figure exercise mean correctly variance direction variance variance along orthogonal direction general result variational proximation give approximation posterior distribution compact comparison suppose instead reverse divergence shall form divergence figure comparison alternative form divergence green contour corresponding standard deviation correlated distribution variable contour represent corresponding level distribution variable given product independent distribution whose parameter minimization divergence reverse divergence used alternative approximate inference framework expectation prop therefore consider general problem section approximation form divergence written form constant term simply entropy depend optimize respect factor easily done multiplier give exercise case optimal solution given marginal distribution note solution require iteration apply result illustrative example distribution vector give result shown figure mean approximation correct place cant probability mass region variable space difference result understood large positive contribution divergence figure another comparison alternative form divergence blue contour show bimodal distribution given mixture contour correspond single distribution best sense divergence contour correspond distribution found numerical minimization divergence showing different local minimum divergence region space near zero unless also close zero thus form divergence lead distribution avoid region small conversely divergence distribution nonzero region nonzero gain insight different behaviour diver consider multimodal distribution unimodal figure practical application true posterior often multimodal posterior mass concentrated number relatively small region parameter space multiple mode arise ability latent space complex dependence parameter type multimodality chapter context mixture manifested multiple maximum likelihood function variational treatment based minimization tend mode contrast minimize resulting approximation would average across mode context mixture model would lead poor predictive distribution average good parameter value typically good parameter value possible make useful inference procedure rather different approach considered detail discus expectation propagation section form divergence member alpha family divergence continuous parameter divergence limit whereas limit value equality exercise suppose distribution minimize respect distribution divergence zero forcing value typically underestimate support tend seek mode mass conversely divergence value typically stretch cover overestimate support obtain symmetric divergence linearly related distance given square root distance valid distance metric
['example', 'univariate', 'gaussian'] example illustrate variational approximation single variable goal infer posterior distribution mean precision given data value assumed drawn independently gaus likelihood function given introduce conjugate prior distribution given gamma distribution together distribution constitute conjugate prior distribution section simple problem posterior distribution found exactly take form distribution however tutorial purpose exercise consider variational approximation posterior distribution given note true posterior distribution factorize optimum factor general result square mean precision given exercise note give maximum likelihood result precision similarly optimal solution factor given hence gamma distribution parameter exhibit behaviour exercise assume functional form optimal distribution arose naturally structure likelihood function corresponding conjugate prior section thus expression optimal distribution moment respect distribution solution therefore make initial guess moment recompute distribution given extract moment recompute distribution since space hidden variable example dimensional illustrate variational posterior distribution plotting contour true posterior approximation figure figure illustration variational inference mean precision contour true posterior distribution shown green contour initial approximation shown blue factor factor contour optimal approximation iterative scheme shown general need iterative approach order solve optimal posterior distribution simple example considering however explicit solution simultaneous equation optimal factor simplify expression considering broad noninformative prior although parameter setting correspond improper prior posterior distribution still well standard result mean gamma distribution together appendix obtain second order moment form substitute moment solve give exercise recognize side familiar unbiased estimator variance distribution bias maximum likelihood solution section
['model', 'comparison'] model comparison well inference hidden variable also wish compare candidate model index prior probability goal approximate posterior probability data slightly complex situation considered different model different structure indeed different dimensionality hidden variable fore simply consider approximation must instead posterior must conditioned must consider readily verify following decomposition based variational distribution exercise lower bound given assuming discrete analysis continuous latent variable provided summation integration maximize respect distribution multiplier result exercise however maximize respect solution different coupled expect conditioned proceed instead individually optimization subsequently resulting value used model selection model usual
['illustration', 'variational', 'mixture', 'gaussians'] illustration variational mixture return discussion mixture model apply vari inference machinery previous section provide good illustration application variational method also demonstrate treatment elegantly resolve many associated maximum likelihood approach reader work example detail many insight practical cation variational method many model corresponding much sophisticated distribution straightforward extension general analysis starting point likelihood function mixture model graphical model figure observation corresponding latent variable binary vector denote data similarly denote latent variable write conditional distribution given form similarly write conditional distribution data vector given latent variable component parameter note working term precision matrix rather covariance matrix somewhat next introduce prior parameter analysis conjugate prior distribution therefore choose section distribution symmetry chosen parameter compo normalization constant distribution figure directed acyclic graph model plate note observation seen parameter effective section prior number observation associated component mixture value small posterior distribution primarily data rather prior similarly introduce independent prior governing mean precision component given conjugate prior distribution mean unknown typically would choose symmetry section resulting model directed graph shown note link since variance distribution function example nice illustration distinction latent vari parameter variable appear inside plate latent variable number variable size data contrast variable outside plate number independently size data parameter perspective graphical model however really fundamental difference
['variational', 'distribution'] variational distribution order formulate variational treatment model next write joint distribution random variable given various factor reader take moment verify decomposition indeed correspond probabilistic graphical model shown figure note variable consider variational distribution latent variable parameter remarkable assumption need make order obtain tractable practical solution mixture model particular functional form factor determined automatically optimization variational distribution note script distribution much distribution argument distinguish different distribution corresponding sequential update equation factor easily derived making general result consider derivation update equation factor factor given make decomposition note interested functional dependence side variable thus term depend absorbed additive normalization constant giving substituting conditional distribution side absorbing term independent additive constant dimensionality data variable taking exponential side obtain distribution value quantity binary value obtain exercise optimal solution factor take functional form prior note given exponential real quantity quantity discrete distribution standard result quantity role responsibility note optimal solution moment respect distribution variable variational update equation coupled must iteratively point shall convenient three statistic data respect responsibility given note analogous quantity maximum likelihood algorithm mixture model consider factor variational posterior general result observe side expression term together term variational posterior give term comprise term leading factorization term side depend used taking exponential side recognize distribution component given finally variational posterior distribution factorize product always product rule write form factor found reading term involve result distribution given exercise update equation analogous equation algorithm maximum likelihood solution mixture computation must order update variational posterior distribution model parameter involve evaluation sum data arose maximum likelihood treatment order perform variational step need expectation responsibility given expression expectation respect variational distribution parameter easily give exercise digamma function result follow standard property distribution appendix substitute make obtain following result responsibility notice similarity corresponding result responsibility maximum likelihood written form used precision place covariance highlight similarity thus optimization variational posterior distribution cycling stage analogous step maximum likelihood algorithm variational equivalent step current distribution model parameter evaluate moment hence evaluate subsequent variational equivalent step keep responsibility recompute variational distribution parameter case variational posterior distribution functional form corresponding factor joint distribution general result consequence choice conjugate distribution section figure show result approach faith data mixture model component convergence component value numerically distinguishable prior value effect understood qualitatively term automatic model data complexity model section complexity penalty component whose parameter away prior value component take essentially responsibility data point hence parameter revert prior value principle component slightly data point broad prior effect small seen numerically mixture model value posterior distribution given exercise consider component prior broad component play role model whereas figure variational mixture plied faithful data ellipsis denote density contour component density inside ellipse mean value ponent number left diagram show iteration variational infer component whose numerically distinguishable zero plotted prior tightly figure prior form recall figure prior solution zero figure component nonzero instead choose obtain three component nonzero component nonzero seen close similarity variational solution mixture algorithm maximum likelihood fact consider limit treatment maximum likelihood algorithm anything small data set dominant computational cost variational algorithm mixture evaluation responsibility together evaluation inversion weighted data covariance matrix computation mirror arise maximum likelihood algorithm little computational overhead approach maximum likelihood however substantial advantage first singularity arise maximum likelihood ponent collapse onto data point absent treatment indeed singularity removed simply introduce prior estimate instead maximum likelihood furthermore choose large number component mixture finally variational treatment open possibility optimal number component mixture without technique cross validation section
['variational', 'lower', 'bound'] variational lower bound also straightforwardly evaluate lower bound model practice useful able monitor bound order test convergence also provide valuable check math expression solution implementation step iterative procedure value bound decrease take stage provide test correctness mathematical derivation update equation difference check update indeed give constrained maximum bound bishop variational mixture lower bound given keep notation uncluttered superscript distribution along subscript expectation operator expectation taken respect random variable argument various term bound easily give following result exercise dimensionality entropy given respectively note term expectation log distribution simply represent negative entropy distribution cation combination term expression summed give lower bound however kept expression rate ease understanding finally worth lower bound alternative approach variational equation section fact since model conjugate prior functional form factor variational posterior distribution known namely discrete taking general parametric form distribution derive form lower bound function parameter distribution bound respect parameter give equation exercise
['predictive', 'density'] predictive density application mixture model often interested predictive density value variable observation corresponding latent variable density given unknown true posterior distribution parameter perform summation give integration intractable approximate predictive density true posterior distribution variational approximation give made factorization term variable integration analytically giving mixture student exercise component mean precision given given size data large predictive distribution mixture exercise
['determining', 'number', 'components'] number component seen variational lower bound used determine distribution number component mixture model section however subtlety need given setting parameter mixture model except degenerate setting exist parameter setting density vari identical parameter value differ component instance consider mixture single variable parameter value parameter value component symmetry give rise value mixture model component parameter setting member family equivalent setting exercise context maximum likelihood redundancy irrelevant parameter optimization algorithm example depending initial parameter solution equivalent play role setting however marginalize possible figure plot variational lower bound versus number mixture model faithful data showing distinct peak component value model trained different random start result shown symbol plotted small random perturbation distinguished note solution suboptimal local maximum pen infrequently parameter value seen figure true posterior distribution multimodal variational inference based minimization tend approximate distribution mode ignore equivalent mode equivalent predictive density concern provided considering model number component however wish compare different value need take account multimodality simple approximate solution term onto lower bound used model comparison exercise figure show plot lower bound multimodality versus number component faithful data worth maximum likelihood would lead value hood function increase monotonically assuming singular solution effect local maximum used determine appropriate model complexity contrast inference automatically make model complexity data section approach determination range model different value trained alternative approach suitable value treat parameter make point estimate value lower bound bishop respect instead probability distribution fully approach lead equation exercise maximization variational update distribution parameter component provide contribution explaining data driven zero optimization effectively removed model automatic relevance determination make single training start relatively large initial value allow surplus component model origin sparsity respect detail context relevance vector machine section
['induced', 'factorizations'] induced factorization variational update equation mixture model assumed particular factorization variational posterior distribution given however optimal solution various factor exhibit additional factorization particular solution given product independent distribution component mixture whereas variational posterior distribution latent variable given independent distribution observation note factorize respect value constrained additional factorization consequence interaction assumed factorization conditional independence property true distribution directed graph figure shall refer additional factorization induced factorization cause arise interaction factorization assumed posterior distribution conditional independence property true joint distribution numerical implementation variational approach important take account additional factorization instance would maintain full precision matrix distribution variable optimal form distribution always precision matrix corresponding factorization respect individual variable induced factorization easily simple graphical test based partition latent variable three disjoint group suppose assuming factorization latent variable general result together product rule probability optimal solution given whether resulting solution factorize word whether happen conditional inde relation test relation hold choice making criterion illustrate consider mixture directed graph figure assuming variational given immediately variational posterior distribution parameter must factorize param path either must node conditional inde test respect path
['variational', 'linear', 'regression'] variational linear regression second illustration variational inference return linear regression model section evidence framework integration making point estimate marginal likelihood fully approach would integrate well parameter although exact integration intractable variational method tractable approximation order discussion shall suppose noise precision parameter known true value although framework easily extended include distribution linear regression model variational treatment exercise turn equivalent evidence framework nevertheless good exercise variational method also foundation variational treatment logistic regression section recall likelihood function prior given introduce prior distribution section know conjugate prior precision given gamma distribution choose thus joint distribution variable given directed graphical model shown figure
['variational', 'distribution'] variational distribution goal approximation posterior distribution employ variational framework section variational figure probabilistic graphical model joint linear regression model posterior distribution given expression equation factor distribution making general result recall factor take joint distribution variable average respect variable factor consider distribution keeping term functional dependence recognize gamma distribution obtain similarly variational equation posterior distribution general result keeping term functional dependence quadratic form distribution complete square usual identify mean covariance giving note close similarity posterior distribution parameter difference variational distribution indeed chosen notation covariance matrix case standard result obtain moment evaluation variational posterior distribution begin distribution alternately factor turn suitable convergence criterion usually term lower bound shortly instructive relate variational solution found evidence framework section consider case corresponding limit broad prior mean variational posterior distribution given comparison show case particularly simple model variational approach give precisely expression evidence function except point estimate value distribution expectation approach give identical result case broad prior
['predictive', 'distribution'] predictive distribution predictive distribution given input easily model variational posterior parameter integral making result model variance given note take form result except value
['lower', 'bound'] lower bound another quantity importance lower bound evaluation various term straightforward making result exercise previous chapter give figure show plot lower bound versus degree polynomial model synthetic data degree three polynomial prior parameter corresponding noninformative prior uniform section section quantity lower bound marginal likelihood model assign equal prior probability different value interpret approximation model probability thus variational framework highest probability model maximum likelihood result ever smaller residual error model increasing complexity residual error driven zero causing maximum likelihood severely model figure plot lower bound order polynomial model data point polynomial inter additive noise variance value bound give prob ability model value bound peak corresponding true model data
['exponential', 'family', 'distributions'] exponential family distribution chapter important role exponential family distribution conjugate prior many model book likelihood drawn exponential family however general case marginal likelihood function data example mixture joint distribution corresponding hidden variable member exponential family whereas marginal distribution mixture hence grouped variable model variable hidden variable make distinction latent variable parameter parameter intensive independent size data whereas latent variable extensive scale number size data example mixture model indicator variable specify component responsible generating data point represent latent variable whereas mean precision proportion represent parameter consider case independent identically distributed data denote data value corresponding latent variable suppose joint distribution latent variable member exponential family natural parameter shall also conjugate prior written recall conjugate prior distribution prior number observation value vector consider variational distribution latent variable parameter general result solve factor thus independent term value hence solution factorize example induced factorization taking exponential section side normalization comparison standard form exponential family similarly variational distribution parameter taking exponential side normalization inspection note solution coupled solve iter procedure variational step evaluate statistic current posterior distribution latent variable compute posterior distribution parameter subsequent variational step posterior distribution natural parameter give rise variational distribution latent variable
['variational', 'message', 'passing'] variational message passing application variational method considering model mixture detail model directed graph shown figure consider variational method model directed graph derive number widely applicable result joint distribution corresponding directed graph written decomposition variable associated node parent corresponding node note latent variable belong variable consider variational approximation distribution assumed factorize respect note node factor variational distribution substitute general result give term side depend absorbed additive constant fact term depend distribution given together conditional distribution conditional distribution correspond child node therefore also depend child node parent child node besides node node blanket node figure thus update factor variational posterior distribution local calculation graph make possible construction general purpose variational inference form model need advance bishop specialize case model conditional structure variational update dure cast term local message passing algorithm bishop particular distribution associated particular node node received message parent child turn child already received message parent evaluation lower bound also many quantity already part message passing scheme distributed message passing formulation good scaling property well large network
['local', 'variational', 'methods'] local variational method variational framework section considered global method sense directly seek approximation full distribution random variable alternative local approach bound function individual variable group variable within model instance might seek bound conditional distribution factor much probabilistic model directed graph purpose bound course simplify resulting distribution local approximation applied multiple variable turn tractable approximation section shall give practical example approach context logistic regression focus bound already seen discussion divergence convexity logarithm function role lower bound global variational approach strictly convex every chord lie function convexity also play section central role local variational framework note discussion equally concave function lower bound upper bound begin considering simple example namely function convex function shown plot figure goal approximate simpler function particular linear function figure linear function lower bound tangent obtain tangent line value making order expansion equality example function figure curve show function blue line show tangent line slope note tangent line ample one shown green smaller value show corresponding plot function given versus maximum figure plot curve show convex function blue line linear function lower bound given value slope contact point tangent line slope found respect discrepancy shown green dashed line given dual function negative intercept tangent line slope therefore obtain tangent line form linear function consistency subsequent discussion different value correspond different tangent line line lower bound function thus write function form convex function simpler function price variational parameter obtain bound must optimize respect formulate approach generally framework convex duality jordan consider illustration convex function shown plot figure example function lower bound best lower bound linear function slope bound given tangent line write equation tangent line slope negative intercept clearly slope tangent determine intercept note line must vertically amount equal vertical distance line function shown figure thus instead consider particular adjust tangent plane tangent particular value tangent line particular value contact point function play dual role related apply duality relation simple example value given obtain conjugate function form previously function shown plot figure check substitute give value original function concave function follow similar argument obtain upper bound function interest convex concave directly apply method obtain bound however seek invertible transformation either function argument change form calculate conjugate function transform back original variable important example frequently pattern recognition logistic sigmoid function stand function neither convex concave however take logarithm obtain function concave easily veri second derivative corresponding conjugate function take exercise form recognize binary entropy function variable whose probability value obtain upper bound appendix sigmoid figure plot show logistic sigmoid function together example exponential upper bound shown blue plot show logistic sigmoid together lower bound shown blue parameter bound exact dashed green line taking exponential obtain upper bound logistic sigmoid form plotted value plot figure also obtain lower bound sigmoid functional form follow jordan make input variable function first take logistic function decompose note function convex function variable veri second derivative lead exercise lower bound linear function whose conjugate function given condition lead tanh denote value corresponding contact point tangent line particular value tanh instead thinking variational parameter play role lead simpler expression conjugate function given hence bound written bound sigmoid becomes bound plot figure bound form exponential quadratic function prove useful seek representation posterior distribution logistic sigmoid function section logistic sigmoid frequently probabilistic model binary vari function odds ratio posterior prob ability corresponding transformation distribution given function unfortunately lower bound derived logistic section sigmoid directly extend method distribution bound although rigorous proof given used apply local variational method problem shall example local variational bound section moment however instructive consider general term bound used suppose wish evaluate integral form logistic sigmoid probability density integral arise model instance wish evaluate distribution case posterior parameter distribution integral intractable employ variational bound write form variational parameter becomes product function analytically give bound freedom choose variational parameter value function resulting value bound within family bound used approximation bound however general exact although bound logistic sigmoid exactly choice value bound exact value quantity value value compromise weighted distribution
['variational', 'logistic', 'regression'] variational logistic regression illustrate local variational method logistic regression model studied section approximation consider variational treatment based approach jordan like method also lead approximation posterior distribution however greater variational approximation lead accuracy method furthermore unlike method variational approach well objective function given bound model evidence logistic regression also perspective monte sampling technique
['variational', 'posterior', 'distribution'] variational posterior distribution shall make variational approximation based local bound section likelihood function logistic sion logistic sigmoid quadratic form therefore convenient choose conjugate prior form moment shall treat constant section shall demonstrate variational formalism extended case unknown hyper parameter whose value data variational framework seek maximize lower bound marginal likelihood logistic regression model marginal likelihood take form note conditional distribution written order obtain lower bound make variational lower bound logistic sigmoid function given reproduce convenience therefore write note bound applied term likelihood function separately variational parameter corresponding training observation multiplying prior distribution obtain following bound joint distribution variational parameter evaluation exact posterior distribution would require normalization left hand side inequality intractable work instead side note function side probability density give variational posterior distribution however longer bound logarithm function monotonically increasing inequality give lower bound joint distribution form substituting prior side inequality becomes function quadratic function obtain corresponding variational approximation posterior distribution linear quadratic term giving variational posterior form framework approximation posterior distribution however additional provided vari parameter lead accuracy approximation jordan considered batch learning context training data available however method intrinsically well sequential learning data point time formulation variational approach sequential case straightforward exercise note bound given problem approach directly generalize cation problem class alternative bound case
['optimizing', 'variational', 'parameters'] variational parameter approximation posterior distribution shall shortly evaluate predictive distribution data point first however need determine variational parameter lower bound marginal likelihood substitute inequality back marginal hood give optimization linear regression model section approach approach recognize function integration view latent variable invoke algorithm second approach integrate analytically perform direct maximization begin considering approach algorithm start choosing initial value parameter denote collectively step algorithm parameter value posterior distribution given step maximize likelihood given expectation taken respect posterior distribution depend substituting obtain term independent derivative respect equal zero line algebra making give note monotonic function restrict attention value without loss generality symmetry bound around thus hence obtain following equation exercise used summarize algorithm variational posterior initialize variational parameter step evaluate posterior distribution given mean step variational posterior compute value given step repeated suitable convergence criterion practice typically iteration alternative approach equation note integral lower bound integrand form integral analytically integral differentiate respect turn give rise exactly equation approach given exercise already application variational method useful able evaluate lower bound given integration analytically exponential quadratic function thus square making standard result normalization distribution obtain closed form solution take form exercise figure illustration approach logistic regression simple linearly separable data plot left show predictive distribution variational inference decision boundary lie roughly cluster data point contour predictive distribution splay away data greater uncertainty cation region plot right show decision boundary corresponding sample parameter vector drawn posterior distribution variational framework also applied situation data sequentially jordan case maintain posterior distribution prior data point posterior making bound give posterior distribution predictive distribution posterior take form approximation section figure show variational predictive distribution thetic data example interesting insight concept large margin section qualitatively similar solution
['inference', 'hyperparameters'] inference prior distribution known constant extend logistic regression model allow value parameter data combining global local variational approximation single framework maintain lower bound marginal likelihood stage combined approach adopted bishop context treatment hierarchical mixture expert model consider simple isotropic prior form analysis readily extended general prior instance wish associate different different subset usual consider conjugate given gamma distribution constant marginal likelihood model take form joint distribution given faced analytically intractable integration shall tackle local global variational approach model begin introduce variational distribution apply decomposition instance take form lower bound divergence point lower bound still intractable form likelihood factor therefore apply local variational bound logistic sigmoid factor inequality place lower bound therefore also lower bound marginal likelihood next assume variational distribution parameter factorization appeal general result expression optimal factor consider distribution term independent substitute giving quadratic function solution square usual obtain similarly optimal solution factor substituting obtain recognize gamma distribution obtain also need optimize variational parameter also done lower bound term independent note precisely form appeal result direct optimization marginal likelihood function leading equation form equation three quantity making suitable cycle quan turn moment given appendix
['expectation', 'propagation'] expectation propagation conclude chapter alternative form deterministic inference known expectation propagation variational method based minimization divergence reverse form give approximation rather different property consider moment problem respect distribution member exponential family written form function divergence becomes constant term independent natural parameter within family distribution setting gradient respect zero giving however already seen negative gradient given expectation distribution result obtain optimum solution simply matching statistic instance minimize divergence setting mean equal mean distribution covariance equal covariance sometimes moment matching example seen figure exploit result obtain practical algorithm approximate inference many probabilistic model joint distribution data hidden variable parameter product factor form would arise example model independent identically distributed data factor data point along factor corresponding prior generally would also apply model directed probabilistic graph factor conditional distribution corresponding node undirected graph factor clique potential interested posterior distribution purpose making prediction well model evidence purpose model comparison posterior given model evidence given considering continuous variable following discussion equally discrete variable integral summation shall pose along respect posterior distribution make prediction intractable form approximation expectation propagation based approximation posterior also given product factor factor approximation factor true posterior factor constant ensure side unity order obtain practical algorithm need constrain factor particular shall assume come exponential family product factor therefore also exponential family statistic example overall approximation also ideally would like determine divergence true posterior approximation given note reverse form divergence used inference general minimization intractable vergence respect true distribution rough could instead minimize divergence corresponding pair factor much simpler problem solve advantage algorithm however individually product factor could well give poor approximation expectation propagation make much better approximation factor turn context factor start factor cycle factor time similar spirit update factor variational framework considered suppose wish factor remove factor product give conceptually determine form factor product close possible keep factor approximation accurate region high posterior probability factor shall example effect apply clutter problem achieve remove factor section current approximation posterior distribution note could instead product factor although practice division usually easier combined factor give distribution figure illustration expectation propagation approximation distribution example considered figure plot show original distribution yellow along global variational green blue approximation plot show corresponding negative logarithm distribution note distribution variational inference consequence different form divergence normalization constant given determine factor diver easily distribution family appeal result tell parameter matching statistic corresponding moment shall assume tractable example choose distribution equal mean distribution covariance generally straightforward obtain member exponential family provided statistic related derivative normalization given approximation figure factor found taking dividing factor used determined multiplying side give used fact value therefore found matching moment combining found integral practice several pass made factor factor turn posterior distribution model evidence factor approximation expectation propagation given joint distribution data stochastic variable form product factor wish approximate posterior distribution distribution form also wish approximate model evidence initialize factor initialize posterior approximation setting convergence choose factor remove posterior division evaluate posterior setting statistic moment equal evaluation normalization constant evaluate store factor evaluate approximation model evidence special case known assumed density moment matching koller factor except unity making factor assumed density appropriate learning data point sequence need learn data point discard considering next point however batch setting opportunity reuse data point many time order achieve curacy idea expectation propagation furthermore apply batch data result undesirable dependence arbitrary order data point considered overcome disadvantage expectation propagation guarantee iteration converge however approximation exponential family iteration converge resulting solution stationary point particular energy function although iteration necessarily decrease value energy function contrast variational iteratively lower bound marginal likelihood iteration decrease bound possible optimize cost function directly case converge although resulting algorithm complex implement another difference variational form divergence algorithm former whereas latter figure distribution multimodal lead poor approximation particular applied mixture result approximation try capture mode posterior distribution conversely model often local variational method approximation figure illustration clutter problem data space dimensionality training data point noted cross drawn mixture component shown green goal infer mean green data
['example', 'clutter', 'problem'] example clutter problem following illustrate algorithm simple exam goal infer mean distribution variable given observation drawn distribution make problem interesting observation background clutter also distributed figure value therefore mixture take form proportion background clutter assumed known prior taken parameter value joint distribution observation given posterior distribution mixture thus computational cost problem exactly would grow exponentially size data exact solution intractable moderately large apply clutter problem identify factor next select distribution family example convenient choose spherical factor approximation therefore take form function form equal prior note imply side density fact shall variance parameter negative simply convenient shorthand notation approximation unity corresponding dimensionality hence initial therefore equal prior iteratively factor taking factor time note need revise term update leave term unchanged state exercise result leave reader detail first remove current estimate division give mean inverse variance given exercise next evaluate normalization constant give similarly compute mean variance mean variance give exercise quantity simple interpretation probability point clutter compute factor whose parameter given process repeated suitable termination criterion instance maximum change parameter value resulting complete figure example approximation factor version clutter problem showing blue green notice current form control range good approximation factor threshold finally evaluate approximation model evidence given example factor approximation clutter problem space shown figure note factor approximation even negative value variance parameter simply approximation curve upwards instead necessarily problematic provided overall approximate posterior variance figure compare performance variational mean theory approximation clutter problem
['expectation', 'propagation', 'graphs'] expectation propagation graph general discussion factor distribution function component similarly factor distribution consider situation factor depend subset variable conveniently expressed framework probabilistic graphical model chapter factor graph representation directed undirected graph posterior mean flop error evidence flop error figure comparison expectation propagation variational inference approximation clutter problem plot show error posterior mean versus number point operation plot show corresponding result model evidence shall focus case distribution fully shall show case expectation propagation loopy belief propagation start show context simple example shall explore general case first recall minimize diver respect distribution optimal solution factor simply corresponding marginal consider factor graph shown left figure context algorithm joint distribution section given seek approximation factorization note normalization constant local normalization generally done belief propagation pose restrict attention approximation factor factorize respect individual variable factor graph shown right figure individual factor overall distribution fully apply algorithm fully approximation pose factor choose factor figure left simple factor graph figure convenience right corresponding approximation remove factor distribution give multiply exact factor give divergence result noted product factor variable factor given corresponding marginal four given multiplying together factor change update involve variable namely obtain factor simply divide give precisely message belief propagation section sage variable node factor node folded message factor node variable node particular message sent factor node variable node given substitute obtain giving message result slightly standard belief propagation message direction time easily modify procedure give standard form algorithm factor time instance unchanged version given term time choose order done wish particular graph follow update scheme corresponding standard belief propagation schedule result exact inference variable factor approximation factor case unimportant consider general factor graph corresponding distribution subset variable associated factor approximate fully distribution form individual variable node suppose wish particular term keeping term remove term give multiply exact factor determine term need consider functional dependence simply corresponding marginal multiplicative constant taking marginal term function variable term correspond factor cancel numerator denominator subsequently divide therefore obtain recognize rule form message vari able node factor node example shown figure quantity message factor node variable node product factor depend variable variable variable common factor word compute outgoing message factor node take product incoming message factor node multiply local factor marginalize thus algorithm special case expectation distribution fully distribution corresponding partially graph could used achieve higher accuracy another generalization group factor together set factor together iteration approach lead improvement accuracy general problem choosing best combination group disconnection open research issue seen variational message passing expectation propagation different form divergence shown broad range message passing algorithm derived framework minimization member alpha family diver given include variational message passing loopy belief propagation expectation propagation well range algorithm space discus message wainwright fractional belief propagation power exercise verify marginal distribution data decomposed term form given given property solve simultaneous hence show provided original distribution nonsingular unique solution mean factor distribution given consider variational distribution form technique multiplier verify minimization divergence respect factor keeping factor lead solution suppose distribution wish approximate distribution writing form divergence show minimization respect lead result given expectation given covariance consider model hidden stochastic variable noted collectively latent variable together model parameter suppose variational distribution tent variable parameter distribution point estimate form vector free parameter show variational optimization distribution equivalent algorithm step step posterior distribution respect alpha family divergence show divergence done writing taking similarly show consider problem mean precision gaus variational approximation considered section show factor form mean precision given respectively similarly show factor gamma distribution form parameter given consider variational posterior distribution precision whose parameter given standard result mean variance gamma distribution given show variational posterior distribution mean given inverse maximum likelihood estimator variance data variance go zero making standard result mean gamma distribution together derive result reciprocal precision variational treat derive decomposition given used mate posterior distribution model variational inference multiplier enforce normalization constraint distribution show maximum lower bound given starting joint distribution general result show optimal variational distribution latent variable mixture given step given text starting derive result optimum vari posterior distribution mixture hence verify expression parameter distribution given distribution verify result result show value variational mixture given verify result term lower bound variational mixture model given verify result term lower bound variational mixture model given exercise shall derive variational equation mixture model direct differentiation lower bound assume variational distribution factorization factor given substitute hence obtain lower bound function parameter distribution bound respect parameter derive equation factor variational distribution show section derive result predictive distribution variational treat mixture model exercise variational solution mixture model size data large show would expect maximum likelihood solution based derived chap note result appendix used help answer exercise first show posterior distribution precision becomes sharply peaked around maximum likelihood solution posterior mean next consider posterior distribution show becomes sharply peaked around maximum likelihood solution similarly show responsibility become equal corresponding maximum likelihood value large making following asymptotic result digamma function large finally making show large predictive distribution becomes mixture show number equivalent parameter setting interchange mixture model component seen mode posterior distribution model member family equivalent mode suppose result running variational inference algorithm approximate posterior mode approximate full posterior distribution mixture distribution mode equal show assume negligible overlap component mixture resulting lower bound single component distribution extra term consider variational mixture model prior distribution instead parameter whose value found variational lower bound marginal likelihood show lower bound respect multiplier enforce constraint lead result note need consider term lower bound dependence bound seen section singularity likelihood treatment mixture model arise treatment discus whether singularity would arise model maximum posterior estimation variational treatment mixture section made approximation posterior figure assumption cause variance posterior distribution certain direction parameter space discus qualitatively effect variational approximation model evidence effect vary number component mixture hence explain whether variational mixture tend underestimate overestimate optimal number component extend variational treatment linear regression include gamma solve assuming variational distribution form derive variational update equation three factor variational distribution also obtain expression lower bound predictive distribution making formula given appendix show variational lower bound linear basis function regression model written form various term rewrite model mixture section conjugate model exponential family section hence general result derive result show function concave second derivative determine form dual function verify minimization respect according indeed function second derivative show logistic function concave derive variational upper bound directly making second order expansion logistic function around point second derivative respect show function concave function consider second derivative respect variable hence show convex function plot graph derive lower bound logistic sigmoid function directly making order series sion function variable value consider variational treatment logistic regression learning data point time must next data point show proximation posterior distribution lower bound distribution prior data point absorbed corresponding variational parameter quantity respect variational parameter show update equation logistic regression model given exercise derive equation variational logistic regression model section direct maximization lower bound given derivative equal zero making result derivative determinant together expression mean covariance variational posterior distribution derive result lower bound variational logistic regression model easily done substituting expression prior together lower bound likelihood function integral next gather together term depend exponential complete square give integral standard result normalization finally take logarithm obtain consider approximation scheme section show inclusion factor lead update model evidence form normalization constant result derive result consider expectation propagation algorithm section suppose factor family functional form distribution show factor update leaf unchanged situation typically factor prior prior factor incorporated exactly need exercise next shall verify result expectation propagation algorithm applied clutter problem begin division formula derive expression square inside exponential identify mean variance also show normalization constant given clutter problem done making general result show mean variance applied clutter problem given prove following result expectation make result next prove result square exponential finally derive result method probabilistic model practical interest exact inference intractable resort form approximation chapter inference algorithm based deterministic approximation include method variational expectation propagation consider mate inference method based numerical sampling also known monte technique although application posterior distribution unobserved vari direct interest situation posterior distribution primarily purpose expectation example order make prediction fundamental problem therefore wish address chapter expectation function respect probability distribution component might comprise discrete continuous variable combination thus case continuous figure schematic illustration function whose expectation respect distribution variable wish evaluate expectation integral summation case discrete variable schematically single continuous variable figure shall suppose expectation complex exactly technique general idea behind sampling method obtain sample drawn independently distribution expectation long sample drawn distribution estimator correct mean variance estimator given exercise variance function distribution worth accuracy estimator therefore depend dimension principle high accuracy achievable relatively small number sample practice twenty independent sample estimate expectation accuracy problem however sample might independent effective sample size might much smaller apparent sample size also back figure note small region large vice expectation dominated region small probability relatively large sample size achieve accuracy many model joint distribution conveniently term graphical model case directed graph variable straightforward sample joint distribution assuming possible sample conditional distribution node following sampling approach section joint distribution variable associated node variable associated parent node obtain sample joint distribution make variable order sampling conditional distribution always possible cause step parent value graph sample joint distribution consider case directed graph node value principle extend procedure least case node discrete variable give following logic sampling approach seen special case sampling section step value variable whose value value value agree sample value proceeds next variable turn however value value disagree whole sample algorithm start node graph algorithm sample correctly posterior distribution simply drawing sample joint distribution hidden variable data variable sample disagree data slight saving sampling joint distribution soon contradictory value however overall probability sample posterior decrease rapidly number variable increase number state variable take increase approach rarely used practice case probability distribution undirected graph sampling strategy sample even prior distribution variable instead expensive technique must employed sampling section well sampling conditional distribution also require sample marginal distribution already strategy sampling joint distribution straightforward obtain sample marginal distribution simply value sample numerous text dealing monte method interest statistical inference perspective include neal also review article brook neal provide additional information sampling method statistical inference diagnostic test convergence chain monte algorithm practical guidance sampling method context machine learning given bishop
['basic', 'sampling', 'algorithms'] basic sampling algorithm section consider simple strategy generating random sample given distribution sample computer algorithm fact number deter calculated must nevertheless appropriate test randomness generating number raise several subtlety press outside scope book shall assume algorithm provided number distributed uniformly indeed environment facility built
['standard', 'distributions'] standard distribution consider generate random number simple nonuniform assuming already available source uniformly distributed random number suppose uniformly distributed interval transform value function distribution case goal choose function resulting value desired distribution obtain inde integral thus exercise transform uniformly distributed random number function inverse inde integral desired distribution figure consider example exponential distribution case lower limit integral thus transform uniformly distributed variable exponential distribution figure geometrical interpretation formation method generating distributed random number inde integral desired uniformly distributed random variable distributed cording another example distribution transformation method applied given distribution case inverse inde integral expressed term function exercise generalization multiple variable straightforward change variable example transformation method consider method generating sample distribution first suppose pair uniformly distributed random number transforming variable distributed uniformly next discard pair unless lead uniform distribution point inside unit circle figure pair evaluate quantity figure method generating random number start generating sample uniform distribution inside unit circle joint distribution given exercise independent distribution zero mean unit variance distribution zero mean unit variance distribution mean variance generate vector valued variable distribution mean variance make decomposition take form press vector valued random variable whose component independent distributed zero mean unit vari mean covariance exercise obviously transformation technique success ability calculate invert inde integral distribution operation feasible limited number simple distribution must turn alternative approach search general strategy consider technique rejection sampling importance sampling though mainly limited distribution thus directly applicable complex problem many dimension form important component general strategy
['rejection', 'sampling'] rejection sampling rejection sampling framework sample relatively complex distribution subject certain constraint begin considering discus extension multiple dimension subsequently suppose wish sample distribution simple standard distribution considered sampling directly cult furthermore suppose often case easily able evaluate given value constant readily unknown order apply rejection sampling need simpler distribution sometimes proposal distribution readily draw sample figure rejection sampling method sample drawn distribution fall grey area tween scaled resulting sample distributed according version next introduce constant whose value chosen value function comparison function distribution figure step rejection sampler generating random number first generate number distribution next generate number uniform distribution pair random number uniform distribution curve function finally sample otherwise thus pair lie grey shaded region pair uniform distribution curve hence corresponding value distributed according desired exercise original value distribution accepted probability probability sample accepted given thus fraction point method ratio area distribution area curve therefore constant small possible subject limitation must nowhere illustration rejection sampling consider task sampling gamma distribution form shown figure suitable proposal distribution therefore transformation method sample need generalize slightly ensure nowhere smaller value gamma distribution transforming uniform random variable give random number distributed according exercise figure plot showing gamma given green curve scaled distribution shown curve sample gamma distribution sampling rejection criterion minimum reject rate setting constant small possible still satisfying requirement resulting comparison function also figure
['adaptive', 'rejection', 'sampling'] adaptive rejection sampling many instance might wish apply rejection sampling prof cult determine suitable analytic form envelope distribution alternative approach construct envelope function based value distribution wild construction envelope function particularly straightforward case cave word derivative nonincreasing function construction suitable envelope function graphically figure function gradient initial grid point intersection resulting tangent line used construct envelope function next sample value drawn envelope distribution straightforward envelope distribution succession exercise figure case distribution concave envelope function rejection sampling tangent line grid point sample point added grid point used envelope distribution figure illustrative example rejection sampling sampling distribution shown green curve rejection sampling proposal also whose scaled version shown curve linear function hence envelope distribution piecewise exponential distribution form sample drawn usual rejection criterion applied sample accepted draw desired distribution however sample incorporated grid point tangent line envelope function thereby number grid point increase envelope function becomes better approximation desired distribution probability rejection decrease variant algorithm evaluation derivative adaptive rejection sampling framework also extended concave simply following rejection sampling step step section giving rise adaptive rejection metropolis sampling clearly rejection sampling practical value require parison function close distribution rate rejection kept minimum examine rejection sampling space high dimensionality consider sake illustration somewhat problem wish sample distribution covariance unit matrix rejection sampling proposal distribution distribution covariance obviously must order optimum value given figure acceptance rate ratio volume thus acceptance rate exponentially dimensionality even percent acceptance ratio approximately illustrative example comparison function close distribution practical exam desired distribution multimodal sharply peaked extremely cult good proposal distribution comparison function figure importance sampling address prob expectation respect distribution cult draw sample instead sample drawn simpler distribution corresponding term summation weighted ratio furthermore exponential decrease acceptance rate dimensionality generic feature rejection sampling although rejection useful technique dimension unsuited problem high dimensionality however play role sophisticated algorithm sampling high dimensional space
['importance', 'sampling'] importance sampling principal reason wishing sample complicated probability distribution able evaluate expectation form technique importance sampling framework expectation provide mechanism drawing sample distribution approximation expectation given able draw sample distribution suppose however impractical sample directly evaluate easily given value simplistic strategy expectation would uniform grid evaluate integrand form obvious problem approach number term summation exponentially dimensionality furthermore already noted kind probability distribution interest often much mass relatively small region space uniform sampling problem small proportion sample make cant contribution would really like choose sample point fall region large ideally product large case rejection sampling importance sampling based proposal distribution easy draw sample figure express expectation form sample drawn quantity known importance weight rect bias sampling wrong distribution note unlike rejection sampling sample often case distribution normalization constant easily whereas unknown similarly wish importance sampling distribution property sample evaluate ratio result hence rejection sampling success importance sampling approach crucially well sampling distribution match desired distribution often case strongly cant proportion mass concentrated relatively small region space importance weight dominated weight large value weight relatively cant thus effective sample size much smaller apparent sample size prob even severe none sample fall region large case apparent variance small even though estimate expectation severely wrong hence major draw back importance sampling method potential produce result arbitrarily error diagnostic indication also highlight sampling distribution namely small zero region cant distribution term graphical model apply sampling technique various way discrete variable simple approach uniform sampling joint distribution directed graph sample joint distribution setting variable evidence equal value variable independently uniform distribution space possible determine corresponding weight sample note sampling distribution uniform possible choice subset variable equality fact every sample necessarily consistent evidence thus weight simply proportional note variable order approach yield poor result posterior distribution uniform often case practice improvement approach likelihood weighted sampling chang based ancestral sampling variable variable turn variable evidence value evidence conditional distribution variable currently value weighting associated resulting sample given method extended sampling importance sampling distribution continually current posterior distribution
['sampling', 'importance', 'resampling'] rejection sampling method section part success determination suitable value constant many pair distribution impractical determine suitable value value large guarantee bound desired distribution lead small acceptance rate case rejection sampling approach also make sampling distribution termine constant stage scheme stage sample drawn second stage weight finally second sample drawn discrete distribution probability given weight resulting sample approximately distributed according distribution becomes correct limit consider case note cumulative distribution value given indicator function equal argument true otherwise taking limit assuming suitable regularity replace sum integral weighted according original sampling distribution cumulative distribution function normal value given initial sample value approximately drawn desired distribution rejection approximation sampling distribution get closer desired distribution initial sample desired distribution weight value also desired distribution moment respect distribution directly original sample together weight
['sampling', 'em', 'algorithm'] sampling algorithm addition providing mechanism direct implementation framework monte method also play role paradigm example maximum likelihood solution particular sampling method used approximate step algorithm model step analytically consider model hidden variable visible variable parameter function respect step likelihood given sampling method approximate integral drawn current estimate posterior distribution function usual step procedure monte algorithm straightforward extend problem mode posterior distribution estimate prior distribution simply function step particular instance monte algorithm stochastic consider mixture model draw sample step latent variable component mixture responsible generating data point step sample taken posterior distribution data effectively make hard assignment data point component mixture step approximation posterior distribution used update model parameter usual suppose move maximum likelihood approach full treatment wish sample posterior distribution param vector principle would like draw sample joint posterior shall suppose cult suppose relatively straightforward sample parameter posterior data augmentation algorithm alter nates step known imputation step analogous step posterior step analogous step algorithm wish sample directly therefore note relation hence draw sample current mate draw sample given relation sample compute estimate posterior distribution given assumption feasible sample approximation note making somewhat distinction parameter hidden variable blur distinction focus simply problem drawing sample given posterior distribution
['markov', 'chain', 'monte', 'carlo'] chain monte previous section rejection sampling importance strategy expectation function suffer severe limitation particularly space high dimensionality therefore turn section general powerful framework chain monte sampling large class distribution scale well dimensionality sample space chain monte method origin physic metropolis towards cant impact statistic rejection importance sampling sample proposal distribution time however maintain record current state proposal distribution current state sequence sample form chain write section assume readily given value although value unknown proposal distribution chosen simple straightforward draw sample directly cycle algorithm generate candidate sample proposal distribution accept sample according appropriate criterion basic metropolis algorithm metropolis assume proposal distribution symmetric value candidate sample accepted probability choosing random number uniform distribution unit interval sample note step cause increase value candidate point certain kept candidate sample accepted otherwise candidate point another candidate sample drawn distribution contrast rejection sampling sample simply metropolis algorithm candidate point previous sample included instead list sample leading multiple copy sample course practical implementation single copy sample would kept along integer weighting factor recording many time state shall long positive value necessary condition distribution however sequence independent sample successive sample highly correlated wish obtain independent sample discard sequence tain every sample large sample practical purpose independent figure show simple illustrative exam sampling distribution metropolis algorithm proposal distribution isotropic insight nature chain monte algorithm looking property example namely simple random figure simple illustration algorithm sample distribution whose contour shown ellipse proposal isotropic whose standard deviation step accepted shown green line step shown total candidate sample walk consider state space integer probability state step initial state state time also zero similarly easily seen thus step random walk exercise distance average proportional square root square root dependence typical random walk behaviour show random walk exploring state space shall central goal designing chain monte method avoid random walk behaviour
['markov', 'chains'] chain chain monte method detail study general property chain detail particular circumstance chain converge desired chain series random variable following conditional independence property hold course directed graph form chain ample shown figure specify chain giving probability distribution initial variable together conditional probability subsequent variable form transition chain homogeneous transition probability marginal probability particular variable expressed term marginal probability previous variable chain form distribution said invariant stationary respect chain step chain leaf distribution invariant thus homogeneous chain transition probability distribution invariant note given chain invariant distribution instance transition probability given identity transformation distribution invariant necessary condition invariant choose transition probability satisfy property detailed balance particular distribution easily seen transition probability detailed balance respect particular distribution leave distribution invariant chain respect detailed balance said reversible goal chain sample given distribution achieve chain desired distribution invariant however must also require distribution invariant distribution irrespective choice initial property invariant distribution equilibrium distribution clearly ergodic chain equilibrium distribution shown homogeneous chain ergodic subject weak restriction invariant distribution transition probability neal practice often construct transition probability base transition mixture distribution form satisfying alternatively base transition combined successive application distribution invariant respect base transition also invariant respect either given case mixture base transition detailed balance mixture transition also satisfy detailed hold transition probability though order application base transition form detailed balance common ample composite transition probability base transition change subset variable
['metropolis', 'hastings', 'algorithm'] algorithm basic metropolis algorithm without actually demon sample distribution giving proof discus generalization known algorithm case proposal distribution longer symmetric function argument particular step algorithm rent state draw sample distribution accept probability label member possible transition considered evaluation acceptance criterion require knowledge normal constant probability distribution symmetric proposal distribution criterion metropolis criterion given show invariant distribution chain algorithm showing detailed balance choice proposal distribution marked effect performance algorithm continuous state space common choice current state leading important variance parameter distribution variance small figure schematic illustration isotropic proposal distribution blue circle sample correlated distribution ellipse different deviation different direction algorithm order keep rejection rate scale proposal distribution order standard deviation lead random walk behaviour number step state approximately independent order standard deviation proportion accepted transition high progress state space take form slow random walk leading long correlation time however variance parameter large rejection rate high kind complex problem considering many step state probability consider distribution strong correlation component scale proposal distribution large possible without high rejection rate order length scale system distribution along extended direction mean random walk number step arrive state independent original state order fact dimension increase rejection rate increase offset step size transition accepted generally number step obtain independent sample scale like deviation neal detail aside remains case length scale distribution vary different different direction metropolis hastings algorithm slow convergence
['gibbs', 'sampling'] sampling sampling simple widely applicable chain monte algorithm seen special case metropolis hastings algorithm consider distribution wish sample suppose chosen initial state chain step sampling procedure value variable value drawn distribution variable conditioned value variable thus replace value drawn distribution component procedure repeated either cycling variable particular order choosing variable step random distribution example suppose distribution three variable step algorithm selected value replace value sampling conditional next replace value sampling conditional distribution value used straight away subsequent sampling step update sample drawn cycling three variable turn sampling initialize sample sample sample sample spent almost entire life house built father engineering united state chair mathematical physic united state yale post received salary time publication vector analysis made crystallography planetary orbit famous work equilibrium substance laid foundation science physical chemistry show procedure sample distribution note distribution invariant sampling step individually hence whole chain fact sample marginal distribution clearly invariant value unchanged also step sample correct conditional distribution conditional marginal distribution together specify joint distribution joint distribution invariant second requirement order sampling dure sample correct distribution ergodic condition none conditional distribution anywhere zero case point space point number step update component variable requirement conditional distribution zero must proven explicitly distribution initial state must also order complete algorithm although sample drawn many iteration effectively become independent distribution course successive sample chain highly correlated obtain sample nearly independent necessary subsample sequence obtain sampling procedure particular instance algorithm consider sampling step variable variable remain transition probability given note component unchanged sampling step also thus factor acceptance probability given used thus step always accepted metropolis algorithm gain insight behaviour sampling investigating application distribution consider correlated variable figure distribution width marginal distribution width typical step size conditional distribution order state according random walk number step obtain independent sample distribution order course distribution uncorrelated sampling procedure would simple problem could rotate order variable however practical application generally infeasible transformation approach reducing random walk behaviour sampling original form problem figure illustration sampling alter update variable whose distribution correlated step size deviation conditional green curve lead slow progress direction elongation joint distribution ellipse number step obtain independent sample distribution conditional distribution general class distribution example distribution conditional distribution step sampling algorithm conditional distribution particular component mean variance frame work value random variable zero mean unit variance parameter method equivalent standard sampling step opposite side mean step leaf desired distribution invariant mean variance effect encourage directed motion state space variable highly correlated framework ordered neal approach distribution practical applicability sampling ease sample drawn conditional distribution case probability distribution graphical model conditional individual node depend variable corresponding blanket figure directed graph wide choice distribution individual node conditioned parent lead conditional distribution sampling concave adaptive sampling method section therefore provide framework monte sampling directed graph broad applicability graph distribution exponential family relationship preserve conjugacy full conditional sampling functional form figure sampling method sample drawn conditional distribution variable variable graphical model conditional distribution function state node blanket undirected graph shown left directed graph blanket parent child shown right conditional distribution conditioned parent node standard sampling technique employed general full conditional distribution complex form permit standard algorithm however concave sampling done adaptive rejection sampling assuming corresponding variable scalar stage sampling algorithm instead drawing sample corresponding conditional distribution make point estimate variable given maximum conditional distribution obtain conditional mode algorithm section thus seen greedy approximation sampling basic sampling technique variable time strong dependency successive sample opposite extreme could draw sample directly joint distribution operation supposing intractable successive sample would independent hope improve simple sampler intermediate strategy sample successively group variable rather individual vari blocking sampling algorithm choosing block variable necessarily disjoint sampling jointly variable block turn conditioned variable
['slice', 'sampling'] slice sampling seen metropolis algorithm step size small result slow random walk behaviour whereas large result high rate technique slice sampling neal adaptive step size automatically match characteristic distribution able evaluate distribution consider case slice sampling additional variable drawing sample joint space shall another example approach discus hybrid monte section goal sample uniformly area distribution figure illustration slice sampling given value value chosen uniformly region slice distribution shown solid horizontal line infeasible sample directly slice sample drawn region previous value given otherwise marginal distribution given sample sampling value alternately sampling given value evaluate sample uniformly range straightforward sample uniformly slice distribution figure practice cult sample directly slice instead sampling scheme leaf uniform distribution invariant detailed balance suppose current value corresponding sample next value considering region choice region characteristic length scale distribution take place want region encompass much slice possible allow large move space little possible region lying outside slice make sampling approach choice region starting region width testing point within slice either point region extended direction increment value point lie outside region candidate value chosen uniformly region lie within slice form lie outside slice region shrunk form point region still another candidate point drawn uniformly reduced region value found lie within slice slice sampling applied distribution repeatedly variable turn manner sampling able compute component function proportional
['hybrid', 'monte', 'carlo', 'algorithm'] hybrid monte algorithm already noted major limitation metropolis algorithm exhibit random walk behaviour whereby distance traversed state space square root number step problem resolved simply taking bigger step lead high rejection rate section introduce sophisticated class transition based analogy physical system property able make large change system state keeping rejection probability small plicable distribution continuous variable readily evaluate gradient probability respect state variable discus dynamical system framework section section explain combined metropolis algorithm yield hybrid monte algorithm background physic section result derived principle
['dynamical', 'systems'] dynamical system dynamical approach stochastic sampling origin algorithm behaviour physical system chain monte simulation goal sample given probability distribution framework dynamic casting probabilistic simulation form system order remain keeping literature area make relevant dynamical system terminology appropriate along dynamic consider evolution state variable continuous time denote classical dynamic newton second motion acceleration object proportional applied force corresponding differential time decompose equation coupled order equation intermediate momentum variable corresponding rate change state variable component position variable dynamic perspective thus position variable corresponding momentum variable joint space position momentum variable phase space without loss generality write probability distribution form potential energy system state system acceleration rate change momentum given applied force negative gradient potential energy convenient reformulate dynamical system framework kinetic energy total energy system potential kinetic energy function express dynamic system term given exercise rowan mathematician physicist child prodigy pointed professor astronomy trinity college fore even graduated important contribution formulation dynamic cant role later development quantum mechanic great achievement development quaternion generalize concept complex number three distinct square root minus satisfy said equation walking along royal canal wife promptly equation side bridge although longer evidence carving stone plaque bridge discovery quaternion equation evolution dynamical system value constant easily seen differentiation second important property dynamical system known theorem preserve volume phase space word consider region within space variable region equation dynamic shape change volume seen rate change location phase space given divergence consider joint distribution phase space whose total energy distribution given result conservation volume conservation dynamic leave invariant seen considering small region phase space approximately constant follow evolution equation time volume region remain unchanged value region hence probability density function also unchanged although invariant value vary dynamic time duration becomes possible make large change systematic random walk behaviour evolution dynamic however sample value constant order arrive ergodic sampling scheme introduce additional move phase space change value also leaving distribution invariant achieve replace value drawn distribution conditioned sampling step hence section also leaf desired distribution invariant independent distribution conditional distribution straightforward sample exercise practical application approach address problem numerical integration equation introduce numerical error devise scheme impact error fact turn integration scheme theorem still hold exactly property important hybrid monte algorithm section scheme leapfrog alternately approximation position momentum variable take form update momentum variable step size update position variable step size second update momentum variable several leapfrog step applied succession seen update momentum variable combined update step size successive update position momentum variable leapfrog order advance dynamic time interval need take step error involved approximation continuous time dynamic zero assuming smooth function limit however nonzero used practice residual error remain shall section effect error hybrid monte algorithm summary dynamical approach alternating tween series leapfrog update momentum variable marginal distribution note dynamic method unlike basic metropolis able make information gradient probability distribution well distribution analogous situation familiar domain function optimization case gradient available highly advantageous make informally fact space dimension additional computational cost gradient function typically factor independent whereas gradient vector piece information piece information given function
['hybrid', 'monte', 'carlo'] hybrid monte previous section nonzero step size leapfrog algorithm introduce error integration dynamical equation hybrid monte neal combine dynamic metropolis algorithm thereby remove bias associated algorithm us chain alternate stochastic update momentum variable dynamical update leapfrog algorithm application leapfrog algorithm resulting candidate state accepted according metropolis criterion based value thus initial state state leapfrog integration candidate state accepted probability leapfrog integration simulate dynamic perfectly every candidate step would automatically accepted value would unchanged numerical error value sometimes decrease would like metropolis criterion remove bias effect ensure resulting sample indeed drawn order case need ensure update equation corresponding leapfrog integration satisfy detailed balance easily leapfrog scheme start leapfrog integration sequence choose random equal probability whether integrate forward time step size backwards time step size note leapfrog integration scheme integration step step size exactly undo effect integration step step size next show leapfrog integration preserve volume exactly fact step leapfrog scheme update either variable variable amount function variable shown figure effect shearing region phase space volume finally result show detailed balance hold consider small region phase space sequence leapfrog iteration step size map region conservation volume leapfrog iteration volume choose initial point distribution update leapfrog interaction probability transition going given factor probability choosing integrate positive step size rather negative similarly probability starting figure step leapfrog algorithm either position variable momentum variable change variable function region phase space without change volume region backwards time region given easily seen probability equal hence detailed balance hold note proof overlap region exercise easily generalized allow overlap cult construct example leapfrog algorithm return starting position number iteration case random replacement momentum value leapfrog integration ensure position variable never phenomenon easily choosing magnitude step size random small interval leapfrog integration gain insight behaviour hybrid monte considering application convenience consider distribution independent component given conclusion equally valid distribution correlated component hybrid monte algorithm exhibit rotational isotropy leapfrog integration pair variable dependently however acceptance rejection candidate point based value value variable thus cant integration error variable could lead high prob ability rejection order discrete leapfrog integration reasonably good approximation true dynamic necessary leapfrog integration scale smaller potential value denote recall goal leapfrog integration hybrid monte move substantial distance phase space state relatively independent initial state still achieve high probability acceptance order achieve leapfrog integration must continued number iteration order contrast consider behaviour simple metropolis algorithm isotropic proposal distribution variance considered order avoid high rejection rate value must order exploration state space proceeds random walk take order step arrive roughly independent state
['estimating', 'partition', 'function'] partition function seen sampling algorithm considered chapter quire functional form probability distribution multiplicative constant thus write value normalization constant also known partition order draw sample however knowledge value useful model comparison since model evidence probability data given model interest consider value might assume direct evaluation function state space intractable model comparison actually ratio partition function model multiplication ratio ratio prior probability give ratio posterior probability used model selection model estimate ratio partition function importance sampling distribution energy function sample drawn distribution partition function analytically example absolute value approach yield accurate result importance sampling closely distribution ratio wide variation practice suitable analytically importance sampling distribution readily found kind complex model considered book alternative approach therefore sample chain distribution transition probability chain given sample given sampling distribution written used directly method ratio partition function require ce corresponding distribution reasonably closely especially problematic wish absolute value partition function complex distribution relatively simple distribution partition function directly estimate ratio partition function directly unlikely successful problem tackled technique known neal barber bishop succession intermediate distribution interpolate simple distribution evaluate normalization desired complex distribution intermediate ratio determined monte method construct sequence intermediate system energy function continuous parameter distribution intermediate ratio found monte single chain restart chain ratio case chain initially system suitable number step move next distribution sequence note however system must remain close equilibrium distribution stage exercise show sample estimator mean equal variance given suppose random variable uniform distribution transform given show distribution given random variable uniformly distributed formation distribution given suppose uniformly distributed unit circle shown figure make change variable given show distributed according random variable zero mean unit covariance matrix suppose positive symmetric matrix decomposition lower triangular matrix zero leading diagonal show variable distribution mean covariance technique generating sample general gaus sample zero mean unit variance exercise show carefully rejection sampling indeed draw sample desired distribution suppose proposal show probability sample value accepted given distribution proportional constant value value note probability drawing value given probability drawing value time probability value given drawn make along product rule probability write form distribution show equal suppose uniform distribution interval show variable distribution given determine expression envelope distribution adaptive rejection sampling requirement continuity making technique section sampling single exponential distribution devise algorithm sampling piecewise exponential distribution show simple random walk integer property hence induction figure probability distribution variable uniform shaded region zero everywhere else show sampling algorithm section detailed balance consider distribution shown figure discus whether standard sampling procedure distribution ergodic therefore whether would sample correctly distribution consider simple node graph shown figure node given distribution mean precision suppose marginal distribution mean precision given gamma distribution write expression conditional distribution would order apply sampling posterior distribution verify update mean variance zero mean unit variance give value mean variance show equation equivalent similarly show equivalent making show conditional figure graph variable prior distribution mean precision verify probability equal hence detailed balance hold hybrid monte algorithm mixture explore model latent variable continuous important motivation model many data set property data point close manifold much lower dimensionality original data space might arise consider artificial data taking digit image image size padding value zero corresponding white location orientation digit varied random figure resulting image point data space however across data image three degree variability corresponding vertical horizontal translation rotation data point therefore live subspace data space whose intrinsic dimensionality three note figure synthetic data taking digit image copy digit random displacement rotation within image field resulting image manifold nonlinear instance translate digit past particular value zero white black back zero clearly nonlinear function digit position example rotation parameter latent variable observe image vector told value translation rotation variable used create real digit image data degree freedom scaling moreover multiple degree freedom complex deformation variability individual well difference writing individual number degree freedom small dimensionality data appendix another example provided flow data given configuration phase degree freedom variability corresponding fraction pipe water fraction determined data space data point close manifold within space case manifold distinct segment corresponding different flow regime segment noisy continuous manifold goal data compression density benefit manifold data point confined precisely smooth dimensional manifold interpret departure data point manifold noise lead naturally generative view model first select within manifold according latent variable distribution generate data point noise drawn conditional distribution data given latent continuous latent variable model distribution latent variable make section variable slate latent variable lead technique principal component analysis well related model factor analysis section chapter begin treatment show naturally maximum likelihood solution seek pace dimensionality subspace magenta line data point onto point green based projection error line model probabilistic bring many parameter model allow number data finally briefly eral latent concept assumption anal model nonlinear latent principal component analysis principal technique cation dimensionality eduction data feature data also tile commonly used definition rise algorithm defined data space data defined linear projection average cost defined mean distance data oint projection definition variance variable goal project data onto pace data hall assume chapter shall consider technique determine appropriate value data begin consider projection onto space define direction space vector convenience without loss generality shall choose unit vector note interested direction defined magnitude data point onto scalar value mean data sample mean given variance data given data covariance matrix defined appendix maximize variance respect clearly constrained maximization prevent appropriate constraint come normalization condition enforce constraint introduce multiplier shall denote make unconstrained maximization setting derivative respect equal zero quantity stationary point say must make variance given variance maximum equal eigenvalue known first principal component define additional principal component incremental fashion choosing direction variance appendix
['principal', 'component', 'analysis'] principal component analysis amongst possible direction orthogonal already considered sider general case dimensional projection space optimal linear variance data defined data covariance matrix corresponding eigenvalue easily shown proof induction summarize principal component analysis mean covariance matrix data finding corresponding eigenvalue algorithm finding eigenvalue well additional theorem related decomposition found loan note computational cost full decomposition matrix size plan project data onto first principal component need find first eigenvalue done efficient technique power method loan scale like alternatively make algorithm
['minimum', 'error', 'formulation'] formulation discus alternative formulation based projection error minimization introduce complete basis vector satisfy basis complete data point exactly linear combination basis vector coefficient different different data point simply rotation system system defined original component equivalent taking inner product making property obtain without loss generality write goal however approximate data point representation restricted number variable corresponding projection onto subspace dimensional linear subspace without loss generality first basis vector approximate data point depend particular data point whereas constant data point free choose minimize distortion reduction dimensional distortion measure shall squared distance original data point approximation data goal minimize consider first minimization respect quantity setting derivative respect zero making condition obtain similarly setting derivative respect zero making relation give substitute make general expansion obtain displacement vector lie space orthogonal principal subspace linear combination figure point must within principal subspace move freely within subspace minimum error given orthogonal projection therefore obtain expression distortion measure function purely form remains task respect must constrained minimization otherwise obtain vacuous result constraint arise condition shall solution expressed term expansion covariance matrix considering formal solution obtain intuition result considering case data space dimensional principal subspace choose direction minimize subject normalization constraint multiplier enforce constraint consider minimization exercise appendix setting derivative respect zero obtain eigenvalue thus define point distortion measure find value minimum solution distortion measure give therefore obtain minimum value choosing smaller eigenvalue thus choose principal subspace eigenvalue result accord intuition order minimize average squared projection distance choose principal component subspace mean data point direction maximum variance case eigenvalue equal choice principal direction give rise value general solution minimization arbitrary arbitrary choosing covariance matrix given usual chosen corresponding value distortion measure given simply eigenvalue orthogonal principal subspace therefore obtain minimum value eigenvalue hence principal subspace corresponding eigenvalue although considered analysis still hold case dimensionality reduction simply rotation ax align principal component finally worth closely related linear dimensionality reduction technique canonical correlation analysis bach jordan whereas work single random variable variable try find corresponding pair linear subspace high component within subspace correlated single component subspace solution expressed term generalized problem
['applications', 'pca'] application illustrate data compression considering line digit data covariance matrix vector figure mean data space represent imago silo data first along figure complete decreasing order shown figure measure particular value gien different figure write data digit data data onto data various made relation follow represent data data mailer greater degree example data point digit data shown figure application data case goal dimensionality rather data algorithm applied successfully data typically done original significantly instance faithful data time eruption order greater erupt applied data first made linear individual zero mean unit known matrix component known original data make data zero mean unit different become first rile equation form ooid figure illustration effect linear applied faithful data plot left show original data plot show result individual variable zero mean unit variance also shown principal ax data plotted range plot right show result whitening data give zero mean unit covariance diagonal matrix element matrix column given define data point value given sample mean defined clearly zero mean covariance given identity matrix appendix appendix operation known whitening data faithful data figure interesting compare fisher linear discriminant section method technique linear dimensionality reduction however unsupervised value whereas fisher linear discriminant also us information difference example figure another common application principal component analysis data visual data point onto principal subspace data point plotted given corresponding second eigenvalue example plot flow data shown figure comparison compo analysis discriminant data belonging class blue onto single maximum lead strong class overlap whereas take class label lead projection onto gean giving much class separation lata lata onto compone blue point correspond flow
['pca', 'high', 'dimensional', 'data'] data application component analysis number data point smaller dimensionality data pace example might want apply data hundred image pace million image space point linear hose point greater indeed pelf find least hose data typical finding matrix scale like image application problem foil define dimensional data matrix whose given matrix written corresponding equation becomes premultiply side give define obtain equation matrix eigenvalue original covariance matrix additional eigenvalue value zero thus solve problem space lower dimensionality computational cost instead order determine multiply side give eigenvalue note however need determine normalization constant assuming unit length give summary apply approach first evaluate find vector eigenvalue compute original data space probabilistic formulation previous section based linear projection data onto subspace lower dimensionality original data space show also expressed maximum likelihood solution probabilistic latent variable model reformulation known probabilistic several advantage conventional probabilistic constrained form distribution number free parameter restricted still model capture dominant correlation data section probabilistic derive algorithm efficient situation leading evaluate data covariance matrix intermediate step combination probabilistic model deal miss value data mixture probabilistic model trained algorithm probabilistic form basis treatment dimensionality principal subspace found automatically data existence likelihood function direct comparison probabilistic density model contrast conventional assign reconstruction cost data point close principal subspace even arbitrarily training data probabilistic used model density hence applied classification problem probabilistic model generatively provide sample distribution formulation probabilistic model independently tipping bishop shall later closely related factor analysis probabilistic simple example framework marginal conditional distribution late probabilistic first explicit latent variable corresponding subspace next define prior distribution latent variable together conditional distribution variable conditioned value latent variable prior distribution given similarly conditional distribution variable conditioned value latent variable form section mean general linear function matrix wand vector note respect element word example naive model shall shortly column span linear subspace within data space principal subspace parameter model scalar governing variance conditional distribution note space space point first drawing value prior drawing mean show density loss assuming zero mean unit latent would rise equivalent view probabilistic model first latent tent specifically defined noise variable generative process figure based latent pace data space contrast data space latent space wish parameter maximum write function need pression marginal rule form marginal given result also derived directly predictive distribution mean covariance give used fact independent random variable hence uncorrelated intuitively think distribution defined taking isotropic spray moving across principal subspace density determined weighted prior distribution density give rise pancake shaped distribution represent marginal density predictive distribution parameter however redundancy corresponding rotation latent space consider matrix orthogonal matrix orthogonality property quantity covariance matrix take form exercise hence independent thus whole family matrix give rise predictive distribution invariance understood term rotation within latent space shall return discussion number independent parameter model later evaluate predictive distribution require inversion matrix computation reduced making matrix inversion identity give matrix defined invert rather directly cost reduced well predictive distribution also require posterior written directly result model give note posterior mean whereas posterior covariance dependent figure probabilistic model data expressed directed graph observation associated value latent variable
['maximum', 'likelihood', 'pca'] maximum likelihood next consider determination model parameter maximum likelihood given data data point probabilistic model expressed directed graph shown figure corresponding likelihood function given setting derivative likelihood respect equal zero give result data mean defined write likelihood function form data covariance matrix defined likelihood quadratic function solution unique maximum confirmed second derivative maximization respect complex nonetheless exact solution shown tipping bishop stationary point likelihood function written matrix whose column given subset size data covariance matrix diagonal matrix element given corresponding eigenvalue arbitrary orthogonal matrix furthermore tipping bishop maximum like function chosen whose eigenvalue solution saddle point similar independently although proof given shall assume order value corresponding eigenvalue principal case column define principal subspace corresponding maximum likelihood solution given section average variance associated dimension orthogonal rotation matrix latent space substitute solution expression make orthogonality property independent simply say predictive density unchanged rotation latent space particular case column principal component scaled variance parameter interpretation scaling factor clear recognize convolution independent distribution case latent space distribution noise model variance additive thus variance direction composed contribution projection latent space distribution data space corresponding column plus isotropic contribution variance added direction noise model worth taking moment study form covariance matrix given consider variance predictive distribution along direction unit vector given first suppose orthogonal principal subspace word given linear combination hence thus model noise variance orthogonal principal subspace average eigenvalue suppose subspace word model correctly capture variance data along principal ax variance direction single average value construct maximum likelihood density model would simply find eigenvalue data covariance matrix evaluate wand result given case would choose convenience however maximum likelihood solution found numerical optimization likelihood function instance algorithm conjugate gradient fletcher wright bishop algorithm resulting value arbitrary column need orthogonal orthogonal basis matrix appropriately loan alternatively algorithm yield principal direction sorted descending order corresponding eigenvalue directly rotational invariance latent space form statistical analogous mixture model case discrete latent variable continuum parameter lead predictive density contrast discrete associated component mixture setting consider case reduction dimension making orthogonality property covariance marginal distribution becomes obtain standard maximum likelihood solution unconstrained distribution covariance matrix given sample conventional generally projection point dimensional data space onto dimensional linear subspace probabilistic however naturally expressed latent space data space application visualization data compression reverse theorem point data space posterior mean covariance latent space mean given given project point data space given section note take form equation linear regression consequence likelihood function linear model similarly posterior covariance given independent take limit posterior mean exercise exercise section orthogonal projection data point onto latent space recover standard model posterior covariance limit zero however density becomes singular latent projection towards origin relative orthogonal projection finally note important role probabilistic model distribution number degree free word number independent parameter whilst still model capture dominant correlation data recall general distribution independent parameter covariance matrix plus another parameter mean thus number parameter scale quadratically become excessive space high dimensionality restrict covariance matrix diagonal independent parameter number parameter linearly dimensionality however treat variable independent hence longer express correlation probabilistic elegant compromise significant correlation still total number parameter linearly number degree freedom model covariance matrix parameter size giving total parameter count however seen redundancy associated rotation system latent space orthogonal matrix express rotation size first column matrix independent parameter column vector must unit length second column independent parameter column must also must orthogonal previous column arithmetic series total independent parameter thus number degree freedom covariance matrix given exercise section section number independent parameter model therefore linearly fixed take recover standard result full covariance case variance along linearly dependent direction column variance along direction given model equivalent isotropic covariance case
['em', 'algorithm', 'pca'] algorithm seen probabilistic model expressed term continuous latent space data point corresponding latent variable therefore make algorithm find maximum likelihood estimate model parameter seem rather pointless already exact maximum likelihood parameter value however space high dimensionality computational advantage iterative procedure rather working directly sample covariance matrix procedure also extended factor analysis model solution finally missing data handled derive algorithm probabilistic following general framework thus write likelihood take expectation respect posterior distribution latent distribution parameter value maximization complete data likelihood yield parameter value data point assumed independent likelihood function take form matrix given already know exact maximum likelihood solution given sample mean defined convenient substitute stage making expression latent conditional distribution respectively expectation respect posterior distribution latent variable obtain note posterior distribution sufficient tic thus step parameter value evaluate exercise follow directly posterior distribution together result defined step maximize respect wand keeping posterior statistic fixed maximization respect straightforward respect make obtain equation algorithm probabilistic proceeds parameter alternately sufficient statistic latent space posterior distribution step parameter value step benefit algorithm computational efficiency application unlike conventional based decomposition sample covariance matrix approach iterative might appear attractive however cycle algorithm much efficient conventional space high dimensionality note covariance matrix computation often interested first corresponding eigenvalue case algorithm however evaluation covariance matrix take computation number data point algorithm snapshot method assume linear combination data vector avoid direct evaluation covariance matrix hence unsuited large data set algorithm also construct covariance matrix explicitly instead demanding step sum data large significant saving offset iterative nature algorithm note algorithm form data point read next data point considered note quantity step vector matrix data point separately step need accumulate sum data point approach advantageous large fully probabilistic model deal missing data provided missing random unobserved variable missing value algorithm give example approach data visualization figure another elegant feature approach take limit corresponding standard still obtain valid algorithm quantity need compute furthermore step emphasize simplicity algorithm define matrix size whose given vector similarly define matrix size whose given vector algorithm becomes step take form form equation simple interpretation discussion step orthogonal projection data point onto current estimate principal subspace correspondingly step principal probabilistic portion data lata plot mean ion lata principal subspace plot variable value note data missing plot miss subspace minimize squared error physical analogy algorithm easily data point dimension tile principal subspace data point energy square lile spring length keep allow attachment point minimize attachment point independently position orthogonal data point onto keep attachment release tile allow tile minimum energy repeated figure
['bayesian', 'pca'] tile tile principal gien according application choose whereas application choice data example figure line digit look lite form group mall large natural practice seen lata algorithm defined data data point shown shown scaled initial defined shown point lata space shown cyan alter step laten update success fixed tile beau employ cro likelihood data hover become probabilistic tipping bishop seek appropriate gien natural approach model marginalize model peel appropriate prior distribution done framework intractable marginal likelihood given bourd different giving marginal likelihood consider simpler approach based figure probabilistic graphical model distribution parameter matrix vector proximation appropriate number data point relatively large corresponding posterior distribution tightly peaked bishop specific choice prior surplus dimension principal subspace model example automatic relevance determination section specifically define independent prior column represent vector principal subspace independent variance precision section column resulting model directed graph shown figure value found iteratively hood function result optimization driven infinity corresponding parameter driven zero posterior distribution becomes delta function origin giving sparse solution effective dimensionality principal subspace determined number finite value correspond vector thought relevant data distribution approach automatically making improving data number vector eigenvalue tuned data reducing complexity model vector origin sparsity context relevance vector machine value training marginal likelihood given given note simplicity also treat parameter rather prior parameter probabilistic integration intractable make assume posterior distribution sharply peaked occur sufficiently large data set equation marginal likelihood respect take simple form dimensionality estimation algorithm update wand equation given similarly step equation given change equation give value given sample mean choose value finite model infinity model equivalent isotropic model encompass value effective dimensionality principal subspace also possible consider smaller value save computational cost limit maximum dimensionality subspace comparison result algorithm standard probabilistic shown figure opportunity illustrate sampling section figure show example sample data dimension latent space data probabilistic model direction high variance direction variance noise result show clearly presence three distinct mode posterior distribution step iteration small value large value three latent variable suppressed course sampling solution make sharp transition three mode model prior matrix fully treatment prior variational method bishop discussion vari approach appropriate dimensionality model
['factor', 'analysis'] factor analysis factor analysis latent variable model closely related probabilistic definition probabilistic conditional distribution variable given latent variable figure diagram matrix element matrix square white positive black negative value whose area proportional magnitude element synthetic data data point dimension distribution standard deviation direction standard deviation direction data dimension direction variance direction show result maximum likelihood probabilistic plot show corresponding model able discover appropriate dimensionality surplus degree freedom taken diagonal rather isotropic covariance diagonal matrix note factor analysis model common probabilistic variable dent given latent variable essence factor analysis model explaining covariance structure data independent vari associated matrix covariance variable matrix factor analysis literature column capture correlation variable loading diagonal element represent independent noise variance variable origin factor analysis discussion factor analysis found book link factor analysis stationary point likelihood function analysis model column scaled sample covariance matrix average eigenvalue later tipping bishop maximum likelihood function ware chosen principal making marginal distribution plot versus number three value showing moot posterior distribution probabilistic latent pace factor place space problematic analysis pace shall factor analysis form model form pace interest particular used wish remove degeneracy latent pace latent rise independent component model parameter model likelihood solution given longer maximum likelihood solution found latent variable model used silo matrix hose exercise compute step convenient often similarly equation take form operator set nondiagonal element matrix zero treatment factor analysis model straightforward application technique book another difference probabilistic factor analysis concern exercise different behaviour transformation data rotate system data space obtain exactly data matrix corresponding rotation matrix however factor analysis analogous property make data vector absorbed corresponding element kernel chapter technique kernel substitution take algorithm expressed term scalar product form generalize algorithm scalar product nonlinear kernel apply technique kernel substitution principal component analysis thereby nonlinear generalization kernel consider data observation space dimensionality order keep notation uncluttered shall assume already sample mean vector first step express conventional form data vector appear form scalar product recall principal component defined covariance matrix sample covariance matrix defined consider nonlinear transformation dimensional feature space data point thereby onto point lion kernel space plot space fight plot blue hole model lobo mean lined goal problem work pace tilt substituting expansion back equation obtain step express term kernel function multiplying side give written matrix notation column vector element find solution following eigenvalue problem exercise removed factor side note solution differ zero eigenvalue affect principal component projection normalization condition coefficient feature space problem resulting principal component also cast term kernel function projection point onto given expressed term kernel function original space orthogonal hence find linear principal component dimensionality feature space however much even infinite thus find number nonlinear principal component exceed note however number nonzero eigenvalue exceed number data point even covariance matrix feature space rank equal reflected fact kernel expansion matrix assumed data given zero mean general case simply compute subtract mean since wish avoid working directly feature space formulate algorithm purely kernel function data point given corresponding element gram matrix given expressed matrix notation exercise matrix every element take value thus evaluate kernel function determine eigenvalue note standard algorithm special case linear kernel figure show example kernel applied synthetic data kernel form applied synthetic data line correspond contour along projection onto corresponding principal component defined constant figure kernel kernel synthetic lata showing flight along line along principal duster half along direction orthogonal finding tor large data often finally linear often retain data projection principal subspace defined ping map space space poil however feature linear pace typically nonlinear dimensional manifold finding preimage
['nonlinear', 'latent', 'variable', 'models'] nonlinear latent variable model exercise chapter class model continuous latent variable namely based distribution well great practical importance model relatively easy analyse data also used component complex model consider briefly generalization framework model either nonlinear fact issue related general probability density simple fixed reference density making nonlinear change variable idea form basis several practical latent variable model shall shortly
['independent', 'component', 'analysis'] independent component analysis begin considering model variable related linearly latent variable latent distribution important class model known independent component analysis consider distribution latent variable understand role model consider situation people talking time record voice microphone ignore effect time delay echo signal received microphone point time given linear combination amplitude voice coefficient linear combination constant infer value sample data invert process assuming nonsingular thereby obtain clean signal voice person example problem blind source separation blind fact given mixed data neither original source coefficient type problem sometimes following approach ignore temporal nature signal treat successive sample consider generative model latent variable corresponding unobserved speech signal amplitude variable given signal value microphone latent variable joint distribution variable given linear combination latent variable need include noise distribution number latent variable equal number variable therefore marginal distribution variable general singular variable simply deterministic linear combination latent variable given data observation exercise likelihood function model function coefficient linear bination likelihood optimization giving rise particular version independent component analysis success approach latent variable distribution recall probabilistic factor analysis distribution given isotropic model therefore distinguish different choice latent variable differ simply rotation latent space directly marginal density hence likelihood function unchanged make transformation orthogonal matrix satisfying matrix given invariant extending model allow general latent distribution change conclusion seen model equivalent isotropic latent variable model another latent variable distribution linear model insufficient find independent component note principal compo represent rotation system data space diagonal covariance matrix data distribution uncorrelated although zero correlation necessary condition independence however sufficient practice common choice distribution given heavy tail reflecting observation many distribution also exhibit property original model bell based objective function defined information maximization advantage probabilistic latent variable formulation help motivate late generalization basic instance independent factor analysis model number latent variable differ variable noisy individual latent variable flex distribution mixture likelihood model reconstruction latent variable variational approach many type model considered huge literature application stone
['autoassociative', 'neural', 'networks'] neural network chapter considered neural network context learn role network predict output variable given value figure layer weight network trained input vector onto error even linear unit hidden layer network equivalent linear principal component anal link bias parameter clarity input output input variable however neural network also applied learning used dimensionality reduction network number output input weight minimize measure reconstruction error input output respect training data consider first form shown figure input output unit hidden unit target used train network simply input vector network input vector onto network said form since number hidden unit smaller number input perfect reconstruction input vector general therefore determine network parameter error function capture degree mismatch input vector reconstruction particular shall choose error form hidden unit linear activation function shown error function unique global minimum minimum network projection onto dimensional subspace first principal component data thus vector weight lead hidden unit figure form basis span principal subspace note however tor need orthogonal result unsurprising since principal component analysis neural network linear dimensionality reduction error function might thought limitation linear dimensionality reduction could overcome nonlinear sigmoidal activation function hidden unit network figure however even nonlinear hidden unit error solution given projection onto principal component subspace therefore advantage layer neural network perform dimensionality reduction standard technique principal component analysis based singular value decomposition give correct solution finite time also generate ordered eigenvalue corresponding figure addition extra hidden unit give network perform reduction input output situation different however additional hidden layer network consider network shown figure output unit linear unit second hidden layer also linear however first third hidden layer sigmoidal nonlinear function network trained minimization error function view network successive functional figure first project original dimensional data onto subspace defined activation unit second hidden layer presence first hidden layer nonlinear unit general particular restricted linear similarly second half network arbitrary functional dimensional space back original input space simple geometrical interpretation case figure network effectively nonlinear principal component analysis figure geometrical interpretation network figure case input unit middle hidden layer function map space space therefore space within original since nonplanar figure point original space dimensional subspace advantage limited linear transformation although standard principal component analysis special case however training network nonlinear optimization problem since error function longer quadratic function network parameter computation ally intensive nonlinear optimization technique must used risk finding suboptimal local minimum error function also dimensionality subspace must training network
['modelling', 'nonlinear', 'manifolds'] nonlinear manifold already noted many natural source data correspond dimensional possibly noisy nonlinear manifold within higher data space property explicitly lead proved density general method consider briefly range technique attempt model nonlinear structure combination linear model make piecewise linear approximation manifold instance clustering technique mean based distance partition data local group standard plied group better approach reconstruction error cluster assignment common cost function stage however approach still suffer limitation absence overall density model prob straightforward define fully probabilistic model simply considering mixture distribution component probabilistic model tipping bishop model discrete latent vari corresponding discrete mixture well continuous latent variable likelihood function algorithm fully treatment based variational inference bishop number component mixture well effective dimensionality individual model data many variant model parameter matrix noise variance tied across component mixture isotropic noise distribution diagonal one giving rise mixture factor analyser beal mixture probabilistic model also extended hierarchically produce interactive data algorithm bishop tipping alternative considering mixture linear model consider single nonlinear model recall conventional find linear subspace pass close data sense concept extended dimensional nonlinear surface form principal curve describe curve data space function vector whose element function scalar many possible way curve natural choice length along curve given point data space find point curve distance denote point particular curve continuous data density principal curve defined every point curve mean point data space project given continuous density many principal curve practice interested finite data set also wish restrict attention smooth curve propose iterative procedure find principal curve somewhat reminiscent algorithm curve first principal component algorithm alter nates data projection step curve step projection step data point assigned value corresponding point curve step point curve given weighted average point project nearby point curve point curve given weight case subspace constrained linear procedure first principal component equivalent power method finding variance matrix principal curve generalized multidimensional manifold principal surface although found limited difficulty data smoothing higher dimension even manifold often used project data onto space ample dimensional purpose visualization another linear technique similar multidimensional scaling find projection data preserve closely possible pairwise distance data point finding distance matrix case distance give equivalent result concept extended wide variety data type term similarity matrix giving nonmetric method dimensionality reduction data worthy mention locally linear first coefficient best data point coefficient invariant rota translation scaling data point hence characterize local geometrical property map data point lower dimensional space coefficient local particular data point considered linear transformation combination translation rotation scaling preserve angle formed data point weight variant transformation expect weight value reconstruct data point space data space spite optimization exhibit local minimum isometric feature goal project data space defined term geodesic distance measured along mani nonlinear latent variable model fold instance point circle geodesic distance measured around circumference circle straight line measured along chord algorithm first data point either finding nearest finding point within sphere radius graph link point distance geodesic distance pair point length along path found standard algorithm finally metric applied geodesic distance matrix find projection focus chapter model vari continuous also consider model continuous latent vari together discrete variable giving rise latent trait model case continuous latent variable even linear relationship latent variable analytically sophisticated technique tipping us variational inference model latent space binary data analogously visualize continuous data note model dual logistic regression problem section case logistic regression observation feature vector single parameter vector whereas latent space visualization model single latent space variable analogous copy latent variable generalization probabilistic latent variable model general exponential family distribution collins already noted arbitrary distribution formed taking random variable transforming suitable general latent variable model density network nonlinear function neural network network enough hidden unit given nonlinear function desired accuracy downside flexible model latent variable order obtain likelihood function longer analytically tractable instead likelihood monte technique drawing sample prior latent variable becomes simple term sample however large number sample point order give accurate representation marginal procedure costly consider restricted form nonlinear function make choice latent variable distribution construct latent vari able model nonlinear efficient train generative topographic bishop bishop bishop us latent distribution defined finite regular grid delta function typically latent space latent space simply contribution grid location lata left tile model lata tile mean tent tile group data point gien linear regression model allow general linear tile tilt usual limitation linear regression model arise generall sion dimensionality data space likelihood analytically form efficiently algorithm model nonlinear manifold tile posterior latent space data back purpose figure comparison data tilt seen probabilistic also regular array point somewhat algorithm data point nearby initially random training process organize manifold unlike mean cost function making difficult parameter model ass also guarantee take place choice appropriate particular data optimize likelihood model define pace component mean foundation also make straightforward define generalization bishop treatment dealing missing section extension discrete variable process define manifold hierarchical model manifold defined continuous surface prototype vector possible compute magnification factor corresponding local expansion compression manifold data bishop well directional curvature manifold along data provide additional insight model exercise exercise proof induction show linear projection onto dimensional subspace variance data defined data covariance matrix given corresponding eigenvalue section result proven case suppose result hold general value show consequently hold dimensionality first derivative variance data respect vector direction data space equal zero done subject constraint orthogonal vector also unit length appendix enforce constraint make property vector show vector finally show variance chosen corresponding eigenvalue ordered decreasing value show minimum value distortion measure given respect subject constraint data covariance matrix introduce matrix multiplier constraint distortion measure matrix notation read whose column gien minimize respect show clearly possible solution column case diagonal matrix corresponding eigenvalue obtain general solution show assumed expansion show general solution give value specific solution column solution equivalent convenient choose solution verify defined unit length assuming unit length suppose replace latent space probabilistic model general distribution parameter model show lead identical model marginal distribution variable valid choice random variable distribution given consider random variable given matrix show also distribution find expression mean covariance discus form distribution draw directed probabilistic graph probabilistic model section component variable shown explicitly separate node hence verify probabilistic model independence structure naive model making result mean covariance general distribution derive result marginal distribution probabilistic model making result show posterior distribution probabilistic model given verify likelihood probabilistic model respect parameter give result mean data vector second derivative likelihood function probabilistic model respect parameter show stationary point unique maximum show limit posterior mean probabilistic model becomes orthogonal projection onto principal subspace conventional show posterior mean probabilistic model towards origin relative orthogonal projection show optimal reconstruction data point probabilistic according least square projection cost conventional given number independent parameter covariance matrix model dimensional latent space data space given verify case number independent parameter general covariance whereas isotropic covariance derive equation probabilistic model maximization likelihood function given figure application probabilistic data data value missing random derive algorithm likelihood function probabilistic model note well missing data value component vector latent variable show special case data value algorithm probabilistic derived section matrix whose column define linear subspace dimensionality within data space dimensionality vector given data approximate data point linear dimensional vector associated square reconstruction cost given first show respect lead analogous expression variable respectively denote sample mean show respect kept fixed give rise respect kept fixed give rise step derive expression number independent parameter factor analysis model section show factor analysis model section invariant rotation latent space considering second derivative show stationary point likelihood function factor analysis model section respect parameter given sample mean defined furthermore show stationary point maximum derive formula step algorithm factor analysis note result exercise parameter sample mean write expression likelihood factor analysis model hence derive corresponding step draw directed probabilistic graphical model discrete mixture probabilistic model model value draw graph parameter value component mixture section student infinite mixture marginalize respect latent variable representation formulate algorithm likelihood function student given data point derive form step consider model latent space distribution conditional distribution vari able arbitrary symmetric positive definite noise covariance matrix suppose make nonsingular linear transformation data variable matrix represent maximum likelihood solution corresponding original untransformed data show resent corresponding maximum likelihood solution data finally show form model case diagonal matrix diagonal matrix case factor analysis remains diagonal hence factor analysis covariant data variable orthogonal portional unit matrix probabilistic matrix remains proportional unit matrix hence covariant rotation ax data space case conventional show vector also satisfy also show solution eigenvalue multiple zero eigenvalue obtain solution also eigenvalue finally show modification affect projection given show conventional linear algorithm special case kernel choose linear kernel function given transformation property probability density change variable show density fixed density everywhere nonzero making nonlinear change variable monotonic function write differential equation satisfied draw diagram transformation density suppose variable independent show covariance matrix variable diagonal show independence sufficient condition variable correlated consider variable write conditional distribution observe dependent showing variable independent show covariance matrix variable diagonal relation show term zero show zero correlation sufficient condition independence data book primarily set data point independent identically distributed assumption express likelihood function product data point prob ability distribution data point many application however assumption poor consider particularly important class data set namely describe sequential data often arise measurement time series example rainfall measurement day particular location daily value currency exchange rate acoustic feature successive time frame used speech recognition example speech data shown figure sequential data also arise context time series example sequence nucleotide base pair along strand sequence character sentence convenience shall sometimes refer past future observation sequence however model chapter equally applicable figure example gram spoken word showing plot intensity spectral versus time index form sequential data temporal sequence useful distinguish stationary nonstationary sequential stationary case data time distribution remains complex nonstationary generative distribution time shall focus stationary case many application forecasting wish able next value time series given observation previous value expect recent observation likely informative historical observation future value example figure show successive observation speech spectrum indeed highly related furthermore would impractical consider general dependence future observation previous observation complexity model would grow without limit number observation increase lead consider model assume future prediction inde figure approach sequence treat independent correspond graph without link pendent recent observation although model tractable also severely limited tain general framework still retaining tractability introduction latent variable leading state space model chapter shall complex model thereby simpler component particular distribution belonging exponential family read framework probabilistic graphical model focus important example state space model namely model latent variable discrete linear dynamical system latent variable model graph tree structure loop inference algorithm
['markov', 'models'] model easiest treat sequential data would simply ignore sequential aspect treat observation corresponding graph figure approach however would fail exploit sequential pattern data correlation observation close sequence suppose instance observe binary variable whether particular given time series recent observation variable wish predict whether rain next treat data information glean data relative frequency rainy day however know practice weather often exhibit trend last several day observing whether rain today therefore cant help rain tomorrow express effect probabilistic model need relax sumption way consider model first note without loss generality product rule express joint distribution sequence observation form assume conditional distribution side independent previous observation except recent obtain chain graphical model figure figure chain observation conditioned value previous servation joint distribution sequence observation model given property conditional distribution section given observation time given easily veri direct evaluation starting prod rule probability thus model predict next observation exercise sequence distribution prediction depend value mediately preceding observation independent observation application model conditional distribution model constrained equal corresponding stationary time series model known homogeneous chain instance conditional distribution depend adjustable parameter whose value might training data distribution chain share value parameter although general independence model still many sequential observation anticipate trend data several successive observation provide important information predict next value allow observation move chain allow prediction depend also value obtain chain graph figure joint distribution given direct evaluation conditional given independent observation figure chain conditional distribution particular observation value previous observation figure represent data chain latent variable observation state corresponding latent variable important graphical structure form foundation hidden model linear system observation previous observation similarly consider extension order chain conditional particular variable previous variable however price number parameter model much suppose observation discrete variable state conditional distribution chain parameter state giving total parameter suppose extend model order chain joint distribution built variable discrete conditional general conditional probability table number parameter model parameter exponentially often render approach impractical value continuous variable conditional distribution node distribution whose mean linear function parent known model alternative approach parametric model neural network technique sometimes delay line delaying previous value variable order predict next value number parameter much smaller completely general model ample grow linearly although expense restricted family conditional distribution suppose wish build model sequence limited assumption order limited number free parameter achieve additional latent variable permit rich class model simple component mixture distribution chapter continuous latent variable model chapter observation introduce corresponding latent variable different type dimensionality variable assume latent variable form chain giving rise graphical structure known state space model shown figure conditional independence property dent given joint distribution model given criterion always path variable latent variable path never blocked thus predictive distribution observation given previous observation exhibit conditional independence prop prediction previous observation variable however satisfy property order shall discus evaluate predictive distribution later section chap important model sequential data graph latent variable discrete obtain hidden model note variable section discrete continuous variety different conditional distribution used model latent variable dependence conditional distribution parent obtain linear dynamical system section
['hidden', 'markov', 'models'] hidden model hidden model instance state space model figure latent variable discrete however examine single time slice model mixture distribution component density given therefore also extension mixture model choice mixture ponent observation selected independently choice component previous observation widely used speech recognition natural language manning handwriting recognition analysis biological sequence protein case standard mixture model latent variable discrete multinomial variable component mixture responsible generating corresponding observation convenient scheme used mixture model chapter allow probability distribution depend state previous latent variable conditional distribution latent variable binary variable conditional distribution table number denote element known transition probability given probability satisfy matrix figure transition diagram showing model whose tent variable three possible state three box black line denote element transition matrix independent parameter write conditional distribution explicitly form initial latent node special parent node marginal distribution vector probability element transition matrix sometimes drawing state node state transition diagram shown figure case note represent probabilistic graphical model node separate variable rather state single variable shown state box rather circle sometimes useful take state transition diagram kind shown figure unfold time give alternative representation transition latent state known lattice trellis diagram section shown case hidden model figure cation probabilistic model distribution variable governing distribution known emission probability might example given form element continuous variable conditional probability table discrete distribution given value vector number corresponding possible state binary vector figure unfold state transition gram figure time obtain lattice trellis representation latent state column diagram latent variable represent emission probability form shall focus attention homogeneous model distribution governing latent variable share parameter similarly emission distribution share parameter extension general case straightforward note mixture model data special case parameter value conditional distribution independent horizontal link graphical model shown figure joint probability distribution latent variable given parameter governing model discussion hidden model independent particular choice emission probability indeed model tractable wide range emission distribution discrete table mixture also possible exploit discriminative model neural network used model exercise emission density directly provide representation converted emission density theorem bishop gain better understanding hidden model considering generative point view recall generate sample mixture figure illustration sampling hidden model state latent variable emission model dimensional contour constant probability density emission distribution corresponding three state latent variable sample point drawn hidden model colour according component line successive observation transition matrix state probability making transition state consequently probability state chose component random probability given generate sample vector correspond component process repeated time generate data independent sample case hidden model procedure choose initial latent variable probability parameter sample corresponding observation choose state variable according transition probability already value thus suppose sample state choose state probability know draw sample also sample next latent variable example ancestral sampling directed graphical model instance model section transition element much element typical data sequence long run point single compo infrequent transition component another generation sample hidden model figure many variant standard model instance imposing constraint form transition matrix mention particular practical importance setting element zero figure example state transition diagram state hidden model note state later state transition diagram state figure typically model initial state probability word every sequence constrained start state transition matrix constrained ensure large change state index occur type model lattice diagram figure many application hidden model example speech recognition character recognition make architecture hidden model consider example digit us data meaning digit trajectory function time form sequence contrast digit data appendix static image example line digit shown figure train hidden model subset data example digit state generate line segment length possible angle emission distribution simply table probability associated angle value state index value transition prob ability zero except keep state index increment model parameter iteration gain insight resulting model running generatively shown figure figure lattice diagram state left state index increase transition figure example digit bottom synthetic digit pled generatively model trained data digit powerful property hidden model ability exhibit degree invariance local warping compression time axis understand consider digit written digit example typical digit distinct section cusp part digit start left sweeping cusp loop bottom left second straight sweep ending bottom right natural variation writing style cause relative size section vary hence location cusp loop within temporal sequence vary generative perspective variation hidden model change number transition state versus number transition successive state note however digit written reverse order starting bottom right ending left even though identical example training probability observation model extremely small speech recognition context warping time axis associated natural variation speed speech hidden model accommodate distortion penalize heavily
['maximum', 'likelihood', 'hmm'] maximum likelihood data determine param maximum likelihood likelihood function joint distribution latent variable joint distribution factorize contrast mixture distribution considered chapter simply treat summation independently perform summation explicitly variable summed state total term thus number term summation exponentially length chain fact summation exponentially many path lattice diagram figure already similar considered infer problem simple chain variable figure able make conditional independence property graph reorder summation order obtain algorithm whose cost scale linearly instead exponentially length chain shall apply similar technique hidden model expression likelihood function generalization mixture distribution summation emission model different setting latent variable direct maximization likelihood function therefore lead complex solution case simple mixture model section recall mixture model data special case therefore turn expectation maximization algorithm framework likelihood function hidden model algorithm start initial selection model parameter denote step take parameter value posterior distribution latent variable posterior evaluate expectation logarithm likelihood function function parameter give function point convenient introduce notation shall denote marginal posterior distribution latent variable denote joint posterior distribution successive latent variable value store number unity similarly store matrix number unity shall also denote conditional probability similar notation probabilistic variable later expectation binary random variable probability take value substitute joint distribution given make obtain goal step evaluate quantity shall discus detail shortly step maximize respect parameter treat constant maximization respect easily appropriate multiplier result exercise algorithm must choosing starting value course respect summation constraint associated interpretation note element zero initially remain zero subsequent update typical procedure would exercise involve random starting value parameter subject constraint note particular cation result case model beyond choosing initial value element appropriate element zero remain zero throughout maximize respect notice term furthermore term exactly form term corresponding function standard mixture data seen comparison case mixture quantity role parameter independent different component term term value independently simply weighted hood function emission density weight shall suppose maximization done instance case emission density maximization function give case discrete multinomial variable conditional distribution observation take form corresponding equation given exercise analogous result hold variable algorithm initial value parameter emission treat data initially emission density maximum likelihood resulting value parameter
['forward', 'backward', 'algorithm'] algorithm next seek procedure quantity corresponding step algorithm graph hidden model shown figure tree know posterior distribution latent variable stage message passing algorithm particular context hidden section model known algorithm algorithm fact several variant basic algorithm lead exact according precise form message along chain jordan shall focus widely used known algorithm well great practical importance right forward backward algorithm nice illustration many concept chapter shall therefore begin section derivation equation making product rule probability conditional independence property shall obtain corresponding graphical model section shall algorithm simply example algorithm section worth evaluation posterior distribution latent variable independent form emission density indeed whether variable continuous discrete require value quantity value every also section next shall omit explicit dependence model parameter throughout therefore begin writing following conditional independence property jordan relation easily proved instance result note every path node node pass node path conditional independence property must hold reader take moment verify property turn exercise application relation also proved directly though greater effort joint distribution hidden model product rule exercise begin recall discrete multinomial variable value component probability component value thus interested posterior distribution given data vector length whose entry correspond value theorem note denominator implicitly conditioned parameter hence likelihood function conditional independence property together product rule probability obtain quantity joint probability observing given data time value whereas conditional probability future data time given value represent number possible setting binary vector shall notation denote value analogous interpretation derive recursion relation allow shall make conditional independence property particular together product rule express term making obtain figure illustration forward recursion evaluation variable fragment lattice quantity taking element step weight given corresponding multiplying data contribution worth taking moment study recursion relation detail note term summation side value step recursion computational cost scaled like forward recursion equation lattice diagram figure order start recursion need initial condition given tell take value starting node chain work along chain evaluate every latent node step recursion multiplying matrix overall cost quantity whole chain similarly recursion relation quantity making conditional independence property giving figure illustration backward recursion evaluation variable fragment lattice quantity taking component step weight given product correspond value value emission density making obtain note case backward message passing algorithm term step absorb effect observation emission probability multiply transition matrix marginalize figure need starting condition recursion namely value setting give correct provided take setting step equation quantity cancel seen instance equation given take form however quantity likelihood function whose value wish monitor optimization useful able evaluate side fact side distribution obtain thus evaluate likelihood function choice instance want evaluate likelihood function running recursion start chain result making fact vector case recursion simply take moment interpret result recall compute likelihood take joint distribution possible value value particular choice hidden state every time step word every term summation path lattice diagram recall exponentially many path likelihood function form reduced computational cost exponential length chain linear swapping order summation multiplication time step contribution path passing state give intermediate quantity next consider evaluation quantity correspond value conditional probability setting theorem made conditional independence property together given thus calculate directly result recursion summarize step train hidden model algorithm make initial selection parameter parameter often either uniformly randomly uniform distribution respecting constraint parameter depend form distribution instance case parameter might algorithm data might covariance matrix corresponding mean cluster forward recursion backward recursion result evaluate stage also evaluate likelihood function step result parameter equation section continue alternate step convergence criterion instance change likelihood function threshold note recursion relation observation enter conditional distribution form recursion therefore independent type dimensionality variable form conditional distribution long value possible state since variable quantity function start algorithm remain throughout seen chapter maximum likelihood approach effective number data point large relation number note hidden model trained effectively likelihood provided training sequence long alternatively make multiple shorter sequence straightforward cation hidden model algorithm case exercise model particularly important given observation sequence given state transition corresponding nondiagonal element seen another quantity interest predictive distribution data wish predict would important application forecasting make product rule together conditional independence property giving running forward recursion summation result summation used value order recursion forward next step order predict subsequent value figure fragment graph representation hidden model note data value thus predictive distribution carried forward inde amount storage application estimation parameter likelihood framework easily extended maximum hood prior model parameter whose value posterior probability done algorithm step step prior distribution function maximization straightforward application technique various point book furthermore variational give fully treatment marginalize section parameter distribution maximum likelihood lead recursion compute posterior probability
['sum', 'product', 'algorithm', 'hmm'] algorithm directed graph hidden model shown tree solve problem local hidden variable algorithm surprisingly turn section equivalent algorithm considered previous section algorithm therefore simple derive recursion formula begin transforming directed graph figure factor graph representative fragment shown figure form graph show variable latent explicitly however purpose inference problem shall always variable simplify factor graph absorbing emission probability transition probability factor lead factor graph representation figure factor given figure form graph describe hidden model derive algorithm denote hidden variable root node message leaf node root general result message propagation message hidden model take form equation represent propagation message forward along chain equivalent alpha recursion derived previous section shall show note variable node perform computation eliminate give recur sion message form recall obtain alpha recursion given also need verify quantity equivalent previously easily done initial condition given identical initial iteratively equation subsequent quantity must next consider message root node back leaf node take form message type since variable node perform computation substitute obtain beta recursion given verify beta variable equivalent initial sage send root variable node identical given section algorithm also evaluate message particular result show local marginal node given product incoming message conditioned variable joint distribution dividing side obtain agreement result similarly derived exercise
['scaling', 'factors'] scaling factor important issue must make forward backward algorithm practice recursion relation note step value previous value multiplying quantity probability often unity work forward along chain value zero exponentially quickly moderate length chain calculation soon exceed dynamic range computer even double precision point used case data implicitly problem likelihood function taking logarithm unfortunately help forming sum product small number fact possible path lattice diagram figure therefore work version whose value remain order unity shall corresponding scaling factor cancel quantity algorithm joint observation latent variable version given expect well numerically probability variable value order relate scaled original variable introduce scaling factor conditional distribution variable product rule turn recursion equation given note stage forward message passing phase used evaluate evaluate store easily done side give similarly variable remain within machine precision quan simply ratio conditional probability recursion result give following recursion variable recursion relation make scaling factor previously phase likelihood function found similarly together given exercise finally note alternative formulation algorithm jordan backward recursion based quantity instead recursion forward quantity available backward whereas forward backward pass algorithm done independently although algorithm comparable computational cost version commonly case hidden model whereas linear dynamical system section recursion analogous form usual
['viterbi', 'algorithm'] algorithm many application hidden model latent variable meaningful interpretation often interest probable sequence hidden state given observation sequence instance speech recognition might wish probable phoneme sequence given series acoustic observation graph hidden model directed tree problem exactly algorithm recall discussion section problem probable sequence latent state state individually probable latter problem running algorithm latent variable individually however state general correspond probable sequence state fact state might even represent sequence zero probability successive state isolation individually probable transition matrix element zero practice usually interested probable sequence state algorithm context hidden model known algorithm note algorithm work probability need variable done algorithm figure show fragment hidden model expanded lattice diagram already noted number possible path lattice exponentially length chain algorithm search space path probable path computational cost linearly length chain algorithm represent hidden model factor graph shown figure treat variable node root message root starting leaf node result message algorithm given figure fragment lattice showing possible path algorithm probable path amongst exponentially many possibility given path corresponding probability given product element matrix corresponding segment path along emission density node path eliminate equation make obtain recursion message form notation message used note keep notation uncluttered omit dependence model parameter probable sequence algorithm also derived directly joint distribution taking logarithm maximization summation easily seen quantity probabilistic exercise interpretation maximization obtain value joint distribution corresponding probable path also wish sequence latent variable value path simply make procedure note maximization must possible value suppose keep record value correspond maximum value value denote function sage chain found probable state function backtrack along chain intuitively understand algorithm naively could consider explicitly exponentially many path lattice evaluate probability select path highest however notice make dramatic saving computational cost suppose path evaluate probability product transition emission probability work forward along path lattice consider particular time step particular state time step many possible path converging correspond node lattice diagram however need retain particular path highest probability state time step need keep track path time step possible path consider possible path leading current state need retain corresponding best path state time reach time step discover state overall probable path unique path coming state trace path back step state time back lattice state
['extensions', 'hidden', 'markov', 'model'] extension hidden model basic hidden model along standard training algorithm based maximum likelihood extended numerous way meet requirement particular application discus important example digit example figure hidden model quite poor generative model data many synthetic digit look quite unrepresentative training data goal sequence cant bene parameter hidden model discriminative rather maximum likelihood technique suppose training observation sequence according class class separate hidden model parameter treat problem parameter value standard cation problem optimize theorem expressed term sequence probability associated hidden model prior probability class optimization cost function complex maximum likelihood particular figure section hidden model distribution observation subset previous observation well hidden state example distribution previous observation every training sequence model compute denominator hidden model coupled discriminative training method widely used speech recognition cant weakness hidden model distribution time system remains given state problem note probability sequence given hidden model spend precisely step state make transition different state given exponentially function many application unrealistic model state duration problem resolved state duration directly diagonal zero state explicitly associated probability distribution duration time generative point view state value number time step system remain state drawn model value variable generally assumed independent corresponding sion density simply approach straightforward cation optimization procedure another limitation standard poor long range correlation variable variable many time step must chain hidden state effect could principle included extra link graphical model figure address generalize give hidden model example shown figure discrete expanded table conditional probability sion distribution case emission density linear framework conditional distribution given value previous observation value whose mean linear combination value variable clearly number additional link graph must limited avoid excessive number free parameter example shown figure observation figure example hidden model case emission probability transition probability depend value sequence observation preceding variable well hidden state although graph look messy appeal fact still simple probabilistic structure particular imagine standard value independent corresponding conditional independence property easily veri every path node node pass least node respect path consequence recursion step algorithm determine posterior distribution latent variable computational time linear length chain similarly step minor cation standard equation case emission density parameter standard linear regression equation chapter seen natural extension standard graphical model fact probabilistic graphical viewpoint plethora different graphical structure based another example hidden model sequence variable addition output variable whose value either latent variable output variable example shown figure framework domain learn sequential data easy show criterion property chain latent variable still hold verify simply note path node node respect node conditional inde property formulation learn algorithm particular determine parameter model likelihood function matrix whose row given consequence conditional independence property likelihood function algorithm step forward backward recursion exercise another variant worthy mention factorial hidden model jordan multiple independent figure factorial hidden model chain latent vari continuous variable possible choice emission model density mean linear nation state corresponding latent variable chain latent variable distribution variable given time step conditional state corresponding latent vari time step figure show corresponding graphical model motivation considering factorial seen order represent bit information given time step standard would need latent state whereas factorial could make binary latent chain primary disadvantage factorial however lie additional complexity training step factorial model straightforward however observation variable dependency latent chain leading step seen figure variable connected path node hence exact step model correspond running forward backward recursion along chain independently conditional independence property individual chain factorial model shown figure suppose chain hidden node simplicity suppose latent variable number state approach would note combination latent variable given time step figure example path green node unobserved node thus path blocked conditional independence property hold individual tent chain factorial model consequence exact step model transform model equivalent standard single chain latent variable latent state standard recursion step computational exponential number latent chain intractable anything small value solution would sampling method chapter elegant deterministic jordan variational inference technique section obtain tractable algorithm approximate inference done simple variational posterior distribution fully respect latent variable alternatively powerful approach variational distribution independent chain corresponding chain latent variable original model latter case variational inference algorithm running independent forward backward recursion along chain also able capture correlation variable within chain clearly many possible probabilistic structure according need particular application graphical model provide general technique structure variational method provide powerful framework inference model exact solution intractable
['linear', 'dynamical', 'systems'] linear dynamical system order motivate concept linear dynamical system consider following simple problem often practical setting suppose wish measure value unknown quantity noisy sensor return observation value plus noise given single measurement best guess assume however improve estimate taking lot measurement random noise term tend cancel let make situation complicated assuming wish measure quantity time take regular measurement point time wish corresponding value simply average measurement error random noise reduced unfortunately obtain single estimate value thereby source error intuitively could imagine better estimate value take recent measurement average slowly random noise level sensor high would make sense choose relatively long window observation average conversely signal quickly noise level small might better directly estimate perhaps could even better take weighted average recent measurement make greater contribution recent one although sort intuitive argument plausible tell form weighted average sort weighing hardly likely optimal fortunately address problem much probabilistic model capture time evolution measurement process inference learning method chapter shall focus widely used model known linear dynamical system seen state space model shown figure latent variable discrete arbitrary emission probability distribution graph course much class probability distribution factorize according consider extension distribution latent variable particular consider continuous latent variable summation algorithm become integral general form inference algorithm however hidden model interesting note historically hidden model linear dynamical system independently expressed graphical model however deep relationship immediately becomes apparent requirement retain algorithm inference linear length chain instance take quantity posterior probability given observation multiply transition probability emission probability marginalize obtain distribution functional form distribution must become complex stage must change parameter value surprisingly distribution property closed multiplication belonging exponential family consider important example practical perspective particular consider state space model latent variable well variable variate distribution whose mean linear function state parent graph seen directed graph unit equivalent joint distribution variable furthermore also functional form sage obtain inference algorithm contrast suppose emission density comprise mixture mean linear even quantity mixture mixture exact inference practical value seen hidden model extension mixture model chapter allow sequential correlation data similar view linear dynamical system generalization continuous latent variable model chapter probabilistic factor analysis pair node latent variable model particular observation however latent variable longer independent form chain model directed graph inference problem algorithm forward analogous message hidden model known equation back ward recursion analogous message known smoother equation equation widely used many application linear dynamical system model joint variable well sequence individually probable latent variable value probable latent sequence thus need consider exercise analogue algorithm linear dynamical system model conditional distribution write transition emission distribution general form initial latent variable also distribution write note order simplify notation additive constant term mean fact straightforward include desired traditionally distribution commonly expressed exercise form term noisy linear equation given noise term distribution parameter model determined maximum likelihood algorithm step need solve inference problem local posterior latent variable algorithm discus next section
['inference', 'lds'] inference turn problem marginal distribution latent variable conditional observation sequence given parameter setting also wish make prediction next latent state next observation conditioned data application inference problem algorithm context linear dynamical system give rise smoother equation worth linear dynamical system linear model joint distribution latent variable simply principle could solve inference problem standard result derived previous chapter role algorithm provide perform computation linear dynamical system identical factorization given hidden model factor graph figure inference algorithm therefore take precisely form except summation latent variable integration begin forward equation treat root node propagate message leaf node root initial message factor subsequent message also convention shall propagate message marginal distribution corresponding denote precisely analogous propagation scaled variable given discrete case hidden model recursion take form substituting respectively making becomes supposing known wish determine value integral easily making result combine result factor side making give made matrix inverse identity also gain matrix thus given value together observation evaluate marginal mean covariance well normalization initial condition recursion equation given given make calculate calculate giving similarly likelihood function linear dynamical system given factor found equation interpret step involved going posterior marginal posterior marginal view quantity prediction mean simply taking mean projecting forward step transition probability matrix mean would give observation given emission probability matrix hidden state mean view update equation mean hidden variable distribution taking mean correction proportional error observation actual observation correction given gain matrix thus view process making successive prediction correcting prediction light observation graphically figure figure linear dynamical system sequence step increasing certainty state variable diffusion arrival data plot blue curve show distribution data step diffusion nonzero variance transition probability give distribution shown plot note relative blue curve shown dashed plot comparison next data observation emission density shown function green plot note density respect inclusion data point lead distribution state density shown blue observation data distribution shown dashed plot comparison consider situation measurement noise small rate latent variable posterior distribution current measurement accordance exercise intuition simple example start section similarly latent variable slowly relative observation noise level posterior mean measurement time exercise important application simple example object moving dimension figure inference problem posterior marginal node given observation next turn problem marginal node given observation temporal data inclusion future well past observation though used prediction play role learning parameter model analogy hidden model problem message node back node information forward message passing stage used compute literature usual formulate backward recursion term rather term must also write form derive recursion start backward recursion figure illustration linear system used track moving object blue point indicate true position object space successive time step green point denote noisy measurement position cross indicate mean posterior distribution position running equation position ellipsis correspond contour standard deviation continuous latent variable written form multiply side substitute make together manipulation obtain exercise made note recursion require forward quantity available backward algorithm also require pairwise posterior form substituting mean given component covariance given exercise
['learning', 'lds'] learning considered inference problem linear dynamical system assuming model parameter known next consider determination parameter maximum likelihood mani model latent variable dressed algorithm general term chapter derive algorithm linear dynamical system denote parameter value particular cycle algorithm parameter value inference algorithm determine posterior distribution latent variable precisely local posterior step particular shall require following expectation used consider likelihood function taking logarithm therefore given made dependence parameter explicit take expectation likelihood respect posterior function step function respect component consider parameter substitute take expectation respect obtain term dependent absorbed additive constant maximization respect easily making maximum likelihood solution distribution section giving exercise similarly optimize substitute giving constant term independent respect parameter give exercise anew anew note anew must result used determine finally order determine value substitute giving respect give exercise parameter learning linear dynamical system maximum likelihood inclusion prior give estimate straightforward fully treatment found analytical technique chapter though detailed treatment lack space
['extensions', 'lds'] extension hidden model considerable interest extending basic linear dynamical system order increase capability although assumption model lead algorithm inference learning also marginal distribution variable simply cant limitation simple extension linear dynamical system mixture initial distribution mixture component forward recursion equation lead mixture hidden variable model tractable many application emission density poor approximation instead mixture emission density posterior also mixture however posterior comprise mixture given mixture thus number component exponentially length chain model impractical generally transition emission model depart exponential family model lead intractable infer problem make deterministic approximation assumed expectation propagation make sampling method chapter section widely used approach make approximation around mean distribution give rise extended hidden model develop interesting extension linear dynamical system expanding graphical representation example switching state space model combination hidden model linear dynamical system model multiple chain continuous latent vari analogous latent chain linear dynamical system together chain discrete variable form used hidden model output time step determined choosing continuous latent chain state discrete latent variable switch observation corresponding conditional output distribution exact inference model intractable vari method lead inference scheme recursion along continuous discrete chain independently note consider multiple chain discrete latent variable switch select remainder obtain analogous model discrete latent variable known switching hidden model
['particle', 'filters'] particle dynamical system example emission density turn sampling method order chapter tractable inference algorithm particular apply sampling formalism section obtain sequential monte algorithm known particle consider class distribution graphical model suppose given value wish draw sample posterior distribution theorem sample drawn made conditional independence property graph figure sampling weight sample used numerator denominator thus posterior distribution sample together corresponding weight note weight satisfy wish sequential sampling scheme shall suppose sample weight time step subsequently value wish weight time step sample distribution straightforward since theorem made conditional independence property follow application criterion graph distribution given mixture distribution sample drawn choosing component probability given drawing sample corresponding component summary view step particle algorithm stage time step sample representation posterior expressed sample corresponding weight mixture representation form obtain corresponding representation next time step draw sample mixture distribution sample evaluate corresponding weight case single variable figure particle sequential monte approach literature various name bootstrap survival condensation algorithm blake exercise technique section verify model shown figure node total conditional independence property similarly show model graph figure node total figure schematic illustration operation particle latent space time step posterior mixture distribution shown schematically circle whose size proportional weight sample drawn distribution weight conditional independence property consider joint probability distribution corresponding directed graph figure product rule probability verify joint distribution conditional independence property similarly show model joint distribution conditional independence property show distribution data state space model directed graph figure satisfy conditional independence property hence exhibit property order consider hidden model emission density parametric model linear regression model neural network vector adaptive parameter describe parameter learned data maximum likelihood verify equation initial state tie transition probability parameter hidden model likelihood function appropriate multiplier enforce summation constraint component show element parameter hidden model initially zero element remain zero subsequent update algorithm consider hidden model emission density show maximization function respect mean covariance parameter give rise equation hidden model discrete observation multinomial distribution show conditional distribution observation given hidden variable given corresponding step given write analogous equation conditional distribution step equation case hidden multiple binary output variable conditional hint refer section discussion corresponding maximum likelihood solution data criterion verify conditional property joint distribution hidden model product rule probability verify independence property joint distribution hidden model starting expression marginal distribution vari factor factor graph together result message algorithm section derive result joint posterior distribution successive latent variable hidden model suppose wish train hidden model maximum likelihood data independent sequence observation note show step algorithm simply evaluate posterior probability latent variable running recursion independently sequence also show step initial probability transition probability parameter form given notational convenience assumed sequence length generalization sequence different length straightforward similarly show equation mean emission model given note equation emission model parameter distribution take analogous form message factor node variable node factor graph together expression joint distribution hidden model show alpha message message factor node variable node factor graph together expression joint distribution hidden model show beta message expression hidden model derive corresponding result expressed term variable exercise derive forward message passing equation algorithm directly expression joint distribution hidden variable taking maximization summation derive recursion quantity show initial condition recursion given show directed graph hidden model given figure expressed factor graph form shown figure write expression initial factor general factor result exercise derive recursion equation initial condition algorithm hidden model shown figure smoother equation allow posterior individual latent variable conditioned variable found linear dynamical system show sequence latent variable value posterior distribution individually probable sequence latent value simply note joint distribution latent variable linear dynamical system hence also make result result prove result together matrix identity derive result gain matrix together result derive together result derive consider generalization include constant term mean show extension recase framework chapter state vector additional component unity matrix extra column corresponding parameter exercise show equation applied independent observation reduce result given section maximum likelihood solution single distribution consider prob mean single random variable given independent observation model linear dynamical system latent variable becomes identity matrix transition prob ability observation independent parameter initial state respectively suppose becomes write corresponding equation starting general result together show equivalent result directly consider independent data consider special case linear dynamical system section equivalent probabilistic transition matrix covariance noise covariance making matrix inversion identity show emission density matrix posterior distribution hidden state result probabilistic consider linear dynamical system form amplitude observation noise go zero show posterior distribution mean zero variance accord intuition noise current observation estimate state variable ignore previous observation consider special case linear dynamical system section state variable constrained equal previous state variable simplicity assume also initial condition unimportant prediction determined purely data proof induction show posterior mean state determined average intuitive result state variable constant best estimate observation starting backwards recursion equation derive smoothing equation linear dynamical starting result pairwise posterior marginal state space model derive form case linear dynamical system starting result substituting verify result covariance verify result equation linear dynamical system verify result equation linear dynamical system verify result equation linear dynamical system model chapter range different model cation regression problem often found performance combining multiple model together instead single model isolation instance might train different model make prediction average prediction made model combination model sometimes committee section cuss way apply committee concept practice also give insight sometimes effective procedure important variant committee method known training multiple model sequence error function used train model performance previous model produce substantial improvement performance single model section instead prediction model alternative form model combination select model make prediction choice model function input variable thus different model come responsible making prediction different region input space widely used framework kind known decision tree process sequence binary selection corresponding traversal tree structure section case individual model generally chosen simple overall model selection process decision tree applied cation regression problem limitation decision tree division input space based hard split model responsible making prediction given value input variable decision process moving probabilistic framework combining model section example model conditional distribution input variable target variable index model form probabilistic mixture form represent model mixture distribution component density well conditioned input variable known mixture expert closely related mixture density network model section
['bayesian', 'model', 'averaging'] model important distinguish model combination method model often confused understand difference sider example density estimation mixture several section component combined model binary latent variable component mixture responsible generating corresponding data point thus model term joint distribution corresponding density variable marginal latent variable case mixture example lead distribution form usual interpretation symbol example model nation independent identically distributed data write marginal probability data form thus data point corresponding latent variable suppose several different model indexed prior probability instance model might mixture another model might mixture distribution marginal distribution data given example model interpretation model responsible generating whole data probability distribution simply uncertainty model size data increase uncertainty posterior probability become increasingly model highlight difference model model combination model whole data single model contrast combine multiple model different data point within data potentially different value latent variable hence different component although considered marginal probability apply predictive density conditional distribution exercise
['committees'] committee construct committee average prediction individual model procedure perspective considering bias variance section model bias component difference model true function variance component sent sensitivity model individual data point recall figure trained multiple polynomial sinusoidal data aver aged resulting function contribution variance term cancel leading prediction corresponding higher order polynomial accurate prediction underlying sinusoidal function data practice course single data introduce variability different model within committee approach bootstrap data set section consider regression problem trying predict value single continuous variable suppose generate bootstrap data set train separate copy predictive model committee prediction given procedure known bootstrap aggregation bagging suppose true regression function trying predict given output model written true value plus error form average error take form expectation respect distribution input vector average error made model acting individually therefore similarly error committee given assume error zero mean uncorrelated obtain exercise apparently dramatic result average error model reduced factor simply version model assumption error individual model uncorrelated practice error typically highly correlated overall error generally small however shown committee error exceed error constituent model order achieve cant improvement turn exercise sophisticated technique building committee known
['boosting'] powerful technique combining multiple base produce form committee whose performance better base describe widely used form algorithm short adaptive give good result even base performance slightly better random hence sometimes base known weak learner originally designed cation problem also extended regression principal difference committee method bagging base trained sequence base trained weighted form data weighting associated data point performance previous particular point base given greater weight used train next sequence trained prediction combined weighted majority voting scheme schematically figure consider cation problem training data input vector along corresponding binary target variable data point given associated weighting parameter initially data point shall suppose procedure available training base weighted data give function stage algorithm train data weighting according performance previously trained give greater weight data point finally desired number base trained combined form committee give different weight different base precise form algorithm given figure schematic illustration framework base trained weighted form train blue arrow weight depend performance base green arrow base trained combined give arrow sign initialize data weighting setting training data weighted error function indicator function equal otherwise evaluate quantity evaluate update data weighting make prediction model given sign base trained weighting equal therefore usual procedure training single subsequent iteration weighting data point data point correctly successive therefore forced place greater emphasis point previous data point continue successive receive ever greater weight quantity represent weighted sures error rate base data therefore weighting give greater weight accurate overall output given algorithm figure subset data point taken cation data shown figure base learner threshold input variable simple form decision tree known decision stump section sion tree single node thus base learner input according whether input feature threshold therefore simply space region linear decision surface parallel ax
['minimizing', 'exponential', 'error'] exponential error originally statistical learning theory leading upper bound generalization error however bound turn loose practical value actual performance much better bound alone would suggest gave different simple interpretation term sequential minimization exponential error function consider exponential error function term linear combination base form training target value goal minimize respect weighting parameter base figure illustration base learner consist simple threshold applied ax show number base learner trained along decision boundary recent base learner dashed black line combined decision boundary semble solid green line data point circle whose radius weight assigned data point training recently added base learner thus instance point base learner given greater weight training base learner instead global error function minimization however shall pose base respect contribution base write error function form constant denote data point correctly denote point turn rewrite error function form minimize respect second term equivalent overall factor front summation affect location minimum similarly respect obtain exercise found weight data point making fact weight next iteration term independent weight data point factor thus obtain finally base trained data point sign combined function according factor affect sign giving
['error', 'functions', 'boosting'] error function exponential error function algorithm considered previous chapter gain insight nature exponential error function consider error given perform variational minimization respect possible function obtain exercise figure plot exponential green error function along hinge blue used support vector machine cation error black note large negative value give linearly creasing penalty whereas loss give exponentially creasing penalty half thus algorithm seeking best odds ratio within space function linear combination base subject constrained minimization resulting sequential optimization strategy result sign function arrive cation decision already seen minimizer error cation given posterior class probability case target variable seen error function given section exponential error function divided error constant factor pass point ease comparison seen continuous approximation ideal cation error advantage exponential error sequential minimization lead simple scheme drawback however large negative value much strongly particular large negative value linearly whereas exponential error function exponentially thus error function much robust outlier data point another important difference exponential function latter likelihood function probabilistic model furthermore exponential error exercise generalize cation problem class contrast probabilistic model easily generalized give section interpretation sequential optimization additive model exponential error open door wide range algorithm extension choice error function also extension regression problem consider error function regression sequential minimization additive model form simply base residual error previous model exercise noted however error robust outlier figure comparison squared error green absolute error showing latter place much emphasis large error hence robust outlier data point algorithm absolute deviation instead error function figure
['tree', 'based', 'models'] model various simple widely used model work partitioning input space cuboid region whose edge ax simple model example constant region model combination method model responsible making prediction given point input space process model given input sequential decision making process corresponding traversal binary tree split branch node focus particular framework cation regression tree cart although many variant going name figure show illustration recursive binary partitioning input space along corresponding tree structure example step figure illustration space partitioned region boundary figure binary tree corresponding input space shown divide whole input space region according whether parameter model independently instance region according whether giving rise region recursive subdivision traversal binary tree shown figure input determine region fall starting tree root node following path leaf node according decision criterion node note decision tree probabilistic graphical model within region separate model predict target variable instance regression might simply predict constant region cation might assign region class property tree based model make popular eld medical diagnosis example readily interpretable human correspond sequence binary decision applied individual input variable stance predict patient disease might temperature greater threshold answer might next blood pressure threshold leaf tree associated diagnosis order learn model training determine structure tree input variable chosen node form split criterion well value threshold parameter split also determine value predictive variable within region consider regression problem goal predict single target variable vector input variable training data input vector along corresponding continuous label partitioning input space given minimize error function optimal value predictive variable within given region given average value data point fall region exercise consider determine structure decision tree even number node tree problem optimal structure choice input variable split well corresponding thresh old minimize error usually infeasible large number possible solution instead greedy generally done starting single root node corresponding whole input space growing tree node time step number candidate region input space split corresponding addition pair leaf node tree choice input variable split well value threshold joint optimization choice region split choice input variable threshold done exhaustive search given choice split variable threshold optimal choice predictive variable given local average data noted repeated possible choice variable split give residual error given greedy strategy growing tree remains issue stop node simple approach would stop reduction residual error fall threshold however found often none available split produce cant reduction error several split substantial error reduction found reason practice grow large tree stopping criterion based number data point associated leaf node prune back resulting tree pruning based criterion balance residual error measure model complexity denote starting tree pruning pruning node word internal node combining corresponding region suppose leaf node indexed leaf node region input space data point total number leaf node optimal prediction region given corresponding contribution residual pruning criterion given regularization parameter overall residual error complexity model measured number leaf node value chosen cation problem process growing pruning tree except error appropriate measure performance proportion data point region assigned class commonly used choice index vanish maximum encourage formation region high proportion data point assigned class cross entropy index better measure cation rate growing tree sensitive node probability also unlike cation rate differentiable exercise hence better gradient based optimization method subsequent pruning tree cation rate generally used human interpretability tree model cart often seen major strength however practice found particular tree structure learned sensitive detail data small change training data result different split problem method kind considered section split ax feature space suboptimal instance separate class whose optimal decision boundary run degree ax would need large number split input space single split furthermore split decision tree hard region input space associated leaf node model last issue particularly problematic regression typically aiming model smooth function tree model produce prediction discontinuity split boundary
['conditional', 'mixture', 'models'] conditional mixture model seen standard decision tree restricted hard split input space constraint relaxed expense interpretability soft probabilistic split function input variable time also give leaf model probabilistic inter arrive fully probabilistic model hierarchical mixture expert consider section alternative motivate hierarchical mixture expert model start standard probabilistic mixture unconditional density model replace component density conditional distribution chapter consider mixture linear regression model section mixture logistic regression model section case independent input variable make generalization allow also depend input obtain mixture expert model finally allow component mixture model mixture expert model obtain hierarchical mixture expert
['mixtures', 'linear', 'regression', 'models'] mixture linear regression model many advantage giving probabilistic interpretation regression model used component complex probabilistic model done instance conditional distribution linear regression model node directed prob graph consider simple example corresponding mixture linear regression model straightforward extension gaus mixture model section case conditional distribution therefore consider linear regression model weight parameter many application appropriate common noise variance precision parameter component case consider restrict attention single target variable though extension multiple output straightforward denote exercise mixture distribution written adaptive parameter model namely likelihood function model given data observation take form vector target variable order maximize likelihood function appeal algorithm turn simple extension algorithm unconditional mixture section therefore build unconditional mixture introduce binary latent variable data point element zero except single value component mixture responsible generating data point joint distribution latent variable graphical model shown figure likelihood function take form exercise figure probabilistic directed graph mixture linear regression model algorithm begin choosing initial value model param step parameter value used evaluate posterior probability responsibility component every data point given responsibility used determine expectation respect posterior distribution likelihood take form step maximize function respect keeping optimization respect need take account constraint done multiplier leading equation form exercise note exactly form corresponding result simple mixture unconditional given next consider maximization respect parameter vector linear regression model substituting distribution function function parameter vector take form constant term contribution weight vector note quantity similar negative standard error single linear regression model inclusion responsibility weighted least square problem term corresponding data point carry weighting given could effective precision data point component linear regression model mixture parameter vector separately whole data step data point weighted responsibility model take data point setting derivative respect equal zero give write matrix notation diagonal matrix size obtain normal equation corresponding weighted least square problem form found context logistic regression note step matrix change solve normal equation afresh subsequent step finally maximize respect keeping term depend function written setting derivative respect equal zero obtain equation form figure illustrate algorithm simple example mixture straight line data input variable target variable predictive density plotted figure parameter value algorithm corresponding plot figure also shown result single linear regression model give unimodal predictive density mixture model give much better representation data distribution higher likelihood value however mixture model also cant probability mass region data predictive distribution bimodal value problem resolved extending model allow mixture function leading model mixture density network section hierarchical mixture expert section figure example synthetic data shown green point input variable target variable together mixture linear regression model whose mean function shown blue line upper three plot show initial left result running iteration result iteration right reciprocal true variance target value lower three plot show corresponding responsibility plotted vertical line data point length blue segment give posterior probability blue line data point similarly segment
['mixtures', 'logistic', 'models'] mixture logistic model logistic regression model conditional distribution target variable given input vector straightforward component distribution mixture model thereby giving rise family conditional distribution single logistic regression model example straightforward combination idea section book help consolidate reader conditional distribution target variable probabilistic mixture logistic regression model given feature vector output component adjustable parameter namely suppose given data corresponding likelihood figure left plot show predictive conditional density corresponding solution figure give likelihood value vertical slice plot particular value corresponding conditional distribution bimodal plot right show predictive density single linear regression model data maximum likelihood model smaller likelihood function given maximize likelihood function iteratively making algorithm latent variable correspond binary indicator variable data point likelihood function given matrix latent variable element initialize algorithm choosing initial value model parameter step parameter value evaluate posterior probability data point given responsibility used likelihood function given step maximization function respect keeping hence maximization respect done usual multiplier enforce summation constraint giving familiar result determine note function term indexed vector different vector step algorithm word different component interact responsibility step note step solution must iteratively instance iterative least square algorithm gradient vector given section gradient respect dent solve separately algorithm thus equation component correspond simply section single logistic regression model weighted data data point carry weight figure show example mixture logistic regression model applied simple cation problem extension model mixture model class straightforward exercise
['mixtures', 'experts'] mixture expert section considered mixture linear regression model section analogous mixture linear although simple mixture extend linear model include plex multimodal predictive distribution still limited increase capability model function input variable known mixture expert model known gating function individual component density expert notion behind terminology differ component model distribution different region input space figure illustration mixture logistic regression model left plot show data point drawn class blue background colour pure pure blue true probability class label plot show result single logistic regression model maximum likelihood background colour corresponding probability class label colour purple model probability around class input space right plot show result mixture logistic regression model give much higher probability correct label many point blue class expert making prediction region gating function determine component dominant region gating function must satisfy usual constraint namely therefore example linear model form expert also linear regression cation model whole model algorithm iterative least square employed step jordan model still cant limitation linear model gating expert function much model gating function give hierarchical mixture expert model jordan understand structure model imagine mixture distribution component mixture mixture distribution simple unconditional mixture hierarchical mixture trivially equivalent single mixture distribution however exercise input dependent hierarchical model becomes model also probabilistic version decision tree section trained maximum likelihood algorithm step treatment section given bishop based variational inference shall discus detail however worth pointing close connection mixture density network section principal advantage mixture expert model step mixture component gating model convex optimization although overall optimization advantage mixture density network approach component density share hidden unit neural network furthermore mixture density network split input space relaxed hierarchical mixture expert soft constrained axis also nonlinear exercise consider model form input vector target vector index different model latent vari able model parameter model suppose model prior probability given training write formula evaluate distribution latent variable model index formula highlight difference different model latent variable within single model error simple committee model error committee given assuming individual error satisfy derive result making inequality special case convex function show average error member simple committee model given error committee given satisfy making equality show result derived previous exercise hod error function square provided convex function consider committee allow unequal weighting constituent model order ensure prediction remain within sensible limit pose require bounded value minimum maximum value given member committee show necessary condition constraint satisfy error function respect show parameter algorithm making variational minimization exponential error function given respect possible function show function given show exponential error function algorithm correspond likelihood probabilistic model done showing corresponding conditional distribution correctly show sequential minimization error additive model form style simply base residual error previous model verify minimize error training value single predictive value optimal solution given mean consider data data point class data point class suppose tree model split leaf node second leaf node point assigned point assigned similarly suppose second tree model split evaluate cation rate tree hence show equal similarly evaluate index tree show lower tree tree extend result section mixture linear regression model case multiple target value vector make result section verify likelihood function mixture linear regression model given technique multiplier appendix show equation mixture linear regression model trained maximum likelihood given already noted squared loss function sion problem corresponding optimal prediction target variable input vector given conditional mean predictive distribution show conditional mean mixture linear regression model section given linear combination mean component note conditional distribution target data multimodal conditional mean give poor prediction extend logistic regression mixture model section mixture class write algorithm parameter model maximum likelihood consider mixture model conditional distribution form mixture component mixture model show hierarchical mixture equivalent conventional mixture model suppose level model arbitrary function show hierarchical model equivalent model finally consider case level mixture constrained linear cation logistic model show hierarchical mixture general mixture linear cation model hint construct single consider mixture component component mixture component given model show mixture component determined model algorithm book detailed information format data set well data book site digit digit data used book taken data subset much data produced national institute standard technology training example test example data collected census bureau employee rest collected child care taken ensure test example written different individual training example original data binary black white create image size aspect ratio consequence used change resolution image resulting digit grey scale image example digit shown figure error rate digit range simple linear carefully designed support vector machine convolutional neural network figure hundred example digit chosen training flow synthetic data arose project measuring proportion water north transfer pipeline bishop based principle gamma idea narrow beam gamma ray pipe attenuation intensity beam information density material along path thus instance beam strongly single attenuation measurement alone degree freedom corresponding fraction fraction water fraction redundant three fraction must address gamma beam different energy word different frequency pipe along path attenuation measured property different material vary function energy measurement attenuation energy independent piece information given known prop water energy simple matter calculate average fraction water hence measured along path gamma beam complication however associated motion along pipe velocity small oat water sitting known laminar stratus figure three geometrical water phase used generate data portion three phase vary water homogeneous stratus annular figure velocity complex geometrical water arise purpose data idealization considered annular water form concentric cylinder water around outside whereas homogeneous water assumed intimately mixed might occur high velocity turbulent condition also figure seen single beam give water fraction measured along path length whereas interested volume fraction water multiple gamma whose beam different region pipe particular data beam spatial arrangement shown single observation therefore dimensional vector fraction water measured along path beam however interested overall volume fraction three phase pipe much like classical problem tomographic construction used medical example figure cross section pipe showing arrangement beam line single dual energy gamma densitometer note vertical beam asymmetrically relative central axis shown dotted line reconstructed number average line measurement typical tomography application hand range geometrical much limited well phase fraction reasonable accuracy densitometer data safety reason intensity gamma beam kept relatively weak obtain accurate measurement attenuation measured beam intensity time interval integration time random measured intensity fact gamma beam comprise discrete packet energy photon practice integration time chosen compromise reducing noise level long integration time temporal variation short integration time data realistic known value absorption property water gamma energy used choice integration time second chosen characteristic typical practical setup point data independently following step choose three phase random equal probability choose three random number uniform distribution foil treat three phase equal footing volume fraction beam line calculate effective path length water given phase perturb path length distribution based known beam intensity integration time allow effect photon statistic point data path length measurement together fraction water binary label phase ration data divided training validation test set independent data point detail data format available book site bishop statistical machine learning technique used predict volume fraction also geometrical phase shown figure dimensional vector measurement dimensional observation vector also used test data visualization data rich interesting structure given degree freedom corresponding fraction water integration time data locally live dimensional manifold integration time individual data point perturbed away manifold photon noise homogeneous phase path length water linearly related fraction water data point close linear manifold annular relationship phase fraction path length nonlinear manifold nonlinear case laminar situation even complex small variation phase fraction cause horizontal phase boundary move across horizontal beam line leading discontinuous jump dimensional observation space nonlinear manifold laminar broken distinct segment note also manifold different phase meet point example pipe entirely instance laminar annular faithful faithful shown figure hydrothermal geyser national park state popular tourist attraction name stem supposed regularity eruption data observation single variable corresponding duration minute time next eruption also minute figure show plot time next eruption versus duration eruption seen time next eruption considerably although knowledge duration current eruption accurately note exist several data set eruption faithful figure faithful geyser national park figure plot time next eruption minute vertical axis versus duration eruption minute horizontal axis faithful data synthetic data throughout book simple synthetic data set illustrate many algorithm regression problem based sinusoidal shown figure input value uniformly range corresponding target value corresponding value function random noise distribution standard deviation various form data different number data point used book second data cation problem class equal prior probability shown figure blue class single class come mixture cause know class prior density ward evaluate plot true posterior probability well minimum decision boundary shown figure figure plot show synthetic regression data along underlying sinusoidal function data point plot show true conditional distribution label green curve mean shaded region span standard deviation side mean figure left plot show synthetic cation data data class shown blue right plot true posterior probability shown colour scale going pure probability class pure blue probability class probability known optimal decision boundary cation rate contour along posterior probability class equal shown green curve decision boundary also plotted probability distribution distribution list statistic expectation variance covariance mode entropy distribution member exponential family widely used building block sophisticated probabilistic model distribution single binary variable example result coin single continuous parameter probability otherwise special case binomial distribution case single observation conjugate prior beta distribution beta distribution continuous variable often used represent probability binary event parameter constrained ensure distribution beta mode beta conjugate prior distribution effective prior number observation respectively density otherwise singularity uniform distribution beta distribution special case distribution binomial binomial distribution give probability observing occurrence sample distribution probability integer equal quantity number way choosing object total identical object pronounced factorial product particular case binomial distribution known distribution large binomial distribution approximately conjugate prior beta distribution distribution random variable subject constraint mode known digamma function parameter subject constraint order ensure distribution form conjugate prior multinomial distribution generalization beta distribution case parameter effective number observation corresponding value binary observation vector beta distribution density everywhere provided gamma gamma probability distribution positive random variable parameter subject constraint ensure distribution mode digamma function gamma distribution conjugate prior precision inverse variance density everywhere special case known exponential distribution widely used distribution continuous variable also known normal distribution case single variable parameter mean variance inverse variance precision square root variance standard deviation conjugate prior conjugate prior gamma distribution unknown joint conjugate prior distribution vector mean vector covariance matrix must symmetric inverse covariance matrix precision matrix also symmetric positive average random variable tend central limit theorem variable distribution entropy given variance covariance linear transformation random variable marginal distribution respect subset variable similarly conditional distribution also conjugate prior conjugate prior conjugate prior marginal distribution conditional distribution given form marginal distribution conditional distribution given given joint distribution following partition conditional distribution given marginal distribution given conjugate prior distribution mean precision unknown also distribution product distribution whose precision proportional gamma distribution conjugate prior distribution mean precision unknown also distribution product distribution whose precision proportional distribution particular case scalar equivalent multinomial generalize distribution binary variable component obtain following discrete distribution element identity matrix parameter must satisfy multinomial distribution generalization binomial give distribution count discrete variable state given total number observation quantity give number way taking identical object value give probability random variable taking state parameter subject constraint conjugate prior distribution parameter normal normal distribution simply another name book term throughout although retain conventional symbol denote distribution consistency shall refer normal gamma distribution distribution similarly normal student distribution employer ness brewery publish pseudonym chose student form student conjugate gamma prior precision distribution grating precision variable therefore mixture mean different variance number degree freedom distribution particular case distribution variable student marginal precision matrix respect conjugate prior take form squared distance limit mean student generalization whose maximum likelihood parameter value robust outlier uniform simple distribution continuous variable interval distribution distribution mi mi distribution also known circular normal circular gaus periodic distribution variable function kind distribution period care must taken interpret distribution simple expectation dependent arbitrary choice origin variable parameter analogous mean parameter known concentration param analogous precision inverse variance large mi distribution approximately distribution conjugate prior precision matrix variate symmetric positive matrix digamma function parameter number degree freedom distribution restricted ensure gamma function normalization factor dimension gamma distribution given parameter matrix determinant intended introductory tutorial assumed reader already familiar basic linear algebra result indicate prove whereas complex case leave interested reader refer standard textbook subject case assume inverse exist matrix dimension formula correctly comprehensive discussion linear algebra found loan extensive collection matrix property given matrix derivative basic matrix identity matrix element index row index column denote identity matrix also unit matrix ambiguity dimensionality simply transpose matrix element transpose veri writing index inverse also easily proven taking transpose useful identity matrix inverse following easily veri right multiplying side suppose dimensionality dimensionality much evaluate side side special case sometimes another useful identity inverse following known identity veri multiplying side useful instance large diagonal hence easy invert many row column conversely side much evaluate side vector said linearly independent relation hold none vector expressed linear combination remainder rank matrix maximum number linearly independent row equivalently maximum number linearly independent column trace determinant trace determinant apply square matrix trace matrix element leading diagonal writing index formula multiple time product three matrix known cyclic property trace operator clearly product number matrix determinant matrix taken product precisely element element column according whether permutation even respectively note thus matrix determinant take form determinant product matrix given shown also determinant inverse matrix given shown taking determinant matrix size useful special case column vector matrix derivative sometimes need consider derivative vector matrix respect scalar derivative vector respect scalar vector whose component given analogous derivative matrix derivative respect vector matrix also instance similarly following easily proven writing component similarly derivative inverse matrix expressed shown equation right multiplying also shall prove later choose element seen writing matrix index notation write result compactly form notation following property proven writing matrix index also equation square matrix size equation corresponding eigenvalue simultaneous homogeneous linear equation condition solution known characteristic equation polynomial order must solution though need distinct rank equal number nonzero eigenvalue particular interest symmetric matrix arise covariance trice kernel matrix symmetric matrix property equivalently inverse symmetric matrix also metric seen taking transpose together symmetry general eigenvalue matrix complex number symmetric matrix eigenvalue real seen left multiplying complex conjugate give next take complex conjugate left multiply give used consider real matrix taking transpose second equation side equation equal hence must real real symmetric matrix chosen orthogonal unit length element identity matrix show left multiply give hence exchange index take transpose second equation make symmetry property subtract equation give hence hence orthogonal eigenvalue equal linear combination also vector eigenvalue select linear combination arbitrarily choose second orthogonal shown generate never linearly dependent hence chosen orthogonal unit length eigenvalue corresponding orthogonal form complete vector expressed linear combination take column matrix matrix said orthogonal interestingly row matrix also orthogonal show note also equation expressed term form diagonal matrix whose diagonal element given eigenvalue consider column vector orthogonal matrix give vector length vector similarly angle vector thus multiplication rigid rotation system diagonal matrix matrix matrix left multiply right multiply obtain taking inverse equation together last equation also written form take determinant obtain similarly taking trace cyclic property trace operator together leave exercise reader verify making result matrix said positive value vector equivalently positive matrix eigenvalue seen setting turn arbitrary vector expanded linear combination note positive element positive example matrix eigenvalue matrix said positive hold value equivalent return output value functional operator take function return output value example functional length curve drawn plane path curve term function context machine learning widely used functional entropy continuous variable choice probability density function return scalar value entropy density thus entropy could equally well written common problem conventional calculus value function similarly calculus variation seek function functional possible function wish particular function maximum minimum calculus variation used instance show path point straight line maximum entropy distribution werent familiar rule ordinary calculus could evaluate conventional derivative making small change variable expanding power taking limit similarly function several variable corresponding partial derivative analogous functional derivative consider much functional change make small change function figure functional derivative considering value functional change function arbitrary function arbitrary function figure denote functional derivative respect following relation seen natural extension continuous variable namely value point functional stationary respect small variation function give must hold arbitrary choice functional derivative must vanish imagine choosing perturbation zero everywhere except point case functional derivative must zero however must true every choice functional derivative must vanish value consider functional integral function derivative well direct value assumed boundary region integration might consider variation function obtain cast form integrate second term part make fact must vanish boundary integral boundary give read functional derivative comparison functional derivative give known equation example equation take form second order differential equation making boundary condition often consider integral whose take form depend derivative case station simply value functional respect probability distribution need maintain normalization constraint probability often conveniently done multiplier appendix strained optimization extension result multidimensional variable straight forward comprehensive discussion calculus variation stationary point function several variable subject constraint consider problem maximum function subject constraint write form approach would solve constraint equation thus express function form substituted give function alone form maximum respect could found differentiation usual give stationary value corresponding value given problem approach cult analytic solution constraint equation expressed explicit also approach treat differently spoil natural symmetry variable elegant often simpler approach based introduction parameter multiplier shall motivate technique geometrical perspective consider variable component constraint equation dimensional surface figure note point constraint surface gradient constraint function orthogonal surface consider point lie constraint surface consider nearby point also lie surface make expansion around constraint surface hence limit figure geometrical picture technique grange multiplier seek maximize function subject constraint dimensional constraint subspace dimensionality curve problem function parallel constraint surface vector normal surface next seek point constraint surface point must property vector also orthogonal constraint surface figure otherwise could increase value moving short distance along constraint surface thus parallel antiparallel vector must exist parameter known multiplier note either sign point convenient introduce function constrained condition setting condition lead constraint equation thus maximum function subject constraint function given stationary point respect vector give equation determine stationary point value interested eliminate without needing value hence term undetermined multiplier simple example suppose wish stationary point function subject constraint figure corresponding function given condition stationary respect give following coupled equation figure simple example maximize subject constraint circle show contour function diagonal line show constraint surface solution equation give stationary point corresponding value multiplier considered problem function subject equality constraint form consider problem subject inequality constraint form figure kind solution possible according whether strained stationary point lie region case straint inactive whether lie boundary case constraint said active former case function play role stationary condition simply stationary point function time latter case solution lie boundary analogous equality straint previously stationary point function however sign multiplier crucial function maximum gradient away region figure therefore value either case product thus solution figure illustration problem subject inequality constraint problem subject function respect subject condition known condition tucker note wish minimize rather maximize function inequality constraint minimize function respect subject finally straightforward extend technique multiplier case multiple equality inequality constraint suppose wish subject introduce multiplier optimize function given subject extension constrained functional derivative similarly straightforward detailed discussion appendix technique multiplier wright
['references'] reference handbook mathematical function dover method monte evaluation partition action physical review constrained algorithm principal component analysis computation probability problem pattern recognition learning method potential function remote control look statistical model cation transaction automatic control general class divergence distribution another journal royal statistical singer reducing binary margin journal machine learning research method statistic springer yang learning algorithm blind signal advance neural system volume press natural gradient work learning neural computation foundation research press asymptotic theory component analysis annals statistic doucet introduction chine learning machine learning introduction computational learning theory university press independent factor analysis computation parameter latent variable model variational uncertainty intelligence proceed fifth conference morgan bach jordan kernel inde pendent component analysis journal machine learning research learning advance information system volume press machine learning approach second press neural network principal component analysis learning example without local minimum neural work barber bishop model comparison monte jordan neural information volume press barber bishop ensemble learning network neural information volume barber bishop ensemble learning neural network bishop generalization neural network machine learning springer latent variable model factor analysis statistical factor analysis related method theory application bather decision theory introduction dynamic sequential sion generalized analysis kernel approach computation inequality associated maximization technique statistical estimation probabilistic function process inequality becker improving vergence learning order method proceeding model summer school morgan bell maximization approach blind blind neural bellman adaptive control process tour university press input output architecture advance neural information system volume press robust linear discrimination linearly separable set method berger statistical decision theory analysis second springer smith theory near limit proceed spatiotemporal model eld transaction conference information theory statistical decision function random process statistical analysis dirty picture journal royal statistical green computation stochastic system statistical science bishop fast procedure international journal neural system bishop exact calculation matrix neural computation bishop smoothing learning algorithm network transaction neural network bishop novelty detection network validation proceeding vision image signal special issue application neural network bishop neural network pattern recognition university press bishop training noise regularization neural bishop neural information volume press bishop variational principal component proceeding ninth conference neural network volume bishop analysis multiphase gamma neural network nuclear method physic research bishop conditional probability distribution periodic variable neural computation bishop pattern recognition machine learning companion springer preparation bishop variational inference engine network becker advance neural information system volume press bishop mixture expert meek proceeding nineteenth conference uncertainty morgan bishop distinguishing text graphic line proceeding ninth international workshop frontier handwriting japan bishop optimization latent variable model advance neural information system volume press bishop alternative advance neural information system volume press bishop cation factor proceeding fifth international conference neural network bridge institute electrical engineer bishop development generative bishop generative topographic neural computation bishop tipping latent variable model data transaction pattern analysis machine intelligence bishop nonlinear image proceeding sixth conference computer vision volume springer jordan model application information retrieval statistic press block model brain review modern physic multidimensional stochastic proximation method annals mathematical statistic tourist guide acta boser training algorithm optimal margin proceeding fifth workshop computational learning colt singular value composition biological cybernetics time series analysis prentice hall infer statistical analysis convex university press koller tractable inference complex stochastic process cooper moral proceeding annual conference uncertainty morgan fast approximate energy minimization graph cut transaction pattern analysis chine intelligence bagging predictor machine learning stone cation regression tree brook chain monte method application able functional interpolation adaptive work complex system back propagation complex system second derivative work review transaction neural network tutorial support machine pattern recognition knowledge discovery data mining blind signal separation tical principle proceeding berger statistical second expert system probabilistic network springer vari learning missing data neural computation geometry neural network error surface neural computation monte method putation springer grant orthogonal least square learning algorithm radial basis function network neural network variational mixture independent component neural computation random eld tic welsh disorder physical system volume university press collins generalization principal component exponential family becker advance neural information system press blind source separation problem statement signal bishop vari model selection mixture distribution proceeding eighth international intelligence statistic morgan stein introduction algorithm press support vector network machine learning cotter application neural network transaction neural network cover hart nearest neighbor tern cation transaction theory cover element information theory probabilistic network expert system springer probability frequency reasonable expectation journal physic scaling second chapman hall statistic spatial data support vector machine learning method university press sparse gaus process neural computation information alternating minimization procedure statistic decision approximation sigmoidal function mathematics control signal system conditional independence statistical theory discussion journal royal statistical society series conditional independence statistical operation annals statistic theory probability son dempster laird maximum likelihood incomplete data algorithm journal royal statistical society smith method nonlinear cation regression know metropolis algorithm journal computer system science learning problem output code journal intelligence research hybrid monte physic letter hart pattern cation scene analysis hart stork tern cation second eddy biological sequence analysis bridge university press anthology probabilistic model medical probabilistic modeling medical springer bootstrap method another look jackknife annals statistic triangle inequality proceeding twelfth international conference machine learning hidden model estimation springer application hidden model noisy speech transaction acoustic speech signal map convergence property energy function biological introduction latent vari able model chapman hall tipping analysis sparse learning becker advance neural information system press feller introduction probability theory application second leighton sand lecture physic chapter fletcher practical method second ponce computer sion modern approach prentice hall experiment algorithm thirteenth international conference machine learning morgan graphical model chine learning digital communication press belief propagation graph cycle jordan advance neural information volume press greedy function gradient machine annals statistic additive logistic regression statistical view annals statistic koller network structure approach structure discovery network chine learning chain graph property journal statistic introduction statistical tern recognition second academic press approximate continuous neural network neural network chang weighting evidence stochastic simulation network certainty intelligence volume code press chain monte stochastic simulation inference chapman hall carlin stern data analysis second chapman hall stochastic laxation distribution restoration image transaction tern analysis machine intelligence beal variational inference mixture factor muller advance neural information system volume press algorithm mixture factor analyzer technical report university param estimation linear dynamical system technical report university variational learning switching model computation jordan super learning incomplete data advance neural system volume morgan jordan factorial hidden model machine learning process regression cation thesis process action neural network adaptive rejection sampling sampling berger smith statistic volume ford university press best adaptive rejection metropolis sampling applied statistic chain monte practice chapman hall wild adaptive rejection sampling sampling applied tic gill wright practical optimization academic press bishop regression noise process treatment neural information volume press loan matrix computation third press good probability weighing smith novel approach state estimation noisy linear operator equation process application ordinary partial differential equation proceeding twentieth international machine learning porteous maximum aposteriori estimation binary image journal royal statistical society series gull development maximum data analysis skilling entropy method stork second order derivative network pruning optimal brain surgeon advance neural system volume morgan principal curve journal statistical element statistical learning springer hastings monte sampling method chain another interpretation algorithm mixture distribution statistic probability letter convolution kernel discrete structure technical report university computer science department propagation uncertainty logic sampling network uncertainty intelligence volume north learning kernel press hertz palmer theory neural computation manifold image digit transaction neural work camp keeping neural network simple scription length weight proceeding sixth annual conference computational learning theory welling view proceeding international conference independent component analysis blind signal volume reducing computational remote environment ridge sion estimation prob learning similarity approach document retrieval cation muller neural information volume press mean approach independent component analysis neural computation approximation capability network neural work white network universal neural network analysis complex tical variable principal component educational psychology relation set variable fast algorithm independent component analysis neural computation blake condensation conditional density propagation visual international journal computer sion representation function step sigmoid function application neural network theory network jordan parameter estimation variational method statistic tutorial variational proximation method advance mean field method press generative model discriminative advance neural information volume press jordan adaptive mixture local neural computation probability theory logic science university press machine learning generative invariant form prior probability estimation problem statistical method speech recognition press block sampling large probabilistic expert system international journal human computer study special issue application uncertain reasoning introduction network press chain monte method approach proximate counting integration approximation algorithm problem principal component analysis second springer jordan learning graphical model press jordan introduction graphical model preparation jordan introduction variational method graphical model jordan learning graphical model press jordan hierarchical mixture expert algorithm computation blind separation source adaptive algorithm based architecture signal approach linear prediction problem transaction society mechanical engineer series journal basic engineering dimension reduction local principal component analysis neural computation koller russel stochastic simulation algorithm dynamic probabilistic network uncertainty intelligence volume morgan discriminative training model thesis university kapur maximum entropy method engineering minimum function several variable inequality side constraint master thesis department mathematics university ka tor journal statistical duction computational learning theory press snell field application mathematical society contextual cation data image sion formation correct feature map biological cybernetics map springer function graph cut transaction pattern analysis chine intelligence arbitrary represent function neural work theorem neural network brown mian hidden model computational biology application protein journal molecular biology factor graph transaction information tucker nonlinear proceeding symposium mathematical statistic probability university press annals mathematical statistic functionally equivalent neural network neural computation process cation advance neural information system number press press bishop hybrid generative model proceeding computer vision pattern york graphical model association variable qualitative quantitative statistic propagation probability mean variance mixed graphical model journal statistical association graphical model university press computation graphical structure application expert system journal royal statistical society method factor analysis large result symposium logical factor analysis number monograph series bishop synchro time mobile device becker advance neural information volume press henry latent structure analysis boser applied code recognition neural computation optimal brain damage advance neural information volume morgan learning applied recognition proceeding support vector machine technical report department statistic university data distribution invariant learning neural scoring rule probability international review monte strategy springer least square quantization transaction information handbook matrix interpolation neural computation evidence framework applied cation network neural putation practical framework network computation method network model neural network chapter springer neural network density network nuclear instrument method physic research ensemble learning model unpublished manuscript department physic university bridge introduction gaus process bishop neural network machine learning springer comparison method handling neural computation information theory infer learning algorithm press network tittering statistic neural network interface chapter university press neal good error correcting code based sparse matrix transaction information theory method analysis observation proceed fifth symposium statistic probability volume university press matrix calculus application statistic econometrics wavelet tour signal process second academic press manning foundation statistical natural language press directional statistic stochastic model control academic press stochastic model selection machine learning generalized linear model second chapman hall logical calculus idea immanent nervous bulletin mathematical biophysics cheng turbo instance pearl belief algorithm journal selected area communication mixture model inference application cluster marcel algorithm extension peel finite mixture model meng maximum like estimation algorithm eral framework metropolis teller teller state calculation fast machine journal chemical physic metropolis monte method journal association fisher discriminant analysis neural network signal expectation propagation proximate inference koller proceeding seventeenth conference uncertainty morgan family approximate inference thesis power technical report research bridge divergence measure sage passing technical report research automatic choice diet advance neural information system volume press press expanded edition learning blind source separation independent component analysis principle practice university press training feed forward neural network thesis university moody darken fast learning network unit computation anchor hierarch triangle inequality survive high data proceeding twelfth uncertainty intelligence muller introduction kernel based learning algorithm transaction neural network muller data analysis statistical science algorithm pattern recognition springer regression theory probability application wong script recognition hidden model neal probabilistic inference chain monte method technical report department computer science university canada neal learning neural network springer lecture note statistic neal monte implementation process model regression cation technical report computer statistic university neal random walk chain monte ordered relaxation jordan learning graphical model press neal chain sampling process mixture model journal computational graphical statistic neal slice sampling annals tic neal view algorithm incremental variant jordan learning graphical model press linear model journal royal society learning machine hill mathematical learning machine morgan wright numerical springer neural network soft weight neural computation essential wavelet application data analysis learning line learning neural network university press process mean theory large margin press process cation neural support vector machine training application memo probability random variable stochastic process second hill statistical field theory pearl probabilistic reasoning gent system morgan fast exact multiplication neural computation maximum likelihood source separation generalization advance neural information system volume press line plane system point space burgh philosophical magazine journal science sixth series fast training support vector machine sequential minimal optimization advance kernel method support vector learning press probability machine advance large margin press large margin dag cation muller advance neural information system volume press network proximation learning proceeding radial basis function interpolation review mason algorithm approximation university press press numerical recipe second university press bishop upper bound error bar generalized linear regression mason mathematics neural network model application induction decision tree machine learning program machine learning morgan fundamental speech recognition prentice hall tutorial hidden model selected application speech recognition proceeding generalized optimization tree fast search proceeding fourth region international conference truth probability foundation math logical essay humanity press mitra generalized verse matrix application evaluation process method nonlinear regression thesis university healing relevance vector machine mentation proceeding international machine learning process machine learning press tung maximum likelihood estimate linear system journal learning word stress suboptimal second order neural network international conference neural network volume pattern recognition network university press stochastic approximation method annals mathematical statistic monte statistical method springer convex analysis university press principle theory brain nonlinear analysis kernel function muller neural information volume press algorithm jordan advance neural information system volume press review linear model neural putation nonlinear dimensionality reduction locally linear bedding science iteratively least square encyclopedia statistical science volume factor analysis learning internal representation propagation research group parallel distributed exploration microstructure cognition volume foundation press search group parallel distributed exploration cognition volume foundation press introduction calculus variation dover savage subjective basis practice technical report department statistic university michigan arbor port distribution neural computation muller nonlinear component analysis kernel eigenvalue problem neural computation support vector algorithm neural computation learning kernel press schwarz dimension model annals statistic schwarz finite element method press process generalization error bound sparse approximation thesis fast forward selection speed sparse process bishop proceeding ninth international work shop intelligence statistic west simulation general probabilistic inference lief network intelligence volume else vier mathematical theory communication bell system technical kernel method pattern analysis press neural network generalize neural work pattern recognition distance advance neural system volume morgan tangent prop formalism specify selected invariance adaptive network moody advance neural information process system volume morgan best practice convolutional neural network applied visual document analysis international conference document analysis recognition computer society turbulence dynamic coherent structure quarterly applied math sparse greedy process regression advance information system volume press sequential conditional probability directed graphical structure network white universal approximation network hidden layer activation function international joint conference neural work volume stone independent component tutorial introduction press sung learning human face detection memo reinforcement learning introduction press bishop bust mixture novelty detection cation mass fourth international conference neural network volume data domain support vector proceeding symposium neural network press jordan beal hierarchical process journal statistical association appear silva global framework linear dimensionality reduction science backgammon program play neural computation meek graphical model proceeding twentieth conference uncertainty canada press regression shrinkage lection lasso journal royal tical society chain exploring distribution annals statistic solution problem hierarchical nonlinear manifold action pattern analysis machine directional curvature visualize folding pattern projection manifold neural network springer tipping probabilistic binary data advance neural information system press tipping sparse learning relevance vector machine journal chine learning research tipping bishop principal component analysis technical port neural research group university tipping bishop mixture probabilistic principal component analyzer neural computation tipping bishop prob principal component analysis journal royal statistical society series tipping fast marginal likelihood maximization sparse model bishop proceeding ninth international workshop intelligence statistic west tong koller restricted proceeding national conference intelligence scaling system large data set data mining knowledge theory motion valiant theory learnable communication association machinery estimation dependence based empirical data springer nature statistical learn theory springer statistical learning theory sensitivity support vector machine proceeding joint conference intelligence workshop statistical wavelet viola robust face detection international journal computer sion error bound code asymptotically optimum algorithm transaction theory principle digital communication hill comparison choosing smoothing parameter spline smoothing problem numerical mathematics wainwright class upper bound partition function transaction theory walker asymptotic behaviour posterior distribution journal royal statistical society walker laud smith inference random distribution related function discussion journal royal society smooth regression analysis journal statistic series functional approximation network approach transaction neural network concise encyclopedia mathematics chapman hall port vector machine graphical model applied statistic adaptive switching circuit convention record volume lehr year neural network madeline proceeding fractional belief propagation becker advance neural system volume press computation neural network neural computation prediction process linear regression linear diction beyond jordan learn graphical model press barber cation process transaction pattern analysis machine intelligence method speed kernel machine advance neural information volume press blake sparse learning visual transaction pattern analysis machine intelligence neural network model conditional density neural computation bishop variational sage passing journal machine learning search fundamental filtering practical approach
